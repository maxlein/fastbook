{
  
    
        "post0": {
            "title": "Title",
            "content": "[appendix] [role=&quot;Jupyter notebook 101&quot;] Appendix: Jupyter notebook 101 . You can read this tutorial in the book, but we strongly suggest reading it in a (yes, you guessed it) Jupyter Notebook. This way, you will be able to actually try the different commands we will introduce here. If you followed one of our tutorial in the previous section, you should have been left in the course folder. Just click on nbs then dl1 and you should find the tutorial named 00_notebook_tutorial. Click on it to open a new tab and you&#39;ll be ready to go. . If you are on your personal machine, clone the course repository with and navigate inside before following the same steps. . Introduction . Let&#39;s build up from the basics, what is a Jupyter Notebook? Well, we wrote this book using jupyter notebooks. It is a document made of cells. You can write in some of them (markdown cells) or your can perform calculations in Python (code cells) and run them like this: . 1+1 . 2 . Cool huh? This combination of prose and code makes Jupyter Notebook ideal for experimentation: we can see the rationale for each experiment, the code, and the results in one comprehensive document. . Other renowned institutions in academy and industry use Jupyter Notebook, including Google, Microsoft, IBM, Bloomberg, Berkeley and NASA among others. Even Nobel-winning economists use Jupyter Notebooks for their experiments and some suggest that Jupyter Notebooks will be the new format for research papers. . Writing . A type of cell in which you can write text is called a Markdown cell. Markdown is a very popular markup language. To specify that a cell is Markdown you need to click in the drop-down menu in the toolbar and select Markdown. . Click on the the &#39;+&#39; button on the left and select Markdown from the toolbar. Now you can type your first Markdown cell. Write &#39;My first markdown cell&#39; and press run. . . You should see something like this: . My first markdown cell . Now try making your first Code cell: follow the same steps as before but don&#39;t change the cell type (when you add a cell its default type is Code). Type something like 3/2. You should see &#39;1.5&#39; as output. . 3/2 . 1.5 . Modes . If you made a mistake in your Markdown cell and you have already run it, you will notice that you cannot edit it just by clicking on it. This is because you are in Command Mode. Jupyter Notebooks have two distinct modes: . Edit Mode:: Allows you to edit a cell&#39;s content. . | Command Mode:: Allows you to edit the notebook as a whole and use keyboard shortcuts but not edit a cell&#39;s content. . | . You can toggle between these two by either pressing ESC and Enter or clicking outside a cell or inside it (you need to double click if its a Markdown cell). You can always tell which mode you&#39;re on: the current cell will have a green border if in Edit Mode and a blue border in Command Mode. Try it! . Other Important Considerations . Your notebook is autosaved every 120 seconds. If you want to manually save it you can just press the save button on the upper left corner or press s in Command Mode. . . To know if your kernel (the python engine executing your instructions behind the scene) is computing or not you can check the dot in your upper right corner. If the dot is full, it means that the kernel is working. If not, it is idle. You can place the mouse on it and the state of the kernel will be displayed. . . There are a couple of shortcuts you must know about which we use all the time (always in Command Mode). These are: . Shift+Enter:: Runs the code or markdown on a cell . | Up Arrow+Down Arrow:: Toggle across cells . | b:: Create new cell . | 0+0:: Reset Kernel . | . You can find more shortcuts by typing h (for help). . You may need to use a terminal in a Jupyter Notebook environment (for example to git pull on a repository). That is very easy to do, just press &#39;New&#39; in your Home directory and &#39;Terminal&#39;. Don&#39;t know how to use the Terminal? We made a tutorial for that as well. You can find it here. . . That&#39;s it. This is all you need to know to use Jupyter Notebooks. That said, we have more tips and tricks below, so don&#39;t jump to the next section just yet. . Markdown formatting . Italics, Bold, Strikethrough, Inline, Blockquotes and Links . The five most important concepts to format your code appropriately when using Markdown are: . Italics:: Surround your text with _ or *. . | Bold:: Surround your text with __ or **. . | inline:: Surround your text with `. . | blockquote:: Place &gt; before your text. . | Links:: Surround the text you want to link with [] and place the link adjacent to the text, surrounded with (). . | . Headings . Notice that including a hashtag before the text in a markdown cell makes the text a heading. The number of hashtags you include will determine the priority of the header (# is level one, ## is level two, ### is level three and #### is level four). We will add three new cells with the + button on the left to see how every level of heading looks. . In the notebook, double click on some headings and find out what level they are! . Lists . There are three types of lists in markdown. . Ordered list: . Step 1 Step 1B | | Step 3 | Unordered list . learning rate | cycle length | weight decay | . Task list . [x] Learn Jupyter Notebooks [x] Writing | [x] Modes | [x] Other Considerations | . | [ ] Change the world | . In the notebook, double click on them to see how they are built! . Code Capabilities . Code cells are different than Markdown cells in that they have an output cell. This means that we can keep the results of our code within the notebook and share them. Let&#39;s say we want to show a graph that explains the result of an experiment. We can just run the necessary cells and save the notebook. The output will be there when we open it again! Try it out by running the next four cells. . # Import necessary libraries from fastai2.vision.all import * import matplotlib.pyplot as plt . from PIL import Image . a = 1 b = a + 1 c = b + a + 1 d = c + b + a + 1 a, b, c ,d . (1, 2, 4, 8) . plt.plot([a,b,c,d]) plt.show() . We can also print images while experimenting. I am watching you. . Image.open(&#39;images/chapter1_cat_example.jpg&#39;) . Running the app locally . You may be running Jupyter Notebook from an interactive coding environment like Gradient, Sagemaker or Salamander. You can also run a Jupyter Notebook server from your local computer. What&#39;s more, if you have installed Anaconda you don&#39;t even need to install Jupyter (if not, just pip install jupyter). . You just need to run jupyter notebook in your terminal. Remember to run it from a folder that contains all the folders/files you will want to access. You will be able to open, view, and edit files located within the directory in which you run this command but not files in parent directories. . If a browser tab does not open automatically once you run the command, you should CTRL+CLICK the link starting with &#39;http://localhost:&#39; and this will open a new tab in your default browser. . Creating a notebook . Now that you have your own jupyter notebook running, you will probably want to write your owns. Click on &#39;New&#39; in the upper left corner and &#39;Python 3&#39; in the drop-down list (we are going to use a Python kernel for all our experiments). . . Shortcuts and tricks . Here is a list of useful tricks when in a Jupyter Notebook. Make sure you learn them early and use them as often as you can! . Command Mode Shortcuts . There are a couple of useful keyboard shortcuts in Command Mode that you can leverage to make Jupyter Notebook faster to use. Remember that to switch back and forth between Command Mode and Edit Mode with Esc and Enter. . m:: Convert cell to Markdown . | y:: Convert cell to Code . | D+D:: Delete cell . | o:: Toggle between hide or show output . | Shift+Arrow up/Arrow down:: Selects multiple cells. Once you have selected them you can operate on them like a batch (run, copy, paste etc). . | Shift+M:: Merge selected cells. . | Shift+Tab (press once):: Tells you which parameters to pass on a function . | Shift+Tab (press three times):: Gives additional information on the method . | . Cell Tricks . There are also some tricks that you can code into a cell: . ?function-name:: Shows the definition and docstring for that function . | ??function-name:: Shows the source code for that function . | doc(function-name):: Shows the definition, docstring and links to the documentation of the function (only works with fastai library imported) . | . Line Magics . Line magics are functions that you can run on cells. They should be at the beginning of a line and take as an argument the rest of the line from where they are called. You call them by placing a &#39;%&#39; sign before the command. The most useful ones are: . %matplotlib inline:: This command ensures that all matplotlib plots will be plotted in the output cell within the notebook and will be kept in the notebook when saved. | . This command is always called together at the beggining of every notebook of the fast.ai course. . %matplotlib inline . %timeit:: Runs a line a ten thousand times and displays the average time it took to run it. | . %timeit [i+1 for i in range(1000)] . 56.1 µs ± 592 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each) . %debug: Allows to inspect a function which is showing an error using the Python debugger. If you type this in a cell just after an error, you will be directed in a console where you can inspect the values of all the variables. .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/app_jupyter.html",
            "relUrl": "/2020/03/19/app_jupyter.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "[appendix] [role=&quot;Creating a blog&quot;] Creating a blog . Blogging with GitHub Pages . Unfortunately, when it comes to blogging, it seems like you have to make a difficult decision: either use a platform that makes it easy, but subjects you and your readers to advertisements, pay walls, and fees, or spend hours setting up your own hosting and weeks learning about all kinds of intricate details. Perhaps the biggest benefit to the &quot;do-it-yourself&quot; approach is that you really owning your own posts, rather than being at the whim of a service provider, and their decisions about how to monetize your content in the future. . It turns out, however, that you can have the best of both worlds! You can host on a platform called GitHub Pages, which is free, has no ads or pay wall, and makes your data available in a standard way such that you can at any time move your blog to another host. But all the approaches I’ve seen to using GitHub Pages have required knowledge of the command line and arcane tools that only software developers are likely to be familiar with. For instance, GitHub&#39;s own documentation on setting up a blog requires installing the Ruby programming language, using the git command line tool, copying over version numbers, and more. 17 steps in total! . We’ve curated an easy approach, which allows you to use an entirely browser-based interface for all your blogging needs. You will be up and running with your new blog within about five minutes. It doesn’t cost anything, and you can easily add your own custom domain to it if you wish to. Here’s how to do it, using a template we&#39;ve created called fast_template. (NB: be sure to check the book website for the latest blog recommendations, since new tools are always coming out; for instance, we&#39;re currently working with GitHub on creating a new tool called &quot;fastpages&quot; which is a more advanced version of fast_template that&#39;s particularly designed for people using Jupyter Notebooks). . Creating the repository . You’ll need an account on GitHub. So, head over there now, and create an account if you don’t have one already. Make sure that you are logged in. Normally, GitHub is used by software developers for writing code, and they use a sophisticated command line tool to work with it. But I&#39;m going to show you an approach that doesn&#39;t use the command line at all! . To get started, click on this link: https://github.com/fastai/fast_template/generate . This will allow you to create a place to store your blog, called a &quot;repository&quot;. You will see the following screen; you have to enter your repository name using the exact form you see below, that is, the username you used at GitHub followed by .github.io. . Creating your repository . Important: Note that if you don&#8217;t use username.github.io as the name, it won&#8217;t work! Once you’ve entered that, and any description you like, click on &quot;create repository from template&quot;. You have the choice to make the repository &quot;private&quot; but since you are creating a blog that you want other people to read, having the underlying files publicly available hopefully won&#39;t be a problem for you. . Now, let&#39;s set up your homepage! . Setting up your homepage . When readers first arrive at your blog the first thing that they will see is the content of a file called &quot;index.md&quot;. This is a markdown file. Markdown is a powerful yet simple way of creating formatted text, such as bullet points, italics, hyperlinks, and so forth. It is very widely used, including all the formatting in Jupyter notebooks, nearly every part of the GitHub site, and many other places all over the Internet. To create markdown text, you can just type in plain regular English. But then you can add some special characters to add special behavior. For instance, if you type a * character around a word or phrase then that will put it in italics. Let’s try it now. . To open the file, click its file name in GitHub. . . To edit it, click on the pencil icon at the far right hand side of the screen. . . You can add, edit, or replace the texts that you see. Click on the &quot;preview changes&quot; button to see how well your markdown text will look on your blog. Lines that you have added or changed will appear with a green bar on the left-hand side. . . To save your changes to your blog, you must scroll to the bottom and click on the &quot;commit changes&quot; green button. On GitHub, to &quot;commit&quot; something means to save it to the GitHub server. . . Next, you should configure your blog’s settings. To do so, click on the file called &quot;_config.yml&quot;, and then click on the edit button like you did for the index file above. Change the title, description, and GitHub username values. You need to leave the names before the colons in place and type your new values in after the colon and space on each line. You can also add to your email and Twitter username if you wish — but note that these will appear on your public blog if you do fill them in here. . Fill the config file . After you’re done, commit your changes just like you did with the index file before. Then wait about a minute, whilst GitHub processes your new blog. Then you will be able to go to your blog in your web browser, by opening the URL: username.github.io (replace &quot;username&quot; with your GitHub username). You should see your blog! . Your blog is onlyine! . Creating posts . Now you’re ready to create your first post. All your posts will go in the &quot;_posts&quot; folder. Click on that now, and then click on the &quot;create file&quot; button. You need to be careful to name your file in the following format: &quot;year-month-day-name.md&quot;, where year is a four-digit number, and month and day are two-digit numbers. &quot;Name&quot; can be anything you want, that will help you remember what this post was about. The &quot;md&quot; extension is for markdown documents. . . You can then type the contents of your first post. The only rule is that the first line of your post must be a markdown heading. This is created by putting # at the start of a line (that creates a level 1 heading, which you should just use once at the start of your document; you create level 2 headings using ##, level 3 with ###, and so forth.) . . As before, you can click on the &quot;preview&quot; button to see how your markdown formatting will look. . . And you will need to click the &quot;commit new file&quot; button to save it to GitHub. . . Have a look at your blog homepage again, and you will see that this post has now appeared! (Remember that you will need to wait a minute or so for GitHub to process it.) . . You’ll also see that we provided a sample blog post, which you can go ahead and delete now. Go to your posts folder, as before, and click on &quot;2020-01-14-welcome.md&quot;. Then click on the trash icon on the far right. . . In GitHub, nothing actually changes until you commit— including deleting a file! So, after you click the trash icon, scroll down to the bottom and commit your changes. . You can include images in your posts by adding a line of markdown like the following: . ![Image description](images/filename.jpg) . For this to work, you will need to put the image inside your &quot;images&quot; folder. To do this, click on the images folder to go into it in GitHub, and then click the &quot;upload files&quot; button. . . Now let&#39;s see how to do all of this directly from your computer. . Synchronizing GitHub and your computer . There’s lots of reasons you might want to copy your blog content from GitHub to your computer. Perhaps you want to read or edit your posts offline. Or maybe you’d like a backup in case something happens to your GitHub repository. . GitHub does more than just let you copy your repository to your computer; it lets you synchronize it with your computer. So, you can make changes on GitHub, and they’ll copy over to your computer, and you can make changes on your computer, and they’ll copy over to GitHub. You can even let other people access and modify your blog, and their changes and your changes will be automatically combined together next time you sync. . To make this work, you have to install an application called GitHub Desktop to your computer. It runs on Mac, Windows, and Linux. Follow the directions at the link to install it, then when you run it it’ll ask you to login to GitHub, and then to select your repository to sync; click &quot;Clone a repository from the Internet&quot;. . . Once GitHub has finished syncing your repo, you’ll be able to click &quot;View the files of your repository in Finder&quot; (or Explorer), and you’ll see the local copy of your blog! Try editing one of the files on your computer. Then return to GitHub Desktop, and you’ll see the &quot;Sync&quot; button is waiting for you to press it. When you click it, your changes will be copied over to GitHub, where you’ll see them reflected on the web site. . . If you haven&#39;t used git before, GitHub Desktop and a blog is a great way to get started. As you&#39;ll discover, it&#39;s a fundamental tool used by most data scientists. Another tool that we hope you now love too is Jupyter Notebooks. And there is a way to write your blog directly with it! . Jupyter for blogging . You can also write blog posts using Jupyter Notebooks! Your markdown cells, code cells, and all outputs will appear in your exported blog post. The best way to do this may have changed by the time you are reading this book, so be sure to check out the book website for the latest information. As we write this, the easiest way to create a blog from notebooks is to use fastpages, which is a more advanced version of fast_template. . To blog with a notebook, just pop it in the _notebooks folder in your blog repo, and it will appear in your blog. When you write your notebook, write whatever you want your audience to see. Since most writing platforms make it much harder to include code and outputs, many of us are in a habit of including less real examples than we should. So try to get into a new habit of including lots of examples as you write. . Often you&#39;ll want to hide boilerplate such as import statements. Add #hide to the top of any cell to make it not show up in output. Jupyter displays the result of the last line of a cell, so there&#39;s no need to include print(). (And including extra code that isn&#39;t needed means there&#39;s more cognitive overhead for the reader; so don&#39;t include code that you don&#39;t really need!) .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/app_blog.html",
            "relUrl": "/2020/03/19/app_blog.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Title",
            "content": "[[chapter_tabular]] Tabular modelling deep dive . Tabular modelling takes data in the form of a table (like a spreadsheet or CSV--comma separated values). The objective is to predict the value in one column, based on the values in the other columns. . Categorical embeddings . In tabular data some columns contain numerical data, like &quot;age&quot;, others contain string values, like &quot;sex&quot;. The numerical data can be directly fed to the model (with some optional preprocessing), but other columns need to be converted to numbers. Since the values in those correspond to different categories, we often call these type of variables categorical variables. The first type are called continuous variables. . jargon: Continuous and categorical variables: &quot;Continuous variables&quot; are numerical data, such as &quot;age&quot; can be directly fed to the model, since you can add and multiply them directly. &quot;Categorical variables&quot; contain a number of discrete levels, such as &quot;movie id&quot;, for which addition and multiplication don&#39;t have meaning (even if they&#39;re stored as numbers). . At the end of 2015, the Rossmann sales competition ran on Kaggle. Competitors were given a wide range of information about various stores in Germany, and were tasked with trying to predict sales on a number of days. The goal was to help them to manage stock properly and to be able to properly satisfy the demand without holding unnecessary inventory. The official training set provided a lot of information about the stores. It was also permitted for competitors to use additional data, as long as that data was made public and available to all participants. . One of the gold medalists used deep learning, in one of the earliest known examples of a state of the art deep learning tabular model. Their method involved far less feature engineering, based on domain knowledge, than the other gold medalists. They wrote a paper, Entity Embeddings of Categorical Variables, about their approach. In an online-only chapter on the book website we show how to replicate their approach from scratch and attain the same accuracy shown in the paper. In the abstract of the paper they say: . : Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables... it is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit... As entity embedding defines a distance measure for categorical variables it can be used for visualizing categorical data and for data clustering . We have already noticed all of these points when we built our collaborative filtering model. We can clearly see that these insights go far beyond just collaborative filtering, however. . The paper also points out that (as we discussed in the last chapter) that an embedding layer is exactly equivalent to placing an ordinary linear layer after every one-hot encoded input layer. They used the diagram in &lt;&gt; to show this equivalence. Note that &quot;dense layer&quot; is another term with the same meaning as &quot;linear layer&quot;, the one-hot encoding layers represent inputs.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Entity embeddings in a neural network . The insight is important because we already know how to train linear layers, so this shows that from the point of view of the architecture and our training algorithm the embedding layer is just another layer. We also saw this in practice in the last chapter, when we built a collaborative filtering neural network that looks exactly like this diagram. . Where we analyzed the embedding weights for movie reviews, the authors of the entity embeddings paper analyzed the embedding weights for their sales prediction model. What they found was quite amazing, and illustrates their second key insight. This is that the embedding makes the categorical variables into something which is both continuous and also meaningful. . The images in &lt;&gt; below illustrate these ideas. They are based on the approaches used in the paper, along with some analysis we have added.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; State embeddings and map . On the left in the image below is a plot of the embedding matrix for the possible values of the State category. For a categorical variable we call the possible values of the variable its &quot;levels&quot; (or &quot;categories&quot; or &quot;classes&quot;), so here one level is &quot;Berlin,&quot; another is &quot;Hamburg,&quot; etc.. On the right is a map of Germany. The actual physical locations of the German states were not part of the provided data; yet, the model itself learned where they must be, based only on the behavior of store sales! . Do you remember how we talked about distance between embeddings? The authors of the paper plotted the distance between embeddings between stores against the actual geographic distance between the stores in practice (see &lt;&gt;). They found that they matched very closely!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Store distances . We&#39;ve even tried plotted the embeddings for days of the week and months of the year, and found that days and months that are near each other on the calendar ended up close as embeddings too, as shown in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Date embeddings . What stands out in these two examples is that we provide the model fundamentally categorical data about discrete entities (German states or days of the week), and then the model learns an embedding for these entities which defines a continuous notion of distance between them. Because the embedding distance was learned based on real patterns in the data, that distance tends to match up with our intuitions. . In addition, it is also valuable in its own right that embeddings are continuous. It is valuable because models are better at understanding continuous variables. This is unsurprising considering models are built of many continuous parameter weights and continuous activation values, which are updated via gradient descent, a learning algorithm for finding the minimums of continuous functions. . Is is also valuable because we can combine our continuous embedding values with truly continuous input data in a straightforward manner: we just concatenate the variables, and feed the concatenation into our first dense layer. In other words, the raw categorical data is transformed by an embedding layer, before it interacts with the raw continuous input data. This is how fastai, and the entity embeddings paper, handle tabular models containing continuous and categorical variables. . An example using this concatenation approach is how Google do their recommendations on Google Play, as they explained in their paper Wide &amp; Deep Learning for Recommender Systems, and as shown in this figure from their paper: . The Google Play recommendation system . Interestingly, Google are actually combining both the two approaches we saw in the previous chapter: the dot product (which Google call Cross Product) and neural network approach. . But let&#39;s pause for a moment. So far, the solution to all of our modelling problems has been: train a deep learning model. And indeed, that is a pretty good rule of thumb for complex unstructured data like images, sounds, natural language text, and so forth. Deep learning also works very well for collaborative filtering. But it is not always the best starting point for analysing tabular data. . Beyond deep learning . Most machine learning courses will throw dozens of different algorithms at you, with a brief technical description of the math behind them and maybe a toy example. You&#39;re left confused by the enormous range of techniques shown and have little practical understanding of how to apply them. . The good news is that modern machine learning can be distilled down to a couple of key techniques that are widely applicable. Recent studies have shown that the vast majority of datasets can be best modeled with just two methods: . Ensembles of decision trees (i.e. Random Forests and Gradient Boosting Machines), mainly for structured data (such as you might find in a database table at most companies) | Multi-layered neural networks learnt with SGD (i.e. shallow and/or deep learning), mainly for unstructured data (such as audio, vision, and natural language) | Although deep learning is nearly always clearly superior for unstructured data, these two approaches tend to give quite similar results for many kinds of structured data. But ensembles of decision trees tend to train faster, are often easier to interpret, do not require special GPU hardware for inference at scale, and often require less hyperparameter tuning. They have been popular for quite a lot longer than deep learning, so there is a more mature ecosystem for tooling and documentation around them. . Most importantly, the critical step of interpreting a model of tabular data is significantly easier for decision tree ensembles. There are tools and methods for answering the pertinent questions. For instance, which columns in the dataset were the most important for your predictions? How are they related to the dependent variable? How do they interact with each other? And which particular features were most important for some particular observation? . Therefore, ensembles of decision trees are our first approach for analysing a new tabular dataset. . The exception to this guideline is when the dataset meets one of these conditions: . There are some high cardinality categorical variables that are very important (&quot;cardinality&quot; refers to the number of discrete levels representing categories, so a high cardinality categorical variable is something like a ZIP Code, which can take on thousands of possible levels) | There are some columns which contain data which would be best understood with a neural network, such as plaintext data. | . In practice, when we deal with datasets which meet these exceptional conditions, we would always try both decision tree ensembles and deep learning to see which works best. It is likely that deep learning would be a useful approach in our example of collaborative filtering, as you have at least two high cardinality categorical variables: the users and the movies. But in practice things tend to be less cut and dried, and there will often be a mixture of high and low cardinality categorical variables and continuous variables. . Either way, it&#39;s clear that we are going to need to add decision tree ensembles to our modelling toolbox! . Up to now we&#39;ve used PyTorch and fastai for pretty much all of our heavy lifting. But these libraries are mainly designed for algorithms that do lots of matrix multiplication and derivatives (that is, stuff like deep learning!) Decision trees don&#39;t depend on these operations at all, so PyTorch isn&#39;t much use. . Instead, we will be largely relying on a library called scikit-learn (also known as sklearn). Scikit-learn is a popular library for creating machine learning models, using approaches that are not covered by deep learning. In addition, we&#39;ll need to do some tabular data processing and querying, so we&#39;ll want to use the Pandas library. Finally, we&#39;ll also need numpy, since that&#39;s the main numeric programming library that both sklearn and Pandas rely on. . We don&#39;t have time to do a deep dive on all these libraries in this book, so we&#39;ll just be touching on some of the main parts of each. For a far more in depth discussion, we strongly suggest Wes McKinney&#39;s Python for Data Analysis, 2nd ed. Wes is the creator of Pandas, so you can be sure that the information is accurate! . First, let&#39;s gather the data we will use. . The dataset . For our dataset, we will be looking at the Blue Book for Bulldozers Kaggle Competition: &quot;The goal of the contest is to predict the sale price of a particular piece of heavy equipment at auction based on its usage, equipment type, and configuration. The data is sourced from auction result postings and includes information on usage and equipment configurations.&quot; . This is a very common type of dataset and prediction problem, and similar to what you may see in your project or workplace. It&#39;s available for download on Kaggle, a website that hosts data science competitions. . Kaggle Competitions . Kaggle is an awesome resource for aspiring data scientists or anyone looking to improve their machine learning skills. There is nothing like getting hands-on practice and receiving real-time feedback to help you improve your skills. . Kaggle provides: . Interesting datasets | Feedback on how you&#39;re doing | A leader board to see what&#39;s good, what&#39;s possible, and what&#39;s state-of-art. | Blog posts by winning contestants sharing useful tips and techniques. | Until now all our datasets have been available to download through fastai&#39;s integrated dataset system. However, the dataset we will be using in this chapter is only available from Kaggle. Therefore, you will need to sign up to Kaggle, then you need to go to the page for the competition. On that page click on &quot;rules&quot;, and then &quot;I understand and accept&quot;. (Although the competition has finished, and you will not be entering it, you still have to agree to the rules to be allowed to download the data). . The easiest way to download Kaggle datasets is to use the Kaggle API. You can install this using pip by running this in a notebook cell: . !pip install kaggle . You need an API key to use the Kaggle API; to get one, go to &quot;my account&quot; on the Kaggle website, and click &quot;create new API token&quot;. This will save a file called kaggle.json to your PC. We need to create this on your GPU server. To do so, open the file you downloaded, copy the contents, and paste them inside &#39;&#39; below, e.g.: creds = &#39;{&quot;username&quot;:&quot;xxx&quot;,&quot;key&quot;:&quot;xxx&quot;}&#39;: . creds = &#39;&#39; . ...then execute this cell (only needs to be run once): . cred_path = Path(&#39;~/.kaggle/kaggle.json&#39;).expanduser() if not cred_path.exists(): cred_path.parent.mkdir(exist_ok=True) cred_path.write(creds) cred_path.chmod(0o600) . Now you can download datasets from Kaggle! We&#39;ll pick a path to download the dataset to: . path = URLs.path(&#39;bluebook&#39;) path . Path(&#39;/home/jhoward/.fastai/archive/bluebook&#39;) . ...and use the Kaggle API to download the dataset to that path, and extract it: . if not path.exists(): path.mkdir() api.competition_download_cli(&#39;bluebook-for-bulldozers&#39;, path=path) file_extract(path/&#39;bluebook-for-bulldozers.zip&#39;) path.ls(file_type=&#39;text&#39;) . (#7) [Path(&#39;TrainAndValid.csv&#39;),Path(&#39;Machine_Appendix.csv&#39;),Path(&#39;random_forest_benchmark_test.csv&#39;),Path(&#39;Test.csv&#39;),Path(&#39;median_benchmark.csv&#39;),Path(&#39;ValidSolution.csv&#39;),Path(&#39;Valid.csv&#39;)] . Now that we have downloaded our dataset, let&#39;s have a look at it! . Look at the data . Kaggle provides information about some of the fields of our dataset; on the Kaggle Data info page they say that the key fields in train.csv are: . SalesID: the unique identifier of the sale | MachineID: the unique identifier of a machine. A machine can be sold multiple times | saleprice: what the machine sold for at auction (only provided in train.csv) | saledate: the date of the sale | . In any sort of data science work, it&#39;s important to look at your data directly to make sure you understand the format, how it&#39;s stored, what type of values it holds, etc.. Even if you&#39;ve read descriptions about your data, the actual data may not be what you expect. We&#39;ll start by reading the training set into a Pandas DataFrame; note that we have to tell Pandas which columns contain dates. Generally it&#39;s a good idea to also specify low_memory=False unless Pandas actually runs out of memory and returns an error. The low_memory parameter, which is True by default, tells Pandas to only look at a few rows of data at a time to figure out what type of data is in each column. This means that Pandas can actually end up using a different data type for different rows, which generally leads to data processing errors or model training problems later. . df = pd.read_csv(path/&#39;TrainAndValid.csv&#39;, low_memory=False) . df.columns . Index([&#39;SalesID&#39;, &#39;SalePrice&#39;, &#39;MachineID&#39;, &#39;ModelID&#39;, &#39;datasource&#39;, &#39;auctioneerID&#39;, &#39;YearMade&#39;, &#39;MachineHoursCurrentMeter&#39;, &#39;UsageBand&#39;, &#39;saledate&#39;, &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;, &#39;fiSecondaryDesc&#39;, &#39;fiModelSeries&#39;, &#39;fiModelDescriptor&#39;, &#39;ProductSize&#39;, &#39;fiProductClassDesc&#39;, &#39;state&#39;, &#39;ProductGroup&#39;, &#39;ProductGroupDesc&#39;, &#39;Drive_System&#39;, &#39;Enclosure&#39;, &#39;Forks&#39;, &#39;Pad_Type&#39;, &#39;Ride_Control&#39;, &#39;Stick&#39;, &#39;Transmission&#39;, &#39;Turbocharged&#39;, &#39;Blade_Extension&#39;, &#39;Blade_Width&#39;, &#39;Enclosure_Type&#39;, &#39;Engine_Horsepower&#39;, &#39;Hydraulics&#39;, &#39;Pushblock&#39;, &#39;Ripper&#39;, &#39;Scarifier&#39;, &#39;Tip_Control&#39;, &#39;Tire_Size&#39;, &#39;Coupler&#39;, &#39;Coupler_System&#39;, &#39;Grouser_Tracks&#39;, &#39;Hydraulics_Flow&#39;, &#39;Track_Type&#39;, &#39;Undercarriage_Pad_Width&#39;, &#39;Stick_Length&#39;, &#39;Thumb&#39;, &#39;Pattern_Changer&#39;, &#39;Grouser_Type&#39;, &#39;Backhoe_Mounting&#39;, &#39;Blade_Type&#39;, &#39;Travel_Controls&#39;, &#39;Differential_Type&#39;, &#39;Steering_Controls&#39;], dtype=&#39;object&#39;) . That&#39;s a lot of columns for us to look at! Try looking through the dataset to get a sense of what kind of information is in each one. We&#39;ll shortly see how to &quot;zero in&quot; on the most interesting bits. . At this point, a good next step is to handle ordinal columns. This refers to columns containing strings or similar, but where those strings have a natural ordering. For instance, here are the levels of ProductSize: . df[&#39;ProductSize&#39;].unique() . array([nan, &#39;Medium&#39;, &#39;Small&#39;, &#39;Large / Medium&#39;, &#39;Mini&#39;, &#39;Large&#39;, &#39;Compact&#39;], dtype=object) . We can tell Pandas about a suitable ordering of these levels like so: . sizes = &#39;Large&#39;,&#39;Large / Medium&#39;,&#39;Medium&#39;,&#39;Small&#39;,&#39;Mini&#39;,&#39;Compact&#39; . df[&#39;ProductSize&#39;] = df[&#39;ProductSize&#39;].astype(&#39;category&#39;) df[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered=True, inplace=True) . The most important data column is the dependent variable, that is the one we want to predict. Recall that a models metric is a function that reflects how good the predictions are. It&#39;s important to note what metric is being used for a project. Generally, selecting the metric is an important part of the project setup. In many cases, choosing a good metric will require more than just selecting a variable that already exists. It is more like a design process. You should think carefully about which metric, or set of metric, actually measures the notion of model quality which matters to you. If no variable represents that metric, you should see if you can build the metric from the variables which are available. . However, in this case Kaggle tells us what metric to use: RMSLE (root mean squared log error) between the actual and predicted auction prices. Here we need do only a small amount of processing to use this: we take the log of the prices, so that m_rmse of that value will give us what we ultimately need. . dep_var = &#39;SalePrice&#39; . df[dep_var] = np.log(df[dep_var]) . We are now ready to have a look at our first machine learning algorithm for tabular data: decision trees. . Decision trees . Decision tree ensembles, as the name suggests, rely on decision trees. So let&#39;s start there! A decision tree asks a series of binary (that is, yes or no) questions about the data. After each question the data at that part of the tree is split between a &quot;yes&quot; and a &quot;no&quot; branch. After one or more questions, either a prediction can be made on the basis of all previous answers or another question is required. . TK: Adding a figure here might be useful . This sequence of questions is now a procedure for taking any data item, whether an item from the training set or a new one, and assigning that item to a group. Namely, after asking and answering the questions, we can say the item belongs to the group of all the other training data items which yielded the same set of answers to the questions. But what good is this? the goal of our model is to predict values for items, not to assign them into groups from the training dataset. The value of this is that we can now assign a prediction value for each of these groups--for regression, we take the target mean of the items in the group. . Let&#39;s consider how we find the right questions to ask. Of course, we wouldn&#39;t want to have to create all these questions ourselves — that&#39;s what computers are for! The basic steps to train a decision tree can be written down very easily: . Loop through each column of the dataset in turn | For each column, loop through each possible level of that column in turn | Try splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable) | Find the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple &quot;model&quot; where our predictions are simply the average sale price of the item&#39;s group | After looping through all of the columns and possible levels for each, pick the split point which gave the best predictions using our very simple model | We now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each, by going back to step one for each group | Continue this process recursively, and until you have reached some stopping criterion for each group — for instance, stop splitting a group further when it has only 20 items in it. | Although this is an easy enough algorithm to implement yourself (and it is a good exercise to do so) we can save some time by using the implementation built into sklearn. . But even before using sklearn, we have to prepare our data somewhat before we can use it. . A: Here&#39;s a productive question to ponder. If you consider that the procedure for defining decision tree essentially chooses one sequence of splitting questions about variables, you might ask yourself, how do we know this procedure chooses the correct sequence? The rule is to choose the splitting question which produces the best split, and then to apply the same rule to groups that split produces, and so on (this is known in computer science as a &quot;greedy&quot; approach). Can you imagine a scenario in which asking a “less powerful” splitting question would enable a better split down the road (or should I say down the trunk!) and lead to a better result overall? . Handling dates . The first piece of data preparation we need to do is to enrich our representation of dates. The fundamental basis of the decision tree which we just described is bisection -- dividing up a group into two. We look at the ordinal variables and divide up the dataset based on whether the variable&#39;s value is greater (or lower) than a threshhold, and we look at the categorical variables and divided up the dataset based on whether the variable&#39;s level is a particular level. So this algorithm has a way of dividing up the dataset based on both ordinal and categorical data. . How does this apply to a common data type, the date? You might want to treat a date as an ordinal value, because it is meaningful to say that one date is greater than another. However, dates are a bit different from most ordinal values in that some dates are qualitatively different from others in a way that that is often relevant to the systems we are modelling. . So in order to help the above algorithm handle dates intelligently, we&#39;d like our model to know more than whether a date is more recent or less recent. We might want our model to make decisions based on that date&#39;s day of week, on whether a day is a holiday, on what month it is in, and so forth. To do this, we replace every date column with a set of date metadata columns, such as holiday, day of week, and month. These columns provide categorical data that we suspect will be useful. . Fastai comes with a function that will do this for us — we just have to pass a column name which contains dates: . df = add_datepart(df, &#39;saledate&#39;) . Let&#39;s do the same for the test set while we&#39;re there: . df_test = pd.read_csv(path/&#39;Test.csv&#39;, low_memory=False) df_test = add_datepart(df_test, &#39;saledate&#39;) . We can see that there are now lots of new columns in our DataFrame: . &#39; &#39;.join(o for o in df.columns if o.startswith(&#39;sale&#39;)) . &#39;saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start saleElapsed&#39; . This is a good first step, but we will need to do a bit more cleaning. For this, we will use fastai objects called TabularPandas and TabularProc. . Using TabularPandas and TabularProc . A second piece of preparatory processing is to be sure we can handle strings and missing data. Out of the box, sklearn cannot do either. Instead we will use fastai&#39;s class TabularPandas, which wraps a Pandas data frame and provides a few conveniences. To populate a TabularPandas, we will use two TabularProcs, Categorify and FillMissing. A TabularProc is like a regular Transform, except that: . It returns the exact same object that&#39;s passed to it, after modifying the object in-place, and | It runs the transform once, when data is first passed in, rather than lazily as the data is accessed. | . Categorify is a TabularProc which replaces a column with a numeric categorical column. FillMissing is a TabularProc which replaces missing values with the median of the column, and creates a new boolean column that is set to True for any row where the value was missing. These two transforms are needed for nearly every tabular dataset you will use, so it&#39;s a good starting point for your data processing. . procs = [Categorify, FillMissing] . TabularPandas will also handle splitting into training vs validation datasets for us. . We need to be very careful about our validation set here. In particular we want to design it so that it is like the test set which Kaggle will use to judge the contest. . Recall the distinction between a validation set and a test set, as discussed in &lt;&gt;. A validation set is data which we hold back from training in order to ensure that the training process does not overfit on the training data. A test set is data which is held back even more deeply, from us ourselves, in order to ensure that we don&#39;t overfit on the validation data, as we explore various model architectures and hyperparameters.&lt;/p&gt; We don&#39;t get to see the test set. But we do want to define our validation data so that it has the same sort of relationship to the training data as the test set will have. . In some cases, just randomly choosing a subset of your data points will do that. This is not one of those cases, because it is a time series. . If you look at the date range represented in the test set, you will discover that it covers a six-month period from May 2012, which is later in time than any date in the training set. This is a good design, because the competition sponsor will want to ensure that a model is able to predict the future. But it means that if we are going to have a useful validation set, we also want the validation set to be later in time. The Kaggle training data ends in April 2012. So we will define a narrower training dataset which consists only of the Kaggle training data from before November 2011, and we define a validation set which is from after November 2011. . To do this we use np.where, a useful function which returns (as the first element of a tuple) the indices of all True values: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; cond = (df.saleYear&lt;2011) | (df.saleMonth&lt;10) train_idx = np.where( cond)[0] valid_idx = np.where(~cond)[0] splits = (list(train_idx),list(valid_idx)) . TabularPandas needs to be told which columns are continuous, and which are categorical. We can handle that automatically using the helper function cont_cat_split. . cont,cat = cont_cat_split(df, 1, dep_var=dep_var) . to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits) . A TabularPandas behaves a lot like a fastai Datasets object, including train and valid attributes. . len(to.train),len(to.valid) . (404710, 7988) . We can see that the data still is displayed as strings for categories... . to.show(3) . UsageBand fiModelDesc fiBaseModel fiSecondaryDesc fiModelSeries fiModelDescriptor ProductSize fiProductClassDesc state ProductGroup ProductGroupDesc Drive_System Enclosure Forks Pad_Type Ride_Control Stick Transmission Turbocharged Blade_Extension Blade_Width Enclosure_Type Engine_Horsepower Hydraulics Pushblock Ripper Scarifier Tip_Control Tire_Size Coupler Coupler_System Grouser_Tracks Hydraulics_Flow Track_Type Undercarriage_Pad_Width Stick_Length Thumb Pattern_Changer Grouser_Type Backhoe_Mounting Blade_Type Travel_Controls Differential_Type Steering_Controls saleIs_month_end saleIs_month_start saleIs_quarter_end saleIs_quarter_start saleIs_year_end saleIs_year_start SalesID_na MachineID_na ModelID_na datasource_na auctioneerID_na YearMade_na MachineHoursCurrentMeter_na saleYear_na saleMonth_na saleWeek_na saleDay_na saleDayofweek_na saleDayofyear_na saleElapsed_na SalesID MachineID ModelID datasource auctioneerID YearMade MachineHoursCurrentMeter saleYear saleMonth saleWeek saleDay saleDayofweek saleDayofyear saleElapsed SalePrice . 0 Low | 521D | 521 | D | #na# | #na# | #na# | Wheel Loader - 110.0 to 120.0 Horsepower | Alabama | WL | Wheel Loader | #na# | EROPS w AC | None or Unspecified | #na# | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | 2 Valve | #na# | #na# | #na# | #na# | None or Unspecified | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | Standard | Conventional | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | 1139246 | 999089 | 3157 | 121 | 3.0 | 2004 | 68.0 | 2006 | 11 | 46 | 16 | 3 | 320 | 1163635200 | 11.097410 | . 1 Low | 950FII | 950 | F | II | #na# | Medium | Wheel Loader - 150.0 to 175.0 Horsepower | North Carolina | WL | Wheel Loader | #na# | EROPS w AC | None or Unspecified | #na# | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | 2 Valve | #na# | #na# | #na# | #na# | 23.5 | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | Standard | Conventional | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | 1139248 | 117657 | 77 | 121 | 3.0 | 1996 | 4640.0 | 2004 | 3 | 13 | 26 | 4 | 86 | 1080259200 | 10.950807 | . 2 High | 226 | 226 | #na# | #na# | #na# | #na# | Skid Steer Loader - 1351.0 to 1601.0 Lb Operating Capacity | New York | SSL | Skid Steer Loaders | #na# | OROPS | None or Unspecified | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | Auxiliary | #na# | #na# | #na# | #na# | #na# | None or Unspecified | None or Unspecified | None or Unspecified | Standard | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | #na# | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | False | 1139249 | 434808 | 7009 | 121 | 3.0 | 2001 | 2838.0 | 2004 | 2 | 9 | 26 | 3 | 57 | 1077753600 | 9.210340 | . TK too big to fit . ...but the underlying items are all numeric: . to.items.head(3) . SalesID SalePrice MachineID ModelID ... saleDay_na saleDayofweek_na saleDayofyear_na saleElapsed_na . 0 1139246 | 11.097410 | 999089 | 3157 | ... | 1 | 1 | 1 | 1 | . 1 1139248 | 10.950807 | 117657 | 77 | ... | 1 | 1 | 1 | 1 | . 2 1139249 | 9.210340 | 434808 | 7009 | ... | 1 | 1 | 1 | 1 | . 3 rows × 79 columns . The conversion of categorical columns to numbers is done by simply replacing each unique level with a number. The numbers associated with the levels are chosen consecutively as they are seen in a column. So there&#39;s no particular meaning to the numbers in categorical columns after conversion. The exception is if you first convert a column to a pandas ordered category (as we did for ProductSize above), in which case the ordering you chose is used. We can see the mapping by looking at the classes attribute: . to.classes[&#39;ProductSize&#39;] . (#7) [&#39;#na#&#39;,&#39;Large&#39;,&#39;Large / Medium&#39;,&#39;Medium&#39;,&#39;Small&#39;,&#39;Mini&#39;,&#39;Compact&#39;] . Since it takes a minute or so to process the data to get to this point, we should save it - that way in the future we can continue our work from here without rerunning the previous steps. fastai provides a save method that uses Python&#39;s pickle system to save nearly any Python object. . (path/&#39;to.pkl&#39;).save(to) . To read this back later, you would type: . to = (path/&#39;to.pkl&#39;).load() . Now that all this preprocessing is done, we are ready to create a decision tree. . Creating the decision tree . We can read in our data (only needed if you&#39;re coming back to this notebook after a break, and don&#39;t want to recreate the TabularPandas object), and define our independent and dependent variables. . to = (path/&#39;to.pkl&#39;).load() . xs,y = to.train.xs,to.train.y valid_xs,valid_y = to.valid.xs,to.valid.y . Now that our data is all numeric, and there are no missing values, we can create a decision tree: . m = DecisionTreeRegressor(max_leaf_nodes=4) m.fit(xs, y); . To see what it&#39;s learned, we can display the tree. To keep it simple, we&#39;ve told sklearn to just create four leaf nodes. . draw_tree(m, xs, size=7, leaves_parallel=True, precision=2) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 Coupler_System ≤ 0.5 mse = 0.48 samples = 404710 value = 10.1 1 YearMade ≤ 1991.5 mse = 0.42 samples = 360847 value = 10.21 0&#45;&gt;1 True 2 mse = 0.12 samples = 43863 value = 9.21 0&#45;&gt;2 False 3 mse = 0.37 samples = 155724 value = 9.97 1&#45;&gt;3 4 ProductSize ≤ 4.5 mse = 0.37 samples = 205123 value = 10.4 1&#45;&gt;4 5 mse = 0.31 samples = 182403 value = 10.5 4&#45;&gt;5 6 mse = 0.17 samples = 22720 value = 9.62 4&#45;&gt;6 Understanding this picture is one of the best ways to understand decision trees. So we will start at the top, and explain each part step-by-step. . The top node represents the initial model before any splits have been done, when all the data is in one group. This is the simplest possible model. It is the result of asking zero questions. It always predict the value to be the average value of the whole dataset. In this case, we can see it predicts a value of 10.10 for the logarithm of the sales price. It gives a mean squared error of 0.48. The square root of this is 0.69. Remember that unless you see m_rmse or a root mean squared error then the value you are looking at is before taking the square root, so it is just the average of the square of the differences. We can also see that there are 404,710 auction records in this group — that is the total size of our training set. The final piece of information shown here is the decision criterion for the very first split that was found, which is to split based on the coupler_system column. . Moving down and to the left, this node shows us that there were 360,847 auction records for equipment where coupler_system was less than 0.5. The average value of our dependent variable in this group is 10.21. But moving down and to the right from the initial model would take us to the records where coupler_system was greater than 0.5. . The bottom row contains our leaf nodes, the nodes with no answers coming out of them, because there are no more questions to be answered. At the far right of this row is the node for coupler_system greater than 0.5, and we can see that the average value is 9.21. So we can see the decision tree algorithm did find a single binary decision which separated high value from low value auction results. Asking only about coupler_system predicts an average value of 9.21 vs 10.1. That&#39;s if we ask only one question. . Returning back to the top node after the first decision point, we can see that a second binary decision split has been made, based on asking whether YearMade is less than or equal to 1991.5. For the group where this is true (remember, this is now following two binary decisions, both coupler_system, and YearMade) the average value is 9.97, and there are 155,724 auction records in this group. For the group of auctions where this decision is false, the average value is 10.4, and there are 205,123 records. So again, we can see that the decision tree algorithm has successfully split our more expensive auction records into two more groups which differ in value significantly. . (TK AG: I think it would be useful here to have a figure which showed a circle or blob shape, which is carved by bisecting lines first into two groups, then into three, then four, as new bisections are introduced. This is a valuable intuition which we have not depicted.) . We can show the same information using Terence Parr&#39;s powerful dtreeviz library: . samp_idx = np.random.permutation(len(y))[:500] dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var, fontname=&#39;DejaVu Sans&#39;, scale=1.6, label_fontsize=10, orientation=&#39;LR&#39;) . G node4 leaf5 node4-&gt;leaf5 leaf6 node4-&gt;leaf6 node1 node1-&gt;node4 leaf3 node1-&gt;leaf3 leaf2 node0 node0-&gt;node1 &lt; node0-&gt;leaf2 &#8805; This shows a chart of the distribution of the data for each split point. We can clearly see that there&#39;s a problem with our YearMade data: there are bulldozers made in the year 1000, apparently! Presumably this is actually just a missing value code. In a decision tree, we can set any value that doesn&#39;t otherwise appear in the data as a missing value code. So for modelling purposes, &#39;1000&#39; is fine; but it makes visualization a bit hard to see, as shown above. So let&#39;s replace it with &#39;1950&#39;: . xs.loc[xs[&#39;YearMade&#39;]&lt;1900, &#39;YearMade&#39;] = 1950 valid_xs.loc[valid_xs[&#39;YearMade&#39;]&lt;1900, &#39;YearMade&#39;] = 1950 . After that change, the split is much clearer in the tree visualization, even although it doesn&#39;t actually change the result of the model in any significant way. This is a great example of how resilient decision trees are to data issues! . m = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs, y) dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var, fontname=&#39;DejaVu Sans&#39;, scale=1.6, label_fontsize=10, orientation=&#39;LR&#39;) . G node4 leaf5 node4-&gt;leaf5 leaf6 node4-&gt;leaf6 node1 node1-&gt;node4 leaf3 node1-&gt;leaf3 leaf2 node0 node0-&gt;node1 &lt; node0-&gt;leaf2 &#8805; Let&#39;s now have the decision tree algorithm build a bigger tree (i.e we are not passing in any stopping criteria such as max_leaf_nodes): . m = DecisionTreeRegressor() m.fit(xs, y); . We&#39;ll create a little function to check the root mean squared error (m_rmse) of our model, since that&#39;s how the competition was judged: . def r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6) def m_rmse(m, xs, y): return r_mse(m.predict(xs), y) . m_rmse(m, xs, y) . 0.0 . So, our model is perfect, right? Not so fast... remember we really need to check the validation set, to ensure we&#39;re not overfitting: . m_rmse(m, valid_xs, valid_y) . 0.337727 . Oops... it looks like we might be overfitting pretty badly. Here&#39;s why: . m.get_n_leaves(), len(xs) . (340909, 404710) . We&#39;ve got nearly as many leaf nodes as data points! That seems a little... over-enthusiastic. Indeed, sklearn&#39;s default settings allow it to continue splitting nodes until there is only one item in a leaf node. Let&#39;s change the stopping rule to tell sklearn to ensure every leaf node has at least 25 auctions: . A: Here&#39;s my intuition for an overfitting decision tree with more leaf nodes than data items: the childhood game of Twenty Questions. In that game, the chooser secretly imagines an object (like, &quot;our television set&quot;), and the guesser gets to pose twenty yes-or-no questions to try to guess the object (like &quot;is it bigger than a breadbox?&quot;). The guesser is not trying to predict a numerical value but just to identify a particular object out of the set of all imaginable objects. When your decision tree has more leafs than there are possible objects in your domain, then it is essentially a well-trained guesser. It has learned the sequence of questions needed to identify a particular data item in the training set, and it is &quot;predicting&quot; only by describing that item&#39;s value. This is a way of memorizing the training set, i.e., of overfitting. . m = DecisionTreeRegressor(min_samples_leaf=25) m.fit(to.train.xs, to.train.y) m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y) . (0.248562, 0.32368) . That looks much better. Let&#39;s check the number of leaves again: . m.get_n_leaves() . 12397 . Much more reasonable! . Building a decision tree is a good way to create a model of our data. It is very flexible, since it can clearly handle nonlinear relationships and interactions between variables. But we can see there is a fundamental compromise between how well it generalises (which we can achieve by creating small trees) and how accurate it is on the training set (which we can achieve by using large trees). . But, how do we get the best of both worlds? We&#39;ll show you right after we handle an important missing detail: how to handle categorical variables. . Categorical variables . This is unlike the situation with deep learning networks, where we one-hot encoded the variables and then fed them to an embedding layer. There, the embedding layer helped to discover the meaning of these variable levels, since each level of a categorical variable does not have a meaning on its own (unless we manually specified an ordering using pandas). So how can these untreated categorical variables do anything useful in a decision tree? For instance, how could something like a product code be used? . The short answer is: it just works! Think about a situation where there is one product code that is far more expensive at auction than any other one. In that case, any binary split will result in that one product code being in some group, and that group will be more expensive than the other group. Therefore, our simple decision tree building algorithm will choose that split. Later during training, the algorithm will be able to further split the subgroup which now contains the expensive product code. Over time, the tree will home in on that one expensive product. . It is also possible to use one hot encoding to replace a single categorical variable with multiple one hot encoded columns, where column represents a possible level of the variable. Pandas has a get_dummies method which does just that. . However, there is not really any evidence that such an approach improves the end result. So, we generally avoid it where possible, because it does end up making your dataset harder to work with. In 2019 this issue was explored in the paper Splitting on categorical predictors in random forests, which said: . : &quot;The standard approach for nominal predictors is to consider all (2^(k − 1) − 1) 2-partitions of the k predictor categories. However, this exponential relationship produces a large number of potential splits to be evaluated, increasing computational complexity and restricting the possible number of categories in most implementations. For binary classification and regression, it was shown that ordering the predictor categories in each split leads to exactly the same splits as the standard approach. This reduces computational complexity because only k − 1 splits have to be considered for a nominal predictor with k categories.&quot; . Building a decision tree is a good way to create a model of our data. It is very flexible, since it can clearly handle nonlinear relationships and interactions between variables. But we can see there is a fundamental compromise between how well it generalises (which we can achieve by creating small trees) and how accurate it is on the training set (which we can achieve by using large trees). . But, how do we get the best of both worlds? A solution is to use random forests. . Random forests . In 1994 Berkeley professor Leo Breiman, one year after his retirement, published a small technical report called Bagging Predictors, which turned out to be one of the most influential ideas in modern machine learning. The report began: . : &quot;Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions... The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests… show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.&quot; Here is the procedure that Breiman is proposing: . Randomly choose a subset of the rows of your data (i.e., &quot;bootstrap replicates of your learning set&quot;) | Train a model using this subset | Save that model, and then return to step one a few times | This will give you a number of trained models. To make a prediction, predict using all of the models, and then take the average of each of those model&#39;s predictions. | This procedure is known as &quot;bagging&quot;. It is based on a deep and important insight: although each of the models trained on a subset of data will make more errors than a model trained on the full dataset, those errors will not be correlated with each other. Different models will make different errors. The average of those errors, therefore, is: zero! So if we take the average of all of the models&#39; predictions, then we should end up with a prediction which gets closer and closer to the correct answer, the more models we have. This is an extraordinary result — it means that we can improve the accuracy of nearly any kind of machine learning algorithm by training it multiple times, each time on a different random subset of data, and average its predictions. . In 2001 Leo Breiman went on to demonstrate that this approach to building models, when applied to decision tree building algorithms, was particularly powerful. He went even further than just randomly choosing rows for each model&#39;s training, but also randomly selected from a subset of columns when choosing each split in each decision tree. He called this method the random forest. Today it is, perhaps, the most widely used and practically important machine learning method. . In essence a random forest is a model that averages the predictions of large number of decision trees, which are generated by randomly varying various parameters that specify what data is used to train the tree and other tree parameters. &quot;Bagging&quot; is a particular approach to &quot;ensembling&quot;, which refers to any approach that combines the results of multiple models together. Let&#39;s get started on creating our own random forest! . Creating a random forest . We can create a random forest just like we created a decision tree. Except now, we are also specifying parameters which indicate how many trees should be in the forest, how we should subset the data items (the rows), and how we should subset the fields (the columns). . In the function definition below, n_estimators defines the number of trees we want, and max_samples defines how many rows to sample for training each tree, while max_features defines how many columns to sample at each split point (where 0.5 means &quot;take half the total number of columns&quot;). We can also pass parameters for choosing when to stop splitting the tree nodes, effectively limiting the depth of tree, by including the same min_samples_leaf parameter we used in the last section. Finally, we pass n_jobs=-1 to tell sklearn to use all our CPUs to build the trees in parallel. By creating a little function for this, we can more quickly try different variations in the rest of this chapter. . def rf(xs, y, n_estimators=40, max_samples=200_000, max_features=0.5, min_samples_leaf=5, **kwargs): return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) . m = rf(xs, y); . Our validation RMSE is now much improved over our last result produced by the DecisionTreeRegressor, which made just one tree using all available data: . m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y) . (0.170896, 0.233502) . One of the most important properties of random forests is that they aren&#39;t very sensitive to the hyperparameter choices, such as max_features. You can set n_estimators to as high a number as you have time to train -- the more trees, the more accurate they will be. max_samples can often be left at its default, unless you have over 200,000 data points, in which case setting it to 200,000 will make it train faster, with little impact on accuracy. max_features=0.5, and min_samples_leaf=4 both tend to work well, although sklearn&#39;s defaults work well too. . The sklearn docs show an example of different max_features choices, with increasing numbers of trees. In the plot, the blue plot line uses the fewest features and the green line uses the most, since it uses all the features. As you can see in &lt;&gt;, the models with the lowest error result from using a subset of features but with a larger number of trees.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Error based on max features and # trees . To see the impact of n_estimators, let&#39;s get the predictions from each individual tree in our forest (these are in the estimators_ attribute): . preds = np.stack([t.predict(valid_xs) for t in m.estimators_]) . As you can see, preds.mean(0) gives the same results as our random forest: . r_mse(preds.mean(0), valid_y) . 0.233502 . question: Why does this give the same result as our random forest? . Here&#39;s how RMSE improves as we add more and more trees. As you can see, the improvement levels off quite a bit after around 30 trees: . plt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]); . Is our validation set worse than our training set because we&#39;re over-fitting, or because the validation set is for a different time period, or a bit of both? With the existing information we&#39;ve shown, we can&#39;t tell. However, random forests have a very clever trick called out-of-bag (OOB) error which can handle this (and more!) . Out-of-bag error . The idea is to calculate error on the training set, but only include the trees in the calculation of a row&#39;s error where that row was not included in training that tree. This allows us to see whether the model is over-fitting, without needing a separate validation set. . This also has the benefit of allowing us to see whether our model generalizes, even if we have such a small amount of data that we want to avoid removing items to create a validation set. The OOB predictions are available in the oob_prediction_ attribute. Note that we compare to training labels, since this is being calculated on the OOB trees on the training set. . A: My intuition for this is that, since every tree was trained with a different randomly selected subset of rows, out-of-bag error is a little like imagining that every tree therefore also has its own validation set. That validation set is simply the rows that were notselected for that tree&#39;s training. . r_mse(m.oob_prediction_, y) . 0.210686 . We can see that our OOB error is much lower than our validation set error. This means that something else is causing our error, in addition to normal generalization error. We&#39;ll discuss the reasons for this later in this chapter. . question: Make a list of reasons why this model&#39;s validation set error on this dataset might be worse than the OOB error. How could you test your hypotheses? . This is one way to interpret our model predictions, let&#39;s focus on more of those now. . Model interpretation . For tabular data, model interpretation is particularly important. For a given model, the things we are most likely to be interested in are: . How confident are we in our predictions using a particular row of data? | For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction? | Which columns are the strongest predictors, which can we ignore? | Which columns are effectively redundant with each other, for purposes of prediction? | How do predictions vary, as we vary these columns? | . As we will see, random forests are particularly well suited to answering these questions. Let&#39;s start with the first one! . Tree variance for prediction confidence . We saw how the model averages the individual tree predictions to get an overall prediction, that is, an estimate of the value. But how can we know the confidence of the estimate? One simple way is to use the standard deviation of predictions across the trees, instead of just the mean. This tells us the relative confidence of predictions. That is, for rows where trees give very different results, you would want to be more cautious of using those results, compared to cases where they are more consistent. . In the earlier section on creating a random forest, we saw how to get predictions over the validation set, using a Python list comprehension to do this for each tree in the forest: . preds = np.stack([t.predict(valid_xs) for t in m.estimators_]) . preds.shape . (40, 7988) . Now we have a prediction for every tree and every auction, for 160 trees and 7988 auctions in the validation set. . Using this we can get the standard deviation of the predictions over all the trees, for each auction: . preds_std = preds.std(0) . Here are the prediction standard deviations for the first 5 auctions, that is, the first 5 rows of the validation set: . preds_std[:5] . array([0.21529149, 0.10351274, 0.08901878, 0.28374773, 0.11977206]) . As you can see, the confidence of the predictions varies widely. For some auctions, there is a low standard deviation because the trees agree. For others, it&#39;s higher, as the trees don&#39;t agree. This is information that would be useful to use in a production setting; for instance, if you were using this model to decide what items to bid on at auction, a low-confidence prediction may cause you to look more carefully into an item before you made a bid. . Feature importance . It&#39;s not normally enough to just to know that a model can make accurate predictions -- we also want to know how it&#39;s making predictions. The most important way to see this is with feature importance. We can get these directly from sklearn&#39;s random forest, by looking in the feature_importances_ attribute. Here&#39;s a simple function we can use to pop them into a DataFrame and sort them: . def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) . The feature importances for our model show that the first few most important columns have a much higher importance score than the rest, with (not surprisingly) YearMade and ProductSize being at the top of the list: . fi = rf_feat_importance(m, xs) fi[:10] . cols imp . 69 YearMade | 0.182890 | . 6 ProductSize | 0.127268 | . 30 Coupler_System | 0.117698 | . 7 fiProductClassDesc | 0.069939 | . 66 ModelID | 0.057263 | . 77 saleElapsed | 0.050113 | . 32 Hydraulics_Flow | 0.047091 | . 3 fiSecondaryDesc | 0.041225 | . 31 Grouser_Tracks | 0.031988 | . 1 fiModelDesc | 0.031838 | . A plot of the feature importances shows the relative importances more clearly: . def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(fi[:30]); . The way these importances are calculated is quite simple yet elegant. The feature importance algorithm loops through each tree, and then recursively explores each branch. At each branch, it looks to see what feature was used for that split, and how much the model improves as a result of that split. The improvement (weighted by the number of rows in that group) is added to the importance score for that feature. This is added across all branches of all trees, and finally the scores are normalized such that they add to 1.0. . Removing low-importance variables . It seems likely that we could use just a subset of the columns by removing the variables of low-importance and still get good results. Let&#39;s try just keeping those with a feature importance greater than 0.005. . to_keep = fi[fi.imp&gt;0.005].cols len(to_keep) . 21 . We&#39;ll retrain our model, using just this subset of the columns. . xs_imp = xs[to_keep] valid_xs_imp = valid_xs[to_keep] . m = rf(xs_imp, y) . ...and here&#39;s the result: . m_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp, valid_y) . (0.181208, 0.232323) . Our accuracy is about the same, but we have far less columns to study: . len(xs.columns), len(xs_imp.columns) . (78, 21) . We&#39;ve found that generally the first step to improving a model is simplifying it. 78 columns are too many for us to study them all in depth! Furthermore, in practice often a simpler, more interpretable model is easier to roll out and maintain. . It also makes our feature importance plot easier to interpret. Let&#39;s look at it again: . plot_fi(rf_feat_importance(m, xs_imp)); . One thing that makes this harder to interpret is that there seem to be some variables with very similar meanings, for example ProductGroup and ProductGroupDesc. Let&#39;s try to remove redundent features. . Removing redundant features . cluster_columns(xs_imp) . . Note: The most similar pairs are found by calculating the rank correlation, which means that all the values are replaced with their rank (i.e. first, second, third, etc within the column), and then the correlation is calculated. (Feel free to skip over this minor detail though, since it&#8217;s not going to come up again in the book!) . In the chart above, you can see pairs of columns that were extremely similar as the ones that were merged together early, far away from the &quot;root&quot; of the tree at the left. Unsurprisingly, the fields ProductGroup and ProductGroupDesc were merged quite early, as were saleYear and saleElapsed, and as were fiModelDesc and fiBaseModel. These might be so closely correlated they are practically synonyms for each other. . Let&#39;s try removing some of these closely related features to see if the model can be simplified without impacting the accuracy. First, we create a function that quickly trains a random forest and returns the OOB score, by using a lower max_samples and higher min_samples_leaf . The score is a number returned by sklearn that is 1.0 for a perfect model, and 0.0 for a random model. (In statistics it&#39;s called R^2, although the details aren&#39;t important for this explanation). We don&#39;t need it to be very accurate--we&#39;re just going to use it to compare different models, based on removing some of the possibly redundant columns. . def get_oob(df): m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15, max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True) m.fit(df, y) return m.oob_score_ . Here&#39;s our baseline. . get_oob(xs_imp) . 0.8771039618198545 . Now we try removing each variable one at a time. . {c:get_oob(xs_imp.drop(c, axis=1)) for c in ( &#39;saleYear&#39;, &#39;saleElapsed&#39;, &#39;ProductGroupDesc&#39;,&#39;ProductGroup&#39;, &#39;fiModelDesc&#39;, &#39;fiBaseModel&#39;, &#39;Hydraulics_Flow&#39;,&#39;Grouser_Tracks&#39;, &#39;Coupler_System&#39;)} . {&#39;saleYear&#39;: 0.8759666979317242, &#39;saleElapsed&#39;: 0.8728423449081594, &#39;ProductGroupDesc&#39;: 0.877877012281002, &#39;ProductGroup&#39;: 0.8772503407182847, &#39;fiModelDesc&#39;: 0.8756415073829513, &#39;fiBaseModel&#39;: 0.8765165299438019, &#39;Hydraulics_Flow&#39;: 0.8778545895742573, &#39;Grouser_Tracks&#39;: 0.8773718142788077, &#39;Coupler_System&#39;: 0.8778016988955392} . Now let&#39;s try dropping multiple variables. We&#39;ll drop one from each of the tightly aligned pairs we noticed above. Let&#39;s see what that does. . to_drop = [&#39;saleYear&#39;, &#39;ProductGroupDesc&#39;, &#39;fiBaseModel&#39;, &#39;Grouser_Tracks&#39;] get_oob(xs_imp.drop(to_drop, axis=1)) . 0.8739605718147015 . Looking good! This is really not much worse than the model with all the fields. Let&#39;s create DataFrames without these columns, and save them: . xs_final = xs_imp.drop(to_drop, axis=1) valid_xs_final = valid_xs_imp.drop(to_drop, axis=1) . (path/&#39;xs_final.pkl&#39;).save(xs_final) (path/&#39;valid_xs_final.pkl&#39;).save(valid_xs_final) . We can load them back later with: . xs_final = (path/&#39;xs_final.pkl&#39;).load() valid_xs_final = (path/&#39;valid_xs_final.pkl&#39;).load() . Now we can check our RMSE again, to confirm it is still a similar accuracy. . m = rf(xs_final, y) m_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y) . (0.183263, 0.233846) . Tk add transition . Partial dependence . The two most important predictors are ProductSize and YearMade. We&#39;d like to understand the relationship between these predictors and sale price. It&#39;s a good idea to first check the count of values per category (provided by the Pandas value_counts method), to see how common each category is: . p = valid_xs_final[&#39;ProductSize&#39;].value_counts(sort=False).plot.barh() c = to.classes[&#39;ProductSize&#39;] plt.yticks(range(len(c)), c); . The largrest group is #na#, which is the label fastai applies to missing values. . Let&#39;s do the same thing for YearMade. However, since this is a numeric feature, we&#39;ll need to draw a histogram, which groups the year values into a few discrete bins: . ax = valid_xs_final[&#39;YearMade&#39;].hist() . Other than the special value 1950 which we used for coding missing year values, most of the data is after 1990. . Now we&#39;re ready to look at partial dependence plots. Partial dependence plots try to answer the question: if a row varied on nothing other than the feature in question, how would it impact the dependent variable? . For instance, how does YearMade impact sale price, all other things being equal? . To answer this question, we can&#39;t just take the average sale price for each YearMade. The problem with that approach is that many other things vary from year to year as well, such as which products are sold, how many products have air-conditioning, inflation, and so forth. So merely averaging over all the auctions that have the same YearMade would also capture the effect of how every other field also changed along with YearMade and how that overall change affected price. . Instead, what we do is replace every single value in the YearMade column with 1950, and then calculate the predicted sale price for every auction, and take the average over all auctions. Then we do the same for 1951, 1952, and so forth until our final year of 2011. This isolates the effect of only YearMade (even if it does so by averaging over some imagined records where we assign a YearMade value that might never actually exist alongside some other values). . A: If you are philosophically minded it is somewhat dizzying to contemplate the different kinds of hypotheticality that we are juggling to make this calculation. First, there&#39;s the fact that every prediction is hypothetical, because we are not noting empirical data. Second, there&#39;s the point that we&#39;re not merely interested in asking how would sale price change if we changed YearMade and everything else along with it. Rather, we&#39;re very specifically asking, how would sale price change in a hypothetical world where only YearMade changed. Phew! It is impressive that we can ask such questions. I recommend Judea Pearl&#39;s recent book on causality, The Book of Why, if you&#39;re interested in more deeply exploring formalisms for analyzing these subtleties. With these averages, we can then plot each of these years on the x-axis, versus each of the predictions on the Y axis. This, finally, is a partial dependence plot. Let&#39;s take a look: . from sklearn.inspection import plot_partial_dependence fig,ax = plt.subplots(figsize=(12, 4)) plot_partial_dependence(m, valid_xs_final, [&#39;YearMade&#39;,&#39;ProductSize&#39;], grid_resolution=20, ax=ax); . Looking first of all at the YearMade plot, and specifically at the section covering after 1990 (since as we noted this is where we have most of the data), we can see a nearly linear relationship between year and price. Remember that our dependent variable is after taking the logarithm, so this means that in practice there is a exponential increase in price. This is what we would expect: depreciation is generally recognised as being a multiplicative factor over time. So, for a given sale date, varying year made ought to show an exponential relationship with sale price. . The ProductSize partial plot is a bit concerning. It shows that the final group, which we saw before is for missing values, has the lowest price. To use this insight in practice, we would want to find out why it&#39;s missing so often, and what that means. Missing values can sometimes be useful predictors--it entirely depends on what causes them to be missing. Sometimes, however, it can show data leakage. . Data leakage . In the paper Leakage in Data Mining: Formulation, Detection, and Avoidance the authors introduce leakage as . : &quot;the introduction of information about the target of a data mining problem, which should not be legitimately available to mine from. A trivial example of leakage would be a model that uses the target itself as an input, thus concluding for example that &#39;it rains on rainy days&#39;. In practice, the introduction of this illegitimate information is unintentional, and facilitated by the data collection, aggregation and preparation process.&quot; They give as an example : &quot;a real-life business intelligence project at IBM where potential customers for certain products were identified, among other things, based on keywords found on their websites. This turned out to be leakage since the website content used for training had been sampled at the point in time where the potential customer has already become a customer, and where the website contained traces of the IBM products purchased, such as the word &#39;Websphere&#39; (e.g. in a press release about the purchase or a specific product feature the client uses).&quot; Data leakage is subtle and can take many forms. In particular, missing values often represent data leakage. . For instance, Jeremy competed in a Kaggle competition designed to predict which researchers would end up receiving research grants. The information was provided by a university, and included thousands of examples of research projects, along with information about the researchers involved, along with whether or not the grant was eventually accepted. The University hoped that they would be able to use models developed in this competition to help them rank which grant applications were most likely to succeed, so that they could prioritise their processing. . Jeremy used a random forest to model the data, and then used feature importance to find out which features were most predictive. He noticed three surprising things: . The model was able to correctly predict who would receive grants over 95% of the time | Apparently meaningless identifier columns were the most important predictors | The columns day of week and day of year were also highly predictive; for instance, the vast majority of grant applications dated on a Sunday were accepted, and many accepted grant applications were dated on January 1. | . For the identifier columns, a partial dependence plots showed that when the information was missing the grant was almost always rejected. It turned out that in practice, the University only filled out much of this information after a grant application was accepted. Often, for applications that were not accepted, it was just left blank. Therefore, this information was not something that was actually available at the time that the application was received, and would therefor not be available for a predictive model — it was data leakage. . In the same way, the final processing of successful applications was often done automatically as a batch at the end of the week, or the end of the year. It was this final processing date which ended up in the data, so again, this information, while predictive, was not actually available at the time that the application was received. . This example shows the most practical and simple approaches to identifying data leakage, which are to build a model, and then: . Check whether the accuracy of the model is too good to be true | Look for important predictors which don&#39;t make sense in practice | Look for partial dependence plot results which don&#39;t make sense in practice. | . Thinking back to our bear detector, this mirrors the advice that we also provided there — it is often a good idea to build a model first, and then do your data cleaning, rather than vice versa. The model can help you identify potentially problematic data issues. . TK Add transition . Tree interpreter . At the start of this section, we said that we wanted to be able to answer five questions: . How confident are we in our projections using a particular row of data? | For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction? | Which columns are the strongest predictors? | Which columns are effectively redundant with each other, for purposes of prediction? | How do predictions vary, as we vary these columns? | . We&#39;ve handled four of these already--so just one to go, which is: &quot;For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?&quot; To answer this question, we need to use the treeinterpreter library. We&#39;ll also use the waterfallcharts library to draw the chart of the results. . !pip install treeinterpreter !pip install waterfallcharts . We have already seen how to compute feature importances across the entire random forest. The basic idea was to look at the contribution of each variable towards improving the model, at each branch of every tree, and then to add up all of these contributions per variable. . We can do exactly the same thing, but for just a single row of data. For instance, let&#39;s say we are looking at some particular item at auction. Our model might predict that this item will be very expensive, and we want to know why. So we take that one row of data, and put it through the first decision tree, looking to see what split is used at each point throughout the tree. For each split, we see what the increase or decrease in the addiction is, compared to the parent node of the tree. We do this for every tree, and add up the total change in importance by split variable. . For instance, let&#39;s pick the first few rows of our validation set: . row = valid_xs_final.iloc[:5] . We can pass these to treeinterpreter: . prediction,bias,contributions = treeinterpreter.predict(m, row.values) . prediction is simply the prediction that the random forest makes. bias is the prediction based on simply taking the mean of the dependent variable (i.e. the model that is the root of every tree). contributions is the most interesting bit--it tells us the total change in predicition due to each of the independent variables. Therefore, the sum of contributions plus bias must equal the prediction, for each row. Let&#39;s look just at the first row: . prediction[0], bias[0], contributions[0].sum() . (array([9.98234598]), 10.104309759725059, -0.12196378442186026) . The clearest way to display the contributions is with a waterfall plot. This shows how each positive and negative contribution from all the independent variables sum up to create the final prediction, which is the right-hand column labeled &quot;net&quot; here: . waterfall(valid_xs_final.columns, contributions[0], threshold=0.08, rotation_value=45,formatting=&#39;{:,.3f}&#39;); . This kind of information is most useful in production, rather than during model development. You can use it to provide useful information to users of your data product about the underlying reasoning behind the predictions. . TK add a transition . Extrapolation and neural networks . TK add an introduction here before stacking header . The extrapolation problem . Let&#39;s consider the simple task of making predictions from 40 data points showing a slightly noisy linear relationship: . x_lin = torch.linspace(0,20, steps=40) y_lin = x_lin + torch.randn_like(x_lin) plt.scatter(x_lin, y_lin); . Although we only have a single independent variable, sklearn expects a matrix of independent variables, not a single vector. So we have to turn our vector into a matrix with one column. In other words, we have to change the shape from [40] to [40,1]. One way to do that is with the unsqueeze method, which adds a new unit axis to a tensor at the requested dimension: . xs_lin = x_lin.unsqueeze(1) x_lin.shape,xs_lin.shape . (torch.Size([40]), torch.Size([40, 1])) . A more flexible approach is to slice an array or tensor with the special value None, which introduces an additional unit axis at that location: . x_lin[:,None].shape . torch.Size([40, 1]) . We can now create a random forest for this data. We&#39;ll use only the first 30 rows to train the model: . m_lin = RandomForestRegressor().fit(xs_lin[:30],y_lin[:30]) . ...and we test the model on the full dataset. The blue dots are the training data, and the red dots are the predictions. . plt.scatter(x_lin, y_lin, 20) plt.scatter(x_lin, m_lin.predict(xs_lin), color=&#39;red&#39;, alpha=0.5); . We have a big problem! Our predictions outside of the domain that our training data covered are all too low. Have a think about why this is… . Remember, a random forest is just the average of the predictions of a number of trees. And a tree simply predicts the average value of the rows in a leaf. Therefore, a tree and a random forest can never predict values outside of the range of the training data. This is particularly problematic for data where there is a trend over time, such as inflation, and you wish to make predictions for a future time.. Your predictions will be systematically too low. . But the problem is actually more general than just time variables. Random forests are not able to extrapolate outside of the types of data you have seen, in a more general sense. That&#39;s why we need to make sure our validation set does not contain out of domain data. . Finding out of domain data . Sometimes it is hard to even know whether your test set is distributed in the same way as your training data or, if it is different, then what columns reflect that difference. There&#39;s actually a nice easy way to figure this out, which is to use a random forest! . But in this case we don&#39;t use a random forest to predict our actual dependent variable. Instead we try to predict whether a row is in the validation set, or the training set. To see this in action, let&#39;s combine our training and validation sets together, create a dependent variable which represents which dataset each row comes from, build a random forest using that data, and get its feature importance: . df_dom = pd.concat([xs_final, valid_xs_final]) is_valid = np.array([0]*len(xs_final) + [1]*len(valid_xs_final)) m = rf(df_dom, is_valid) rf_feat_importance(m, df_dom)[:6] . cols imp . 5 saleElapsed | 0.859446 | . 9 SalesID | 0.119325 | . 13 MachineID | 0.014259 | . 0 YearMade | 0.001793 | . 8 fiModelDesc | 0.001740 | . 11 Enclosure | 0.000657 | . This shows that there are three columns that are very different between training and validation set: saleElapsed, SalesID, and MachineID. saleElapsed is fairly obvious, since it&#39;s the number of days between the start of the dataset and each row, so it directly encodes the date. SalesID suggests that identifiers for auction sales might increment over time. MachineID suggests something similar might be happening for individual items sold in those auctions. . We&#39;ll try training the original RF model, removing each of these in turn, and also checking the baseline model RMSE: . m = rf(xs_final, y) print(&#39;orig&#39;, m_rmse(m, valid_xs_final, valid_y)) for c in (&#39;SalesID&#39;,&#39;saleElapsed&#39;,&#39;MachineID&#39;): m = rf(xs_final.drop(c,axis=1), y) print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y)) . orig 0.232795 SalesID 0.23109 saleElapsed 0.236221 MachineID 0.233492 . It looks like we should be able to remove SalesID and MachineID without losing any accuracy; let&#39;s check: . time_vars = [&#39;SalesID&#39;,&#39;MachineID&#39;] xs_final_time = xs_final.drop(time_vars, axis=1) valid_xs_time = valid_xs_final.drop(time_vars, axis=1) m = rf(xs_final_time, y) m_rmse(m, valid_xs_time, valid_y) . 0.231307 . Removing these variables has slightly improved the model&#39;s accuracy; but more importantly, it should make it more resilient over time, and easier to maintain and understand. We recommend that for all datasets you try building a model where your dependent variable is is_valid, like the above. It can often uncover subtle domain shift issues that you may otherwise miss. . One thing that might help in our case is to simply avoid using old data. Often, old data shows relationships that just aren&#39;t valid any more. Let&#39;s try just using the most recent few years of the data: . xs[&#39;saleYear&#39;].hist(); . filt = xs[&#39;saleYear&#39;]&gt;2004 xs_filt = xs_final_time[filt] y_filt = y[filt] . Here&#39;s the result of training on this subset: . m = rf(xs_filt, y_filt) m_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y) . (0.17768, 0.230631) . It&#39;s a tiny bit better, which shows that you shouldn&#39;t always just use your entire dataset; sometimes a subset can be better. . Let&#39;s see if using a neural network helps. . Using a neural network . We can use the same approach to build a neural network model. Let&#39;s first replicate the steps we took to set up the TabularPandas object: . df_nn = pd.read_csv(path/&#39;TrainAndValid.csv&#39;, low_memory=False) df_nn[&#39;ProductSize&#39;] = df_nn[&#39;ProductSize&#39;].astype(&#39;category&#39;) df_nn[&#39;ProductSize&#39;].cat.set_categories(sizes, ordered=True, inplace=True) df_nn[dep_var] = np.log(df_nn[dep_var]) df_nn = add_datepart(df_nn, &#39;saledate&#39;) . We can leverage the work we did to trim unwanted column in the random forest, by using the same set of columns for our neural network. . df_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]] . Categorical columns are handled very differently in neural networks, compared to decision tree approaches. As we have seen in &lt;&gt;, a great way to handle categorical variables is by using embeddings. In order to create embeddings, fastai needs to know which columns should be treated as categorical variables. It does this by comparing the number of distinct levels in the variable (this is known as the cardinality of the variable) to the max_card parameter. Anything lower than this is going to be treated as a categorical variable by fastai. Embedding sizes larger than 10,000 should generally only be used after you&#39;ve tested whether there are better ways to group the variable, so we&#39;ll use 9000 as our max_card.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; cont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var) . However, one variable that we absolutely do not want to treat as categorical is the saleElapsed variable. A categorical variable cannot, by definition, extrapolate outside the range of values that it has seen. But we want to be able to predict auction sale prices in the future. Therefore, we need to make this a continuous variable: . cont_nn.append(&#39;saleElapsed&#39;) cat_nn.remove(&#39;saleElapsed&#39;) . Let&#39;s take a look at the cardinality of each of our categorical variables that we have chosen so far: . df_nn_final[cat_nn].nunique() . YearMade 73 ProductSize 6 Coupler_System 2 fiProductClassDesc 74 ModelID 5281 Hydraulics_Flow 3 fiSecondaryDesc 177 fiModelDesc 5059 ProductGroup 6 Enclosure 6 fiModelDescriptor 140 Drive_System 4 Hydraulics 12 Tire_Size 17 dtype: int64 . The fact that there are two variables pertaining to the &quot;model&quot; of the equipment, both with similar very high cardinalities, suggests that they may contain similar, redundant information. Note that we would not necessarily see this in the dendrogram, since that relies on similar variables being sorted in the same order (that is, they need to have similarly named levels). Having a column with 5000 levels means needing a number 5000 columns in our embedding matrix, so this would be nice to avoid if possible. Let&#39;s see what the impact of removing one of these model columns has on the random forest: . xs_filt2 = xs_filt.drop(&#39;fiModelDescriptor&#39;, axis=1) valid_xs_time2 = valid_xs_time.drop(&#39;fiModelDescriptor&#39;, axis=1) m2 = rf(xs_filt2, y_filt) m_rmse(m, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y) . (0.176706, 0.230642) . There&#39;s minimal impact, so we will remove it as a predictor for our neural network. . cat_nn.remove(&#39;fiModelDescriptor&#39;) . We can create our TabularPandas object in the same way as when we created our random forest, with one very important addition: normalisation. A random forest does not need any normalisation--the tree building procedure cares only about the order of values in a variable, not at all about how they are scaled. But as we have seen, a neural network definitely does care about this. Therefore, we add the Normalize processor when we build our TabularPandas object. . procs_nn = [Categorify, FillMissing, Normalize] to_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var) . Tabular models and data don&#39;t generally require much GPU RAM, so we can use larger batch sizes. . dls = to_nn.dataloaders(1024) . As we&#39;ve discussed, it&#39;s a good idea to set y_range for regression models, so let&#39;s find the min and max of our dependent variable: . y = to_nn.train.y y.min(),y.max() . (8.465899897028686, 11.863582336583399) . We can now create the Learner to create this tabular model. As usual, we use the application-specific learner function, to take advantage of its application-customized defaults. We set the loss function to MSE, since that&#39;s what this competition uses. . By default, for tabular data fastai creates a neural network with two hidden layers, with 200 and 100 activations each, respectively. This works quite well for small datasets, but here we&#39;ve got quite a large dataset, so we increase the layer sizes to 500 and 250. . from fastai2.tabular.all import * . learn = tabular_learner(dls, y_range=(8,12), layers=[500,250], n_out=1, loss_func=F.mse_loss) . learn.lr_find() . (0.005754399299621582, 0.0002754228771664202) . There&#39;s no need to use fine_tune, so we&#39;ll train with 1-cycle for a few epochs and see how it looks... . learn.fit_one_cycle(5, 1e-2) . epoch train_loss valid_loss time . 0 | 0.069705 | 0.062389 | 00:11 | . 1 | 0.056253 | 0.058489 | 00:11 | . 2 | 0.048385 | 0.052256 | 00:11 | . 3 | 0.043400 | 0.050743 | 00:11 | . 4 | 0.040358 | 0.050986 | 00:11 | . We can use our r_mse function to compare to the random forest result we got earlier. . preds,targs = learn.get_preds() r_mse(preds,targs) . 0.2258 . It&#39;s quite a bit better than the random forest (although it took longer to train, and it&#39;s more fussy about hyperparameter tuning). . Before we move on, let&#39;s save our model in case we want to come back to it again later. . learn.save(&#39;nn&#39;) . TK add transition of make this an aside . fastai&#39;s Tabular classes . In fastai, a tabular model is simply a model which takes columns of continuous or categorical data, and predicts a category (a classification model) or a continuous value (a regression model). Categorical independent variables are passed through an embedding, and concatenated, as we saw in the neural net we used for collaborative filtering, and then continuous variables are concatenated as well. . The model created in tabular_learner is an object of class TabularModel. Take a look at the source for tabular_learner now (remember, that&#39;s tabular_learner?? in Jupyter). You&#39;ll see that like collab_learner, it first calls get_emb_sz to calculate appropriate embedding sizes (which you can override by using the emb_szs parameter, which is a dictionary containing any column names you want to set sizes for manually), and it sets a few other defaults. Other than that, it just creates the TabularModel, and passes that to TabularLearner (and note that TabularLearner is identical to Learner, except for a customized predict method). . That means that really all the work is happening in TabularModel, so take a look at the source for that now. With the exception of the BatchNorm1d and Dropout layers (which we&#39;ll be learning about shortly) you now have the knowledge required to understand this whole class. Take a look at the discussion of EmbeddingNN at the end of the last chapter. Recall that it passed n_cont=0 to TabularModel. We now can see why that was: because there are zero continuous variables (in fastai the n_ prefix means &quot;number of&quot;, and cont is an abbreviation for &quot;continuous&quot;). . Tk add transition . Ensembling . Think back to the original reasoning behind why random forests work so well: each tree has errors, but those errors are not correlated with each other, so the average of those errors should tend towards zero once there are enough trees. Similar reasoning could be used to consider averaging the predictions of models trained using different algorithms. . In our case, we have two very different models, trained using very different algorithms: a random forest, and a neural network. It would be reasonable to expect that the kinds of errors that each one makes would be quite different. Therefore, we might expect that the average of their predictions would be better than either one&#39;s individual predictions. . As we mentioned earlier in this chapter, the approach of combining multiple models&#39; predictions together is called ensembling. A random forest is itself an ensemble. But we can then include a random forest in another ensemble--an ensemble of the random forest and the neural network! Whilst it is not going to make the difference between a successful and unsuccessful modelling process, it can certainly add a nice little boost to any models that you have built. . One minor issue we have to be aware of is that our PyTorch model and our sklearn model create data of different types--PyTorch gives us a rank 2 tensor (i.e a column matrix), whereas numpy gives us a rank 1 array (a vector). squeeze() removes any unit axes from a tensor, and to_np converts it into a numpy array. . rf_preds = m.predict(valid_xs_time) ens_preds = (to_np(preds.squeeze()) + rf_preds) /2 . This gives us a better result than either model achieved on its own: . r_mse(ens_preds,valid_y) . 0.22291 . In fact, this result is better than any score shown on the Kaggle leaderboard. This is not directly comparable, however, because the Kaggle leaderboard uses a separate dataset that we do not have access to. Kaggle does not allow us to submit to this old competition, to find out how we would have gone, so we have no way to directly compare. But our results certainly look very encouraging! . There is another important approach to ensembling, called boosting, where we add models, instead of averaging them. . Boosting . So far our approach to ensembling has been to use bagging, which involves combining many models together by averaging them, where each model is trained on a different data subset. When this is applied to decision trees, this is called a random forest. . Here is how boosting works: . Train a small model which under fits your dataset | Calculate the predictions in the training set for this model | Subtract the predictions from the targets; these are called the &quot;residuals&quot;, and represent the error for each point in the training set | Go back to step one, but instead of using the original targets, use the residuals as the target for the training | Continue doing this until you reach some stopping criterion, such as a maximum number of trees, or you observe your validation set error getting worse. | . Using this approach, each new tree will be attempting to fit the error of all of the previous trees combined. Because we are continually creating new residuals, by subtracting the predictions of each new tree from the residuals from the previous tree, the residuals will get smaller and smaller. . To make predictions with an ensemble of boosted trees, we calculate the predictions from each tree, and then add them all together. There are many models following this basic approach, and many names for the same models! Gradient boosting machines (GBMs) and gradient boosted decision trees (GBDTs) are the terms you&#39;re most likely to come across, or you may see the names of specific libraries implementing these; at the time of writing, XGBoost is the most popular. . Note that, unlike random forests, there is nothing to stop us from overfitting. Using more trees in a random forest does not lead to overfitting, because each tree is independent of the others. But in a boosted ensemble, the more trees you have, the better the training error becomes, and eventually you will see overfitting on the validation set. . We are not going to go into details as to how to train a gradient boosted tree ensemble here, because the field is moving rapidly, and any guidance we give will almost certainly be outdated by the time you read this! As we write this, sklearn has just added a HistGradientBoostingRegressor class, which provides excellent performance. There are many hyperparameters to tweak for this class, and for all gradient boosted tree methods we have seen. Unlike random forests, gradient boosted trees are extremely sensitive to the choices of these hyperparameters. So in practice, most people will use a loop which tries a range of different hyperparameters, to find which works best. . TK add transition. Or maybe make this an aside? . Combining embeddings with other methods . The abstract of the entity embedding paper we mentioned at the start of this chapter states: &quot;the embeddings obtained from the trained neural network boost the performance of all tested machine learning methods considerably when used as the input features instead&quot;. It includes this very interesting table: . . This is showing the mean average percent error (MAPE) compared amongst four different modelling techniques, three of which we have already seen, along with &quot;KNN&quot; (K nearest neighbours), which is a very simple baseline method. The first numeric column contains the results using just the methods on the data provided in the competition; the second column shows what happens if you first train a neural network with categorical embeddings, and then use those categorical embeddings instead of the raw categorical columns in the model. As you see, in every case, the models are dramatically improved by using the embeddings, instead of the raw category. . This is a really important result, because it shows that you can get much of the performance improvement of a neural network, without actually having to use a neural network at all at inference time. You could just use an embedding, which is literally just an array lookup, along with a small decision tree ensemble. . These embeddings need not even be necessarily learned separately for each model or task in an organisation. Instead, once a set of embeddings are learned for some column for some task, they could be stored in a central place, and reused across multiple models. In fact, we know from private communication with other practitioners at large companies that this is already happening in many places. . Conclusion: our advice for tabular modeling . We have dicussed two approaches to tabular modelling: decision tree ensembles, and neural networks. And we have mentioned two different decision tree ensembles: random forests, and gradient boosting machines. Each is very effective, but each also has compromises: . Random forests are the easiest to train, because they are extremely resilient to hyperparameter choices, and require very little preprocessing. They are very fast to train, and should not overfit, if you have enough trees. But, they can be a little less accurate, especially if extrapolation is required, such as predicting future time periods . Gradient boosting machines in theory are just as fast to train as random forests, but in practice you will have to try lots of different hyperparameters. They can overfit. But they are often a little bit more accurate than random forests. . Neural networks take the longest time to train, and require extra preprocessing such as normalisation; this normalisation needs to be used at inference time as well. They can provide great results, and extrapolate well, but only if you are careful with your hyperparameters, and are careful to avoid overfitting. . We suggest starting your analysis with a random forest. This will give you a strong baseline, and you can be confident that it&#39;s a reasonable starting point. You can then use that model for feature selection and partial dependence analysis, to get a better understanding of your data. . From that foundation, you can try neural nets and GBMs, and if they give you significantly better results on your validation set in a reasonable amount of time, you can use them. If decision tree ensembles are working well for you, try adding the embeddings for the categorical variables to the data, and see if that helps your decision trees learn better. . Questionnaire . What is a continuous variable? | What is a categorical variable? | Provide 2 of the words that are used for the possible values of a categorical variable. | What is a &quot;dense layer&quot;? | How do entity embeddings reduce memory usage and speed up neural networks? | What kind of datasets are entity embeddings especially useful for? | What are the two main families of machine learning algorithms? | Why do some categorical columns need a special ordering in their classes? How do you do this in pandas? | Summarize what a decision tree algorithm does. | Why is a date different from a regular categorical or continuous variable, and how can you preprocess it to allow it to be used in a model? | Should you pick a random validation set in the bulldozer competition? If no, what kind of validation set should you pick? | What is pickle and what is it useful for? | How are mse, samples, and values calculated in the decision tree drawn in this chapter? | How do we deal with outliers, before building a decision tree? | How do we handle categorical variables in a decision tree? | What is bagging? | What is the difference between max_samples and max_features when creating a random forest? | If you increase n_estimators to a very high value, can that lead to overfitting? Why or why not? | What is out of bag error? | Make a list of reasons why a model&#39;s validation set error might be worse than the OOB error. How could you test your hypotheses? | How can you answer each of these things with a random forest? How do they work?: How confident are we in our projections using a particular row of data? | For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction? | Which columns are the strongest predictors? | How do predictions vary, as we vary these columns? | . | What&#39;s the purpose of removing unimportant variables? | What&#39;s a good type of plot for showing tree interpreter results? | What is the extrapolation problem? | How can you tell if your test or validation set is distributed in a different way to your training set? | Why do we make saleElapsed a continuous variable, even although it has less than 9000 distinct values? | What is boosting? | How could we use embeddings with a random forest? Would we expect this to help? | Why might we not always use a neural net for tabular modeling? | Further research . Pick a competition on Kaggle with tabular data (current or past) and try to adapt the techniques seen in this chapter to get the best possible results. Compare yourself to the private leaderboard. | Implement the decision tree algorithm in this chapter from scratch yourself, and try it on this dataset. | Use the embeddings from the neural net in this chapter in a random forest, and see if you can improve on the random forest results we saw. | Explain what each line of the source of TabularModel does (with the exception of the BatchNorm1d and Dropout layers). | &lt;/div&gt; . .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_tabular.html",
            "relUrl": "/2020/03/19/_tabular.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Chapter 7",
            "content": "[[chapter_sizing_and_tta]] Training a state-of-the-art model . This chapter introduces more advanced techniques for training an image classification model and get state-of-the-art results. You can skip it if you want to learn more about other applications of deep learning and come back to it later--nothing in this chapter will be assumed in later chapters. . We will look at powerful data augmentation techniques, the progressive resizing approach and test time augmentation. To show all of this, we are going to train a model from scratch (not transfer learning) using a subset of ImageNet called Imagenette. It contains ten very different categories from the original ImageNet dataset, making for quicker training when we want to experiment. . This is going to be much harder to do well than our previous datasets because we&#39;re using full-size, full-color images, which are photos of objects of different sizes, in different orientations, in different lighting, and so forth... So in this chapter we&#39;re going to introduce some important techniques for getting the most out of your dataset, especially when you&#39;re training from scratch, or transfer learning to a very different kind of dataset to what the pretrained model used. . Imagenette . When fast.ai first started there were three main datasets that people used for building and testing computer vision models: . ImageNet: 1.3 million images of various sizes around 500 pixels across, in 1000 categories, which took a few days to train | MNIST: 50,000 28x28 pixel greyscale handwritten digits | CIFAR10: 60,000 32x32 colour images in 10 classes | . The problem is that the small datasets didn&#39;t actually generalise effectively to the large ImageNet dataset. The approaches that worked well on ImageNet generally had to be developed and trained on ImageNet. This led to many people believing that only researchers with access to giant computing resources could effectively contribute to developing image classification algorithms. . We thought that seemed very unlikely to be true. We had never actually seen a study that showed that ImageNet happen to be exactly the right size, and that other datasets could not be developed which would provide useful insights. So we thought we would try to create a new dataset which researchers could test their algorithms on quickly and cheaply, but which would also provide insights likely to work on the full ImageNet dataset. . About three hours later we had created Imagenette. We selected 10 classes from the full ImageNet which look very different to each other. We hope that it would be possible to create a classifier that worked to recognise these classes quickly and cheaply. When we tried it out, we discovered we were right. We then tried out a few algorithmic tweaks to see how they impacted Imagenette, found some which worked pretty well, and tested them on ImageNet as well — we were very pleased to find that our tweaks worked well on ImageNet too! . There is an important message here: the dataset you get given is not necessarily the dataset you want; it&#39;s particularly unlikely to be the dataset that you want to do your development and prototyping in. You should aim to have an iteration speed of no more than a couple of minutes — that is, when you come up with a new idea you want to try out, you should be able to train a model and see how it goes within a couple of minutes. If it&#39;s taking longer to do an experiment, think about how you could cut down your dataset, or simplify your model, to improve your experimentation speed. The more experiments you can do, the better! . Let&#39;s get started with this dataset: . from fastai2.vision.all import * path = untar_data(URLs.IMAGENETTE) . First we&#39;ll get our dataset into a DataLoaders object, using the presizing trick we saw in &lt;&gt;:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; dblock = DataBlock(blocks=(ImageBlock(), CategoryBlock()), get_items=get_image_files, get_y=parent_label, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) dls = dblock.dataloaders(path, bs=64) . ...and do a training that will serve as a baseline: . model = xresnet50() learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.583403 | 2.064317 | 0.401792 | 01:03 | . 1 | 1.208877 | 1.260106 | 0.601568 | 01:02 | . 2 | 0.925265 | 1.036154 | 0.664302 | 01:03 | . 3 | 0.730190 | 0.700906 | 0.777819 | 01:03 | . 4 | 0.585707 | 0.541810 | 0.825243 | 01:03 | . That&#39;s a good baseline, since we are not using a pretrained model, but we can do better. When working with models that are being trained from scratch, or fine-tuned to a very different dataset to that used for the pretraining, there are additional techniques that are really important. In the rest of the chapter we&#39;ll consider some of the key approaches you&#39;ll want to be familiar with. The first one is normalizing your data. . Normalization . When training a model, it helps if your input data is normalized, that is, as a mean of 0 and a standard deviation of 1. But most images and computer vision libraries will use values between 0 and 255 for pixels, or between 0 and 1; in either case, your data is not going to have a mean of zero and a standard deviation of one. . Let&#39;s grab a batch of our data and look at those values, by averaging over all axes except for the channel axis, which is axis 1: . x,y = dls.one_batch() x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([0.4842, 0.4711, 0.4511], device=&#39;cuda:5&#39;), TensorImage([0.2873, 0.2893, 0.3110], device=&#39;cuda:5&#39;)) . As we expected, its mean and standard deviation is not very close to the desired values of zero and one. This is easy to do in fastai by adding the Normalize transform. This acts on a whole mini batch at once, so you can add it to the batch_tfms section of your data block. You need to pass to this transform the mean and standard deviation that you want to use; fastai comes with the standard ImageNet mean and standard deviation already defined. (If you do not pass any statistics to the Normalize transform, fastai will automatically calculate them from a single batch of your data.) . Let&#39;s add this transform (using imagenet_stats as Imagenette is a subset of ImageNet) and have a look at one batch now: . def get_dls(bs, size): dblock = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, get_y=parent_label, item_tfms=Resize(460), batch_tfms=[*aug_transforms(size=size, min_scale=0.75), Normalize.from_stats(*imagenet_stats)]) return dblock.dataloaders(path, bs=bs) . dls = get_dls(64, 224) . x,y = dls.one_batch() x.mean(dim=[0,2,3]),x.std(dim=[0,2,3]) . (TensorImage([-0.0787, 0.0525, 0.2136], device=&#39;cuda:5&#39;), TensorImage([1.2330, 1.2112, 1.3031], device=&#39;cuda:5&#39;)) . Let&#39;s check how normalization helps training our model here: . model = xresnet50() learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.632865 | 2.250024 | 0.391337 | 01:02 | . 1 | 1.294041 | 1.579932 | 0.517177 | 01:02 | . 2 | 0.960535 | 1.069164 | 0.657207 | 01:04 | . 3 | 0.730220 | 0.767433 | 0.771845 | 01:05 | . 4 | 0.577889 | 0.550673 | 0.824496 | 01:06 | . Although it only helped a little here, normalization becomes especially important when using pretrained models. The pretrained model only knows how to work with data of the type that it has seen before. If the average pixel was zero in the data it was trained with, but your data has zero as the minimum possible value of a pixel, then the model is going to be seeing something very different to what is intended! . This means that when you distribute a model, you need to also distribute the statistics used for normalization, since anyone using it for inference, or transfer learning, will need to use the same statistics. By the same token, if you&#39;re using a model that someone else has trained, make sure you find out what normalization statistics they used, and match them. . We didn&#39;t have to handle normalization in previous chapters because when using a pretrained model through cnn_learner, the fastai library automatically adds the proper Normalize transform; the model has been pretrained with certain statistics in Normalize (usually coming from the ImageNet dataset), so the library can fill those for you. Note that this only applies with pretrained models, which is why we need to add it manually here, when training from scratch. . All our training up until now have been done at size 224. We could have begun training at a smaller size before going to that. This is called progressive resizing. . Progressive resizing . When fast.ai and its team of students won the DAWNBench competition, one of the most important innovations was something very simple: start training using small images, and end training using large images. By spending most of the epochs training with small images, training completed much faster. By completing training using large images, the final accuracy was much higher. We call this approach progressive resizing. . jargon: progressive resizing: Gradually using larger and larger images as you train. . As we have seen, the kinds of features that are learned by convolutional neural networks are not in any way specific to the size of the image — early layers find things like edges and gradients, and later layers may find things like noses and sunsets. So, when we change image size in the middle of training, it doesn&#39;t mean that we have two find totally different parameters for our model. . But clearly there are some differences between small images and big ones, so we shouldn&#39;t expect our model to continue working exactly as well, with no changes at all. Does this remind you of something? When we developed this idea, it reminded us of transfer learning! We are trying to get our model to learn to do something a little bit different to what it has learned to do before. Therefore, we should be able to use the fine_tune method after we resize our images. . There is an additional benefit to progressive resizing: it is another form of data augmentation. Therefore, you should expect to see better generalisation of your models that are trained with progressive resizing. . To implement progressive resizing it is most convenient if you first create a get_dls function which takes an image size and a batch size, and returns your DataLoaders: . Now you can create your DataLoaders with a small size, and fit_one_cycle in the usual way, for a few less epochs than you might otherwise do: . dls = get_dls(128, 128) learn = Learner(dls, xresnet50(), loss_func=CrossEntropyLossFlat(), metrics=accuracy) learn.fit_one_cycle(4, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.902943 | 2.447006 | 0.401419 | 00:30 | . 1 | 1.315203 | 1.572992 | 0.525765 | 00:30 | . 2 | 1.001199 | 0.767886 | 0.759149 | 00:30 | . 3 | 0.765864 | 0.665562 | 0.797984 | 00:30 | . Then you can replace the DataLoaders inside the Learner, and fine_tune: . learn.dls = get_dls(64, 224) learn.fine_tune(5, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.985213 | 1.654063 | 0.565721 | 01:06 | . epoch train_loss valid_loss accuracy time . 0 | 0.706869 | 0.689622 | 0.784541 | 01:07 | . 1 | 0.739217 | 0.928541 | 0.712472 | 01:07 | . 2 | 0.629462 | 0.788906 | 0.764003 | 01:07 | . 3 | 0.491912 | 0.502622 | 0.836445 | 01:06 | . 4 | 0.414880 | 0.431332 | 0.863331 | 01:06 | . As you can see, we&#39;re getting much better performance, and the initial training on small images was much faster on each epoch. . You can repeat the process of increasing size and training more epochs as many times as you like, for as big an image as you wish--but of course, you will not get any benefit by using an image size larger than the size of your images on disk. . Note that for transfer learning, progressive resizing may actually hurt performance. This would happen if your pretrained model was quite similar to your transfer learning task and dataset, and was trained on similar sized images, so the weights don&#39;t need to be changed much. In that case, training on smaller images may damage the pretrained weights. . On the other hand, if the transfer learning task is going to be on images that are of different sizes, shapes, or style to those used in the pretraining tasks, progressive resizing will probably help. As always, the answer to &quot;does it help?&quot; is &quot;try it!&quot;. . Another thing we could try is applying data augmentation to the validation set: up until now, we have only applied it on the training set and the validation set always gets the same images. But maybe we could try to make predictions for a few augmented versions of the validation set and average them. This is called test time augmentation. . Test time augmentation . We have been using random cropping as a way to get some useful data augmentation, which leads to better generalisation, and results in a need for less training data. When we use random cropping, fastai will automatically use centre-cropping for the validation set — that is, it will select the largest square area it can in the centre of the image, such that it does not go past the image edges. . This can often be problematic. For instance, in a multi-label dataset sometimes there are small objects towards the edges of an image; these could be entirely cropped out by the centre cropping. Even for datasets such as the pet breed classification data we&#39;re working on now, it&#39;s possible that some critical feature necessary for identifying the correct breed, such as the colour of the nose, could be cropped out. . One solution to this is to avoid random cropping entirely. Instead, we could simply squish or stretch the rectangular images to fit into a square space. But then we miss out on a very useful data augmentation, and we also make the image recognition more difficult for our model, because it has to learn how to recognise squished and squeezed images, rather than just correctly proportioned images. . Another solution is to not just centre crop for validation, but instead to select a number of areas to crop from the original rectangular image, pass each of them through our model, and take the maximum or average of the predictions. In fact, we could do this not just for different crops, but for different values across all of our test time augmentation parameters. This is known as test time augmentation (TTA). . jargon: test time augmentation (TTA): during inference or validation, creating multiple versions of each image, using data augmentation, and then taking the average or maximum of the predictions for each augmented version of the image . TK pic of TTA . Depending on the dataset, test time augmentation can result in dramatic improvements in accuracy. It does not change the time required to train at all, but will increase the amount of time for validation or inference by the number of test time augmented images requested. By default, fastai will use the unaugmented centre crop image, plus four randomly augmented images. . You can pass any DataLoader to fastai&#39;s tta method; by default, it will use your validation set: . preds,targs = learn.tta() accuracy(preds, targs).item() . 0.8737863898277283 . As we can see, using TTA gives us good a boost of performance, with no additional training required. However, it does make inference slower--if you&#39;re averaging 5 images for TTA, inference will be 5x slower. . Data augmentation helps train better models as we saw. Let&#39;s now focus on a new data augmentation technique called Mixup. . Mixup . Mixup, introduced in the 2017 paper mixup: Beyond Empirical Risk Minimization, is a very powerful data augmentation technique which can provide dramatically higher accuracy, especially when you don&#39;t have much data, and don&#39;t have a pretrained model that was trained on data similar to your dataset. The paper explains: &quot;While data augmentation consistently leads to improved generalization, the procedure is dataset-dependent, and thus requires the use of expert knowledge.&quot; For instance, it&#39;s common to flip images as part of data augmentation, but should you flip only horizontally, or also vertically? The answer is that it depends on your dataset. In addition, if flipping (for instance) doesn&#39;t provide enough data augmentation for you, you can&#39;t &quot;flip more&quot;. It&#39;s helpful to have data augmentation techniques where you can &quot;dial up&quot; or &quot;dial down&quot; the amount of data augmentation, to see what works best for you. . Mixup works as follows, for each image: . Select another image from your dataset at random | Pick a weight at random | Take a weighted average (using the weight from step 2) of the selected image with your image; this will be your independent variable | Take a weighted average (with the same weight) of this image&#39;s labels with your image&#39;s labels; this will be your dependent variable | In pseudo-code, we&#39;re doing (where t is the weight for our weighted average): . image2,target2 = dataset[randint(0,len(dataset)] t = random_float(0.5,1.0) new_image = t * image1 + (1-t) * image2 new_target = t * target1 + (1-t) * target2 . For this to work, our targets need to be one-hot encoded. The paper describes this using these equations (where $ lambda$ is the same as t in our code above): . An excerpt from the Mixup paper . Sidebar: Papers and math . We&#39;re going to be looking at more and more research papers from here on in the book. Now that you have the basic jargon, you might be surprised to discover how much of them you can understand, with a little practice! One issue you&#39;ll notice is that greek letters, such as $ lambda$, appear in most papers. It&#39;s a very good idea to learn the names of all the greek letters, since otherwise it&#39;s very hard to read the papers to yourself, and remember them, and it&#39;s also hard to read code based on them (since code often uses the name of the greek letter spelled out, such as lambda). . The bigger issue with papers is that they use math, instead of code, to explain what&#39;s going on. If you don&#39;t have much of a math background, this will likely be intimidating and confusing at first. But remember: what is being shown in the math, is something that will be implemented in code. It&#39;s just another way of talking about the same thing! After reading a few papers, you&#39;ll pick up more and more of the notation. If you don&#39;t know what a symbol is, try looking it up on Wikipedia&#39;s list of mathematical symbols or draw it on Detexify which (using machine learning!) will find the name of your hard-drawn symbol. Then you can search online for that name to find out what it&#39;s for. . End sidebar . Here&#39;s what it looks like when we take a linear combination of images, as done in Mixup: . The third image is built by adding 0.3 times the first one and 0.7 times the second. In this example, should the model predict church? gas station? The right answer is 30% church and 70% gas station since that&#39;s what we&#39;ll get if we take the linear combination of the one-hot encoded targets. For instance, if church has for index 2 and gas station as for index 7, the one-hot-encoded representations are . [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] and [0, 0, 0, 0, 0, 0, 0, 1, 0, 0] . (since we have ten classes in total) so our final target is . [0, 0, 0.3, 0, 0, 0, 0, 0.7, 0, 0] . This all done for us inside fastai by adding a Callback to our Learner. Callbacks are what is used inside fastai to inject custom behavior in the training loop (like a learning rate schedule, or training in mixed precision). We&#39;ll be learning all about callbacks, including how to make your own, in &lt;&gt;. For now, all you need to know is that you use the cbs parameter to Learner to pass callbacks.&lt;/p&gt; Here is how you train a model with Mixup: . model = xresnet50() learn = Learner(dls, model, loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=Mixup) learn.fit_one_cycle(5, 3e-3) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; So what happens if we train a model where our data is &quot;mixed up&quot; in this way? Clearly, it&#39;s going to be harder to train, because it&#39;s harder to see what&#39;s in each image. And the model has to predict two labels per image, rather than just one, as well as figuring out how much each one is weighted. Overfitting seems less likely to be a problem, because we&#39;re not showing the same image each epoch, but are instead showing a random combination of two images. . Mixup requires far more epochs to train to a better accuracy, compared to other augmentation approaches we&#39;ve seen. You can try training Imagenette with and without Mixup by using the examples/train_imagenette.py script in the fastai repo. At the time of writing, the leaderboard in the Imagenette repo is showing that mixup is used for all leading results for trainings of &gt;80 epochs, and for few epochs Mixup is not being used. This is inline with our experience of using Mixup too. . One of the reasons that mixup is so exciting is that it can be applied to types of data other than photos. In fact, some people have even shown good results by using mixup on activations inside their model, not just on inputs--these allows Mixup to be used for NLP and other data types too. . There&#39;s another subtle issue that Mixup deals with for us, which is that it&#39;s not actually possible with the models we&#39;ve seen before for our loss to ever be perfect. The problem is that our labels are ones and zeros, but softmax and sigmoid never can equal one or zero. So when we train our model, it causes it to push our activations ever closer to zero and one, such that the more epochs we do, the more extreme our activations become. . With Mixup, we no longer have that problem, because our labels will only be exactly one or zero if we happen to &quot;mix&quot; with another image of the same class. The rest of the time, our labels will be a linear combination, such as the 0.7 and 0.3 we got in the church and gas station example above. . One issue with this, however, is that Mixup is &quot;accidentally&quot; making the labels bigger than zero, or smaller than one. That is to say, we&#39;re not explicitly telling our model that we want to change the labels in this way. So if we want to change to make the labels closer, or further away, from zero and one, we have to change the amount of Mixup--which also changes the amount of data augmentation, which might not be what we want. There is, however, a way to handle this more directly, which is to use label smoothing. . Label smoothing . In the theoretical expression of the loss, in classification problems, our targets are one-hot encoded (in practice we tend to avoid doing it to save memory, but what we compute is the same loss as if we had used one-hot encoding). That means the model is trained to return 0 for all categories but one, for which it is trained to return 1. Even 0.999 is not good enough, the model will get gradients and learn to predict activations that are even more confident. This encourages overfitting and gives you at inference time a model that is not going to give meaningful probabilities: it will always say 1 for the predicted category even if it&#39;s not too sure, just because it was trained this way. . It can become very harmful if your data is not perfectly labeled. In the bear classifier we studied in &lt;&gt;, we saw that some of the images were mislabeled, or contained two different kinds of bears. In general, your data will never be perfect. Even if the labels were manually produced by humans, they could make mistakes, or have differences of opinions on images harder to label.&lt;/p&gt; Instead, we could replace all our 1s by a number a bit less than 1, and our 0s by a number a bit more than 0, and then train. This is called label smoothing. By encouraging your model to be less confident, label smoothing will make your training more robust, even if there is mislabeled data, and will produce a model that generalizes better at inference. . This is how label smoothing works in practice: we start with one-hot encoded labels, then replace all zeros by $ frac{ epsilon}{N}$ (that&#39;s the greek letter epsilon, which is what was used in the paper which introduced label smoothing, and is used in the fastai code) where $N$ is the number of classes and $ epsilon$ is a parameter (usually 0.1, which would mean we are 10% unsure of our labels). Since you want the labels to add up to 1, replace the 1 by $1- epsilon + frac{ epsilon}{N}$. This way, we don&#39;t encourage the model to predict something overconfident: in our Imagenette example where we have 10 classes, the targets become something like: . [0.01, 0.01, 0.01, 0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01] . (here for a target that corresponds to the index 3). In practice, we don&#39;t want to one-hot encode the labels, and fortunately we won&#39;t need too (the one-hot encoding is just good to explain what label smoothing is and visualize it). . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Sidebar: Label smoothing, the paper . Here is how the reasoning behind label smoothing was explained in the paper: . &quot;This maximum is not achievable for finite $z_k$ but is approached if $z_y gg z_k$ for all $k neq y$ -- that is, if the logit corresponding to the ground-truth label is much great than all other logits. This, however, can cause two problems. First, it may result in over-fitting: if the model learns to assign full probability to the ground-truth label for each training example, it is not guaranteed to generalize. Second, it encourages the differences between the largest logit and all others to become large, and this, combined with the bounded gradient $ frac{ partial ell}{ partial z_k}$, reduces the ability of the model to adapt. Intuitively, this happens because the model becomes too confident about its predictions.&quot; . Let&#39;s practice our paper reading skills to try to interpret this. &quot;This maximum&quot; is refering to the previous section of the paper, which talked about the fact that 1 is the value of the label for the positive class. So any value (except infinity) can&#39;t result in 1 after sigmoid or softmax. In a paper, you won&#39;t normally see &quot;any value&quot; written, but instead it would get a symbol; in this case, it&#39;s $z_k$. This is helpful in a paper, because it can be refered to again later, and the reader knows what value is being discussed. . The it says: $z_y gg z_k$ for all $k neq y$. In this case, the paper immediately follows with &quot;that is...&quot;, which is handy, because you can just read the English instead of the math. In the math, the $y$ is refering to the target ($y$ is defined earlier in the paper; sometimes it&#39;s hard to find where symbols are defined, but nearly all papers will define all their symbols somewhere), and $z_y$ is the activation corresponding to the target. So to get close to 1, this activation needs to be much higher than all the others for that prediction. . Next up is &quot;if the model learns to assign full probability to the ground-truth label for each training example, it is not guaranteed to generalize&quot;. This is saying that making $z_y$ really big means we&#39;ll need large weights and large activations throughout our model. Large weights lead to &quot;bumpy&quot; functions, where a small change in input results in a big change to predictions. This is really bad for generalization, because it means just one pixel changing a bit could change our prediction entirely! . Finally, we have &quot;it encourages the differences between the largest logit and all others to become large, and this, combined with the bounded gradient $ frac{ partial ell}{ partial z_k}$, reduces the ability of the model to adapt&quot;. The gradient of cross entropy, remember, is basically output-target, and both output and target are between zero and one. So the difference is between -1 and 1, which is why the paper says the gradient is &quot;bounded&quot; (it can&#39;t be infinite). Therefore our SGD steps are bounded too. &quot;Reduces the ability of the model to adapt&quot; means that it is hard for it to be updated in a transfer learning setting. This follows because the difference in loss due to incorrect predictions is unbounded, but we can only take a limited step each time. . End sidebar . To use it in practice, we just have to change the loss function in our call to Learner: . model = xresnet50() learn = Learner(dls, model, loss_func=LabelSmoothingCrossEntropy(), metrics=accuracy) learn.fit_one_cycle(5, 3e-3) . Like Mixup, you won&#39;t generally see significant improvements from label smoothing until you train more epochs. Try it yourself and see: how many epochs do you have to train before label smoothing shows an improvement? . Conclusion . You have now seen everything you need to train a state-of-the-art model in computer vision, whether from scratch or using transfer learning. Now all you have to do is experiment on your own problems! See if training longer with Mixup and/or label smoothing avoids overfitting and gives you better results. Try progressive resizing, and test time augmentation. . Most importantly, remember that if your dataset is big, there is no point prototyping on the whole thing. Find a small subset that is representative of the whole, like we did with Imagenette, and experiment on it. . In the next three chapters, we will look at the other applications directly supported by fastai: collaborative filtering, tabular and text. We will go back to computer vision in the next section of the book, with a deep dive in convolutional neural networks in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Questionnaire . What is the difference between ImageNet and Imagenette? When is it better to experiment on one versus the other? | What is normalization? | Why didn&#39;t we have to care about normalization when using a pretrained model? | What is progressive resizing? | Implement progressive resizing in your own project. Did it help? | What is test time augmentation? How do you use it in fastai? | Is using TTA at inference slower or faster than regular inference? Why? | What is Mixup? How do you use it in fastai? | Why does Mixup prevent the model from being too confident? | Why does a training with Mixup for 5 epochs end up worse than a training without Mixup? | What is the idea behind label smoothing? | What problems in your data can label smoothing help with? | When using label smoothing with 5 categories, what is the target associated with the index 1? | What is the first step to take when you want to prototype quick experiments on a new dataset. | Further research . Use the fastai documentation to build a function that crops an image to a square in the four corners, then implement a TTA method that averages the predictions on a center crop and those four crops. Did it help? Is it better than the TTA method of fastai? | Find the Mixup paper on arxiv and read it. Pick one or two more recent articles introducing variants of Mixup and read them, then try to implement them on your problem. | Find the script training Imagenette using Mixup and use it as an example to build a script for a long training on your own project. Execute it and see if it helped. | Read the sidebar on the math of label smoothing, and look at the relevant section of the original paper, and see if you can follow it. Don&#39;t be afraid to ask for help! | &lt;/div&gt; .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_sizing_and_tta.html",
            "relUrl": "/2020/03/19/_sizing_and_tta.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Chapter 14",
            "content": "[[chapter_resnet]] Resnets . In this chapter, we will build on top of the CNNs (Convolutional Neural Networks) introduced in the previous chapter and explain to you the ResNet (for residual network) architecture. It was introduced in 2015 in this article and is by far the most used model architecture nowadays. More recent developments in image models almost always use the same trick of residual connections, and most of the time, they are just a tweak of the original ResNet. . We will first show you the basic ResNet as it was first designed, then explain to you what modern tweaks make it more performamt. But first, we will need a problem a little bit more difficult than the MNIST dataset, since we are already close to 100% accuracy with a regular CNN on it. . Going back to Imagenette . It&#39;s going to be tough to judge any improvement we do to our models when we are already at an accuracy that is as high as we saw on MNIST in the previous chapter, so we will tackle a tougher image classification problem by going back to Imagenette. We&#39;ll stick with small images to keep things reasonably fast. . Let&#39;s grab the data--we&#39;ll use the already-resized 160px version to make things faster still, and will random crop to 128px: . def get_data(url, presize, resize): path = untar_data(url) return DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(valid_name=&#39;val&#39;), get_y=parent_label, item_tfms=Resize(presize), batch_tfms=[*aug_transforms(min_scale=0.5, size=resize), Normalize.from_stats(*imagenet_stats)], ).dataloaders(path, bs=128) . dls = get_data(URLs.IMAGENETTE_160, 160, 128) . dls.show_batch(max_n=4) . When we looked at MNIST we were dealing with 28 x 28 pixel images. For Imagenette we are going to be training with 128 x 128 pixel images. Later on we would like to be able to use larger images as well — at least as big as 224 x 224 pixels, the ImageNet standard. Do you recall how we managed to get a single vector of activations for each image out of the MNIST convolutional neural network? . The approach we used was to ensure that there were enough stride two convolutions such that the final layer would have a grid size of one. Then we just flattened out the unit axes that we ended up with, to get a vector for each image (so a matrix of activations for a mini batch). We could do the same thing for Imagenette, but that&#39;s going to cause two problems: . We are going to need lots of stride two layers to make our grid one by one at the end — perhaps more than we would otherwise choose | The model will not work on images of any size other than the size we originally trained on. | . One approach to dealing with the first of these issues would be to flatten the final convolutional layer in a way that handles a grid size other than one by one. That is, we could simply flatten a matrix into a vector as we have done before, by laying out each row after the previous row. In fact, this is the approach that convolutional neural networks up until 2013 nearly always did. The most famous is the 2013 ImageNet winner VGG, still sometimes used today. But there was another problem with this architecture: not only does it not work with images other than those of the same size as the training set, but it required a lot of memory, because flattening out the convolutional create resulted in many activations being fed into the final layers. Therefore, the weight matrices of the final layers were enormous. . This problem was solved through the creation of fully convolutional networks. The trick in fully convolutional networks is to take the average of activations across a convolutional grid. In other words, we can simply use this function: . def avg_pool(x): return x.mean((2,3)) . As you see, it is taking the mean over the X and Y axes. This function will always convert a grid of activations into a single activation per image. PyTorch provides a slightly more versatile module called nn.AdaptiveAvgPool2d, which averages a grid of activations into whatever sized destination you require (although we nearly always use the size of one). . A fully convolutional network, therefore, has a number of convolutional layers, some of which will be stride two, at the end of which is an adaptive average pooling layer, a flatten layer to remove the unit axes, and finally a linear layer. Here is our first fully convolutional network: . def block(ni, nf): return ConvLayer(ni, nf, stride=2) def get_model(): return nn.Sequential( block(3, 16), block(16, 32), block(32, 64), block(64, 128), block(128, 256), nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(256, dls.c)) . We&#39;re going to be replacing the implementation of block in the network with other variants in a moment, which is why we&#39;re not calling it conv any more. We&#39;re saving some time by taking advantage of fastai&#39;s ConvLayer that already provides the functionality of conv from the last chapter (plus a lot more!) . stop: Consider this question: Would this approach makes sense for an optical character recognition (OCR) problem such as MNIST? We see the vast majority of practitioners tackling OCR and similar problems tend to use fully convolutional networks, because that&#39;s what nearly everybody learns nowadays. But it really doesn&#39;t make any sense! You can&#39;t decide whether, for instance, whether a number is a &quot;3&quot; or an &quot;8&quot; by slicing it into small pieces, jumbling them up, and deciding whether on average each piece looks like a &quot;3&quot; or an &quot;8&quot;. But that&#39;s what adaptive average pooling effectively does! Fully convolutional networks are only really a good choice for objects that don&#39;t have a single correct orientation or size (i.e. like most natural photos). . Once we are done with our convolutional layers, we will get activations of size bs x ch x h x w (batch size, a certain number of channels, height and width). We want to convert this to a tensor of size bs x ch, so we take the average over the last two dimensions and flatten the trailing 1 x 1 dimension like we did in our previous model. . This is different from regular pooling in the sense that those layers will generally take the average (for average pooling) or the maximum (for max pooling) of a window of a given size: for instance max pooling layers of size 2 that were very popular in older CNNs reduce the size of our image by half on each dimension by taking the maximum of each 2 by 2 window (with a stride of 2). . As before, we can define a Learner with our custom model and then train it on the data we grabbed before: . def get_learner(m): return Learner(dls, m, loss_func=nn.CrossEntropyLoss(), metrics=accuracy ).to_fp16() learn = get_learner(get_model()) . learn.lr_find() . (0.47863011360168456, 3.981071710586548) . 3e-3 is very often a good learning rate for CNNs, and that appears to be the case here too, so let&#39;s try that: . learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.901582 | 2.155090 | 0.325350 | 00:07 | . 1 | 1.559855 | 1.586795 | 0.507771 | 00:07 | . 2 | 1.296350 | 1.295499 | 0.571720 | 00:07 | . 3 | 1.144139 | 1.139257 | 0.639236 | 00:07 | . 4 | 1.049770 | 1.092619 | 0.659108 | 00:07 | . That&#39;s a pretty good start, considering we have to pick the correct one of ten categories, and we&#39;re training from scratch for just 5 epochs! But we can do way better than this using a deeper model. However, just stacking new layers won&#39;t really improve our results (you can try and see for yourself!). To work around this problem, ResNets introduce the idea of skip connections. Let&#39;s have a look at what it is exactly. . Building a modern CNN: ResNet . We now have all the pieces needed to build the models we have been using in each computer vision task since the beginning of this book: ResNets. We&#39;ll introduce the main idea behind them and show how it improves accuracy on Imagenette compared to our previous model, before building a version with all the recent tweaks. . Skip-connections . In 2015 the authors of the ResNet paper noticed something that they found curious. Even after using batchnorm, they saw that a network using more layers was doing less well than a network using less layers — and there were no other differences between the models. Most interestingly, the difference was observed not only in the validation set, but also in the training set; so, it wasn&#39;t just a generalisation issue, but a training issue. As the paper explains: . : Unexpectedly, such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to higher training error, as [previously reported] and thoroughly verified by our experiments. They showed the graph in &lt;&gt;, with training error on the left, and test on the right.&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Training of networks of different depth . As the authors mention here, they are not the first people to have noticed this curious fact. But they were the 1st to make a very important leap: . : Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model. Being an academic paper, this process written in a rather inaccessible way — but it&#39;s actually saying something very simple: start with the 20 layer neural network that is trained well, and add another 36 layers that do nothing at all (for instance, they linear layer with a single weight equal to one, and bias equal to 0). This would be a 56 layer network which does exactly the same thing as the 20 layer network. This shows that there are always deep networks which should be at least as good as any shallow network. But for some reason, SGD does not seem able to find them. jargon: Identity mapping: a function that just returns its input without changing it at all. Also known as identity function. Actually, there is another way to create those extra 36 layers, which is much more interesting. What if we replaced every occurrence of conv(x) with x + conv(x), where conv is the function from the previous chapter which does a 2nd convolution, then relu, then batchnorm. Furthermore, recall that batchnorm does gamma*y + beta. What if we initialized gamma for every one of these batchnorm layers to zero? Then our conv(x) for those extra 36 layers will always be equal to zero, which means x+conv(x) will always be equal to x. . What has that gained us, then? The key thing is that those 36 extra layers, as they stand, are an identity mapping, but they have parameters, which means they are trainable. So, we can start with our best 20 layer model, add these 36 extra layers which initially do nothing at all, and then fine tune the whole 56 layer model. If those extra 36 layers can be useful, then they can learn parameters to do so! . The ResNet paper actually proposed a variant of this, which is to instead &quot;skip over&quot; every 2nd convolution, so effectively we get x+conv2(conv1(x)). This is shown by the diagram in &lt;&gt; (from the paper).&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; A simple ResNet block . That arrow on the right is just the x part of x+conv2(conv1(x)), and is known as the identity branch or skip connection. The path on the left is the conv2(conv1(x)) part. You can think of the identity path as providing a direct route from the input to the output. . In a ResNet, we don&#39;t actually train it by first training a smaller number of layers, and then add new layers on the end and fine-tune. Instead, we use ResNet blocks (like the above) throughout the CNN, initialized from scratch in the usual way, and trained with SGD in the usual way. We rely on the skip connections to make the network easier to train for SGD. . There&#39;s another (largely equivalent) way to think of these &quot;ResNet blocks&quot;. This is how the paper describes it: . : Instead of hoping each few stacked layers directly fit a desired underlying mapping, we explicitly let these layers fit a residual mapping. Formally, denoting the desired underlying mapping as H(x), we let the stacked nonlinear layers fit another mapping of F(x) := H(x)−x. The original mapping is recast into F(x)+x. We hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers. Again, this is rather inaccessible prose—so let&#39;s try to restate it in plain English! If the outcome of a given layer is x, when using a ResNet block that return y = x+block(x), we&#39;re not asking the block to predict y, we are asking it to predict the difference between y-x. So the job of those blocks isn&#39;t to predict certain features anymore, but a little extra step that will minimize the error between x and the desired y. ResNet is, therefore, good at learning about slight differences between doing nothing and some other feature that the layer learns. Since we predict residuals (reminder: &quot;residual&quot; is predictions minus targets), this is why those kinds of models were named ResNets. . One key concept that both of these two ways of thinking about ResNets share is the idea of &quot;easy to learn&quot;. This is an important theme. Recall the universal approximation theorem, which states that a sufficiently large network can learn anything. This is still true. But there turns out to be a very important difference between what a network can learn in principle, and what it is easy for it to learn under realistic data and training regimes. Many of the advances in neural networks over the last decade have been like the ResNet block: the result of realizing how to make something which was always possible actually feasible. . Note: The original paper didn&#8217;t actually do the trick of using zero for the initial value of gamma in the batchnorm layer; that came a couple of years later. So the original version of ResNet didn&#8217;t quite begin training with a truly identity path through the ResNet blocks, but nonetheless having the ability to &quot;navigate through&quot; the skip connections did indeed make it train better. Adding the batchnorm gamma init trick made the models train at even higher learning rates. Here&#39;s the definition of a simple ResNet block (where norm_type=NormType.BatchZero causes fastai to init the gamma weights of that batchnorm layer to zero): . class ResBlock(Module): def __init__(self, ni, nf): self.convs = nn.Sequential( ConvLayer(ni,nf), ConvLayer(nf,nf, norm_type=NormType.BatchZero)) def forward(self, x): return x + self.convs(x) . One problem with this, however, is that it can&#39;t handle a stride other than 1, and it requires that ni==nf. Stop for a moment, to think carefully about why this is... . The issue is that with a stride of, say, 2, on one of the convolutions, the grid size of the output activations will be half the size on each axis of the input. So then we can&#39;t add that back to x in forward because x and the output activations have different dimensions. The same basic issue occurs if ni!=nf: the shapes of the input and output connections won&#39;t allow us to add them together. . To fix this, we need a way to change the shape of x to match the result of self.convs. Halving the grid size can be done using an average pooling layer with a stride of 2: that is, a layer which takes 2x2 patches from the input, and replaces them with their average. . Changing the number of channels can be done by using a convolution. We want this skip connection to be as close to an identity map as possible, however, which means making this convolution as simple as possible. The simplest possible convolution is one where the kernel size is 1. That means that the kernel is size ni*nf*1*1, so it&#39;s only doing a dot product over the channels of each input pixel--it&#39;s not combining across pixels at all. This kind of 1x1 convolution is very widely used in modern CNNs, so take a moment to think about how it works. . question: Create a 1x1 convolution with F.conv2d or nn.Conv2d and apply it to an image. What happens to the shape of the image? . jargon: 1x1 convolution: A convolution with a kernel size of one. . Here&#39;s a ResBlock using these tricks to handle changing shape in the skip connection: . def _conv_block(ni,nf,stride): return nn.Sequential( ConvLayer(ni, nf, stride=stride), ConvLayer(nf, nf, act_cls=None, norm_type=NormType.BatchZero)) . class ResBlock(Module): def __init__(self, ni, nf, stride=1): self.convs = _conv_block(ni,nf,stride) self.idconv = noop if ni==nf else ConvLayer(ni, nf, 1, act_cls=None) self.pool = noop if stride==1 else nn.AvgPool2d(2, ceil_mode=True) def forward(self, x): return F.relu(self.convs(x) + self.idconv(self.pool(x))) . Note that we&#39;re using the noop function here, which simply returns its input unchanged (noop is a computer science term that stands for &quot;no operation&quot;). In this case, idconv does nothing at all if nf==nf, and pool does nothing if stride==1, which is what we wanted in our skip connection. . Also, you&#39;ll see that we&#39;ve removed relu (act_cls=None) from the final convolution in convs and from idconv, and moved it to after we add the skip connection. The thinking behind this is that the whole ResNet block is like a layer, and you want your activation to be after your layer. . Let&#39;s replace our block with ResBlock, and try it out: . def block(ni,nf): return ResBlock(ni, nf, stride=2) learn = get_learner(get_model()) . learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.973174 | 1.845491 | 0.373248 | 00:08 | . 1 | 1.678627 | 1.778713 | 0.439236 | 00:08 | . 2 | 1.386163 | 1.596503 | 0.507261 | 00:08 | . 3 | 1.177839 | 1.102993 | 0.644841 | 00:09 | . 4 | 1.052435 | 1.038013 | 0.667771 | 00:09 | . It&#39;s not much better. But the whole point of this was to allow us to train deeper models, and we&#39;re not really taking advantage of that yet. To create a deeper model that&#39;s, say, twice as deep, all we need to do is replace our block with two ResBlocks in a row: . def block(ni, nf): return nn.Sequential(ResBlock(ni, nf, stride=2), ResBlock(nf, nf)) . learn = get_learner(get_model()) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.964076 | 1.864578 | 0.355159 | 00:12 | . 1 | 1.636880 | 1.596789 | 0.502675 | 00:12 | . 2 | 1.335378 | 1.304472 | 0.588535 | 00:12 | . 3 | 1.089160 | 1.065063 | 0.663185 | 00:12 | . 4 | 0.942904 | 0.963589 | 0.692739 | 00:12 | . Now we&#39;re making good progress! . The authors of the ResNet paper went on to win the 2015 ImageNet challenge. At the time, this was by far the most important annual event in computer vision. We have already seen another ImageNet winner: the 2013 winners, Zeiler and Fergus. It is interesting to note that in both cases the starting point for the breakthroughs were experimental observations. Observations about what layers actually learn, in the case of Zeiler and Fergus, and observations about which kind of networks can be trained, in the case of the ResNet authors. This ability to design and analyse thoughtful experiments, or even just to see an unexpected result say &quot;hmmm, that&#39;s interesting&quot; — and then, most importantly, to figure out what on earth is going on, with great tenacity, is at the heart of many scientific discoveries. Deep learning is not like pure mathematics. It is a heavily experimental field, so it&#39;s important to be a strong practitioner, not just a theoretician. . Since the ResNet was introduced, there&#39;s been many papers studying it and applying it to many domains. One of the most interesting, published in 2018, is Visualizing the Loss Landscape of Neural Nets. It shows that using skip connections help smoothen the loss function, which makes training easier as it avoids falling into a very sharp area. &lt;&gt; shows a stunning picture from the paper, showing the bumpy terrain that SGD has to navigate to optimize a regular CNN (left) versus the smooth surface of a ResNet (right).&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Impact of ResNet on loss landscape . This first model is already good, but further research has discovered more tricks we can apply to make it better. . A state-of-the-art ResNet . In Bag of Tricks for Image Classification with Convolutional Neural Networks, the authors study different variations of the ResNet architecture that come at almost no additional cost in terms of number of parameters or computation. By using this tweaked ResNet50 architecture and Mixup they achieve 94.6% top-5 accuracy on ImageNet, instead of 92.2% with a regular ResNet50 without Mixup. This result is better than regular ResNet models that are twice as deep (and twice as slow, and much more likely to overfit). . jargon: top-5 accuracy: A metric testing how often the label we want is in the top 5 predictions of our model. It was used in the Imagenet competition, since many images contained multiple objects, or contained objects that could be easily confused or may even have been mislabeled with a similar label. In these situations, looking at top-1 accuracy may be inappropriate. However, recently CNNs have been getting so good that top-5 accuracy is nearly 100%, so some researchers are using top-1 accuracy for Imagenet too now. . So, as we scale up to the full ResNet, we won&#39;t show the original one, but the tweaked one, since it&#39;s substantially better. It differs a little bit from our previous implementation, in that instead of just starting with ResNet blocks, it begins with a few convolutional layers followed by a max pooling layer. This is what the first layers look like: . def _resnet_stem(*sizes): return [ ConvLayer(sizes[i], sizes[i+1], 3, stride = 2 if i==0 else 1) for i in range(len(sizes)-1) ] + [nn.MaxPool2d(kernel_size=3, stride=2, padding=1)] . #hide_output _resnet_stem(3,32,32,64) . [ConvLayer( (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ), ConvLayer( (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ), ConvLayer( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() ), MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)] . [ConvLayer( (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1) (2): ReLU() ), ConvLayer( (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(32, eps=1e-05, momentum=0.1) (2): ReLU() ), ConvLayer( (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1) (2): ReLU() ), MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=False)] . jargon: Stem: The stem of a CNN are its first few layers. Generally, the stem has a different structure to the main body of the CNN. . The reason that we have a stem of plain convolutional layers, instead of ResNet blocks, is based on a very important insight about all deep convolutional neural networks: the vast majority of the computation occurs in the early layers. Therefore, we should keep the early layers as fast and simple as possible. . To see why so much computation occurs in the early layers, consider the very first convolution on a 128 pixel input image. If it is a stride one convolution, then it will apply the kernel to every one of the 128×128 pixels. That&#39;s a lot of work! In the later layers, however, the grid size could be as small as 4x4 or even 2x2. So there are far fewer kernel applications to do. . On the other hand, the first layer convolution only has three input features, and 32 output features. Since it is a 3x3 kernel, this is 3×32×3×3 = 864 parameters in the weights. On the other hand, the last convolution will be 256 input features and 512 output features, which will be 1,179,648 weights! So the first layers contain vast majority of the computation, but the last layers contain the vast majority of the parameters. . A ResNet block takes more computation than a plain convolutional block, since (in the stride two case) a ResNet block has three convolutions and a pooling layer. That&#39;s why we want to have plain convolutions to start off our ResNet. . We&#39;re now ready to show the implementation of a modern ResNet, with the &quot;bag of tricks&quot;. The ResNet use four groups of ResNet blocks, with 64, 128, 256 then 512 filters. Each groups starts with a stride 2 block, except for the first one, since it&#39;s just after a MaxPooling layer. . class ResNet(nn.Sequential): def __init__(self, n_out, layers, expansion=1): stem = _resnet_stem(3,32,32,64) self.block_szs = [64, 64, 128, 256, 512] for i in range(1,5): self.block_szs[i] *= expansion blocks = [self._make_layer(*o) for o in enumerate(layers)] super().__init__(*stem, *blocks, nn.AdaptiveAvgPool2d(1), Flatten(), nn.Linear(self.block_szs[-1], n_out)) def _make_layer(self, idx, n_layers): stride = 1 if idx==0 else 2 ch_in,ch_out = self.block_szs[idx:idx+2] return nn.Sequential(*[ ResBlock(ch_in if i==0 else ch_out, ch_out, stride if i==0 else 1) for i in range(n_layers) ]) . The _make_layer function is just there to create a series of n_layers blocks. The first one is is going from ch_in to ch_out with the indicated stride and all the others are blocks of stride 1 with ch_out to ch_out tensors. Once the blocks are defined, our model is purely sequential, which is why we define it as a subclass of nn.Sequential. (Ignore the expansion parameter for now--we&#39;ll discuss it in the next section. For now, it&#39;ll be 1, so it doesn&#39;t do anything.) . The various versions of the models (ResNet 18, 34, 50, etc) just change the number of blocks in each of those groups. This is the definition of a ResNet18: . rn = ResNet(dls.c, [2,2,2,2]) . Let&#39;s train it for a little bit and see how it fares compared to the previous model: . learn = get_learner(rn) learn.fit_one_cycle(5, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.673882 | 1.828394 | 0.413758 | 00:13 | . 1 | 1.331675 | 1.572685 | 0.518217 | 00:13 | . 2 | 1.087224 | 1.086102 | 0.650701 | 00:13 | . 3 | 0.900428 | 0.968219 | 0.684331 | 00:12 | . 4 | 0.760280 | 0.782558 | 0.757197 | 00:12 | . Even although we have more channels (and our model is therefore even more accurate), our training is just as fast as before, thanks to our optimized stem. . To make our model deeper without taking too much compute or memory, the ResNet paper introduced another kind of block for ResNets with a depth of 50 or more, using something called a bottleneck. . Bottleneck layers . Instead of stacking two convolutions with a kernel size of 3, bottleneck layers use three different convolutions: two 1x1 (at the beginning and the end) and one 3x3, as shown in the right of &lt;&gt; the ResNet paper (using an example of 64 channel output, comparing to the regular ResBlock on the left).&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Comparison of regular and bottleneck ResNet blocks . Why is that useful? 1x1 convolutions are much faster, so even if this seems to be a more complex design, this block executes faster than the first ResNet block we saw. This then lets us use more filters: as we see on the illustration, the number of filters in and out is 4 times higher (256) and the 1 by 1 convs are here to diminish then restore the number of channels (hence the name bottleneck). The overall impact is that we can use more filters in the same amount of time. . Let&#39;s try replacing our ResBlock with this bottleneck design: . def _conv_block(ni,nf,stride): return nn.Sequential( ConvLayer(ni, nf//4, 1), ConvLayer(nf//4, nf//4, stride=stride), ConvLayer(nf//4, nf, 1, act_cls=None, norm_type=NormType.BatchZero)) . We&#39;ll use this to create a ResNet50, which uses this bottleneck block, and uses group sizes of (3,4,6,3). We now need to pass 4 in to the expansion parameter of ResNet, since we need to start with four times less channels, and we&#39;ll end with four times more channels. . Deeper networks like this don&#39;t generally show improvements when training for only 5 epochs, so we&#39;ll bump it up to 20 epochs this time to make the most of our bigger model. And to really get great results, let&#39;s use bigger images too: . dls = get_data(URLs.IMAGENETTE_320, presize=320, resize=224) . We don&#39;t have to do anything to account for the larger 224 pixel images--thanks to our fully convolutional network, it just works. This is also why we were able to do progressive resizing earlier in the book--the models we used were fully convolutional, so we were even able to fine-tune models trained with different sizes. . rn = ResNet(dls.c, [3,4,6,3], 4) . learn = get_learner(rn) learn.fit_one_cycle(20, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.613448 | 1.473355 | 0.514140 | 00:31 | . 1 | 1.359604 | 2.050794 | 0.397452 | 00:31 | . 2 | 1.253112 | 4.511735 | 0.387006 | 00:31 | . 3 | 1.133450 | 2.575221 | 0.396178 | 00:31 | . 4 | 1.054752 | 1.264525 | 0.613758 | 00:32 | . 5 | 0.927930 | 2.670484 | 0.422675 | 00:32 | . 6 | 0.838268 | 1.724588 | 0.528662 | 00:32 | . 7 | 0.748289 | 1.180668 | 0.666497 | 00:31 | . 8 | 0.688637 | 1.245039 | 0.650446 | 00:32 | . 9 | 0.645530 | 1.053691 | 0.674904 | 00:31 | . 10 | 0.593401 | 1.180786 | 0.676433 | 00:32 | . 11 | 0.536634 | 0.879937 | 0.713885 | 00:32 | . 12 | 0.479208 | 0.798356 | 0.741656 | 00:32 | . 13 | 0.440071 | 0.600644 | 0.806879 | 00:32 | . 14 | 0.402952 | 0.450296 | 0.858599 | 00:32 | . 15 | 0.359117 | 0.486126 | 0.846369 | 00:32 | . 16 | 0.313642 | 0.442215 | 0.861911 | 00:32 | . 17 | 0.294050 | 0.485967 | 0.853503 | 00:32 | . 18 | 0.270583 | 0.408566 | 0.875924 | 00:32 | . 19 | 0.266003 | 0.411752 | 0.872611 | 00:33 | . We&#39;re getting a great result now! Try adding Mixup, and then training this for a hundred epochs while you go get lunch. You&#39;ll have yourself a very accurate image classifier, trained from scratch. . The bottleneck design we&#39;ve shown here is only used in ResNet50, 101, and 152 in all official models we&#39;ve seen. ResNet18 and 34 use the non-bottleneck design seen in the previous section. However, we&#39;ve noticed that the bottleneck layer generally works better even for the shallower networks. This just goes to show that the little details in papers tend to stick around for years, even if they&#39;re actually not quite the best design! Questioning assumptions and &quot;stuff everyone knows&quot; is always a good idea, because this is still a new field, and there&#39;s lots of details that aren&#39;t always done well. . TK add conclusion . Questionnaire . How did we get to a single vector of activations in the convnets used for MNIST in previous chapters? Why isn&#39;t that suitable for Imagenette? | What do we do for Imagenette instead? | What is adaptive pooling? | What is average pooling? | Why do we need Flatten after an adaptive average pooling layer? | What is a skip connection? | Why do skip connections allow us to train deeper models? | What does &lt;&gt; show? How did that lead to the idea of skip connections?&lt;/li&gt; What is an identity mapping? | What is the basic equation for a ResNet block (ignoring batchnorm and relu layers)? | What do ResNets have to do with &quot;residuals&quot;? | How do we deal with the skip connection when there is a stride 2 convolution? How about when the number of filters changes? | How can we express a 1x1 convolution in terms of a vector dot product? | What does the noop function return? | Explain what is shown in &lt;&gt;.&lt;/li&gt; When is top-5 accuracy a better metric than top-1 accuracy? | What is the stem of a CNN? | Why use plain convs in the CNN stem, instead of ResNet blocks? | How does a bottleneck block differ from a plain ResNet block? | Why is a bottleneck block faster? | How do fully convolution nets (and nets with adaptive pooling in general) allow for progressive resizing? | &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Further research . Try creating a fully convolutional net with adaptive average pooling for MNIST (note that you&#39;ll need fewer stride 2 layers). How does it compare to a network without such a pooling layer? | In &lt;&gt; we introduce Einstein summation notation. Skip ahead to see how this works, and then write an implementation of the 1x1 convolution operation using torch.einsum. Compare it to the same operation using torch.conv2d.&lt;/li&gt; Write a &quot;top 5 accuracy&quot; function using plain PyTorch or plain Python. | Train a model on Imagenette for more epochs, with and without label smoothing. Take a look at the Imagenette leaderboards and see how close you can get to the best results shown. Read the linked pages describing the leading approaches. | &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; | | | .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_resnet.html",
            "relUrl": "/2020/03/19/_resnet.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Chapter 2",
            "content": "[[chapter_production]] From model to production . The five lines of code we saw in &lt;&gt; are just one small part of the process of using deep learning in practice. In this chapter, we&#39;re going to use a computer vision example to look at the end-to-end process of creating a deep learning application. More specifically: we&#39;re going to build a bear classifier! In the process, we&#39;ll discuss the capabilities and constraints of deep learning, learn about how to create datasets, look at possible gotchas when using deep learning in practice, and more. Many of the key points will apply equally well to other deep learning problems, such as we showed in &lt;&gt;. If you work through a problem similar in key respects to our example problems, we expect you to get excellent results with little code, quickly.&lt;/p&gt; Let&#39;s start with how you should frame your problem. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The practice of deep learning . We&#39;ve seen that deep learning can solve a lot of challenging problems quickly and with little code. As a beginner there&#39;s a sweet spot of problems that are similar enough to our example problems that you can very quickly get extremely useful results. However, deep learning isn&#39;t magic! The same 5 lines of code won&#39;t work on every problem anyone can think of today. Underestimating the constraints and overestimating the capabilities of deep learning may lead to frustratingly poor results. At least until you gain some experience to solve the problems that arise. Overestimating the constraints and underestimating the capabilities of deep learning may mean you do not attempt a solvable problem because you talk yourself out of it. . We often talk to people who overestimate both the constraints, and the capabilities of deep learning. Both of these can be problems: underestimating the capabilities means that you might not even try things which could be very beneficial; underestimating the constraints might mean that you fail to consider and react to important issues. . The best thing to do is to keep an open mind. If you remain open to the possibility that deep learning might solve part of your problem with less data or complexity than you expect, then it is possible to design a process where you can find the specific capabilities and constraints related to your particular problem as you work through the process. This doesn&#39;t mean making any risky bets — we will show you how you can gradually roll out models so that they don&#39;t create significant risks, and can even backtest them prior to putting them in production. . Let&#39;s start with how you should frame your problem. . Starting your project . So where should you start your deep learning journey? The most important thing is to ensure that you have some project that you are working on — it is only through working on your own projects that you will get real experience of building and using models. When selecting a project, the most important consideration is data availability. Regardless of whether you are doing a project just for your own learning, or for practical application in your organization, you want something where you can get started quickly. We have seen many students, researchers, and industry practitioners waste months or years while they attempt to find their perfect dataset. The goal is not to find the perfect dataset, or the perfect project, but just to get started, and iterate from there. . If you take this approach, then you will be on your third iteration of learning and improving whilst the perfectionists are still in the planning stages! . We also suggest that you iterate from end to end in your project; that is, don&#39;t spend months fine tuning your model, or polishing the perfect GUI, or labelling the perfect dataset… Instead, complete every step as well as you can in a reasonable amount of time, all the way to the end. For instance, if your final goal is an application that runs on a mobile phone, then that should be what you have after each iteration. But perhaps in the early iterations you take some shortcuts, for instance by doing all of the processing on a remote server, and using a simple responsive web application. By completing the project and to end, you will see where the most tricky bits are, and which bits make the biggest difference to the final result. . As you work through this book, we suggest that you both complete lots of small experiments, by running and adjusting the notebooks we provide, at the same time that you gradually develop your own projects. That way, you will be getting experience with all of the tools and techniques that were explaining, as we discuss them. . s: To make the most of this book, take the time to experiment between each chapter, be it on your own project or exploring the notebooks we provide. Then try re-writing those notebooks from scratch on a new dataset. It&#39;s only by practicing (and failing) a lot that you will get an intuition on how to train a model. By using the end to end iteration approach you will also get a better understanding of how much data you really need. For instance, you may find you can only easily get 200 labelled data items, and you can&#39;t really know until you try whether that&#39;s enough to get the performance you need for your application to work well in practice. . In an organizational context you will be able to show your colleagues that your idea can really work, by showing them a real working prototype. We have repeatedly observed that this is the secret to getting good organizational buy in for a project. . Since it is easiest to get started on a project where you already have data available, that means it&#39;s probably easiest to get started on a project related to something you are already doing, because you already have data about things that you are doing. For instance, if you work in the music business, you may have access to many recordings. If you work as a radiologist, you probably have access to lots of medical images. If you are interested in wildlife preservation, you may have access to lots of images of wildlife. . Sometimes, you have to get a bit creative. Maybe you can find some previous machine learning project, such as a Kaggle competition, that is related to your field of interest. Sometimes, you have to compromize. Maybe you can&#39;t find the exact data you need for the precise project you have in mind; but you might be able to find something from a similar domain, or measured in a different way, tackling a slightly different problem. Working on these kinds of similar projects will still give you a good understanding of the overall process, and may help you identify other shortcuts, data sources, and so forth. . Especially when you are just starting out with deep learning it&#39;s not a good idea to branch out into very different areas to places that deep learning has not been applied to before. That&#39;s because if your model does not work at first, you will not know whether it is because you have made a mistake, or if the very problem you are trying to solve is simply not solvable with deep learning. And you won&#39;t know where to look to get help. Therefore, it is best at first to start with something where you can find an example online of somebody who has had good results with something that is at least somewhat similar to what you are trying to achieve, or where you can convert your data into a format similar what someone else has used before (such as creating an image from your data). Let&#39;s have a look at the state of deep learning, jsut so you know what kinds of things deep learning is good at right now. . The state of deep learning . Let&#39;s start by considering whether deep learning can be any good at the problem you are looking to work on. In general, here is a summary of the state of deep learning is at the start of 2020. However, things move very fast, and by the time you read this some of these constraints may no longer exist. We will try to keep the book website up-to-date; in addition, a Google search for &quot;what can AI do now&quot; there is likely to provide some up-to-date information. . Computer vision . There are many domains in which deep learning has not been used to analyse images yet, but those where it has been tried have nearly universally shown that computers can recognise what items are in an image at least as well as people can — even specially trained people, such as radiologists. This is known as object recognition. Deep learning is also good at recognizing whereabouts objects in an image are, and can highlight their location and name each found object. This is known as object detection (there is also a variant of this we saw in &lt;&gt;, where every pixel is categorized based on what kind of object it is part of--this is called segmentation). Deep learning algorithms are generally not good at recognizing images that are significantly different in structure or style to those used to train the model. For instance, if there were no black-and-white images in the training data, the model may do poorly on black-and-white images. If the training data did not contain hand-drawn images then the model will probably do poorly on hand-drawn images. There is no general way to check what types of image are missing in your training set, but we will show in this chapter some ways to try to recognize when unexpected image types arise in the data when the model is being used in production (this is known as checking for out of domain data).&lt;/p&gt; One major challenge for object detection systems is that image labelling can be slow and expensive. There is a lot of work at the moment going into tools to try to make this labelling faster and easier, and require less handcrafted labels to train accurate object detection models. One approach which is particularly helpful is to synthetically generate variations of input images, such as by rotating them, or changing their brightness and contrast; this is called data augmentation and also works well for text and other types of model. We will be discussing it in detail in this chapter. . Another point to consider is that although your problem might not look like a computer vision problem, it might be possible with a little imagination to turn it into one. For instance, if what you are trying to classify is sounds, you might try converting the sounds into images of their acoustic waveforms and then training a model on those images. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Text (natural language processing) . Just like in computer vision, computers are very good at categorising both short and long documents based on categories such as spam, sentiment (e.g. is the review positive or negative), author, source website, and so forth. We are not aware of any rigourous work done in this area to compare to human performance, but anecdotally it seems to us that deep learning performance is similar to human performance here. Deep learning is also very good at generating context-appropriate text, such as generating replies to social media posts, and imitating a particular author&#39;s style. It is also good at making this content compelling to humans, and has been shown to be even more compelling than human-generated text. However, deep learning is currently not good at generating correct responses! We don&#39;t currently have a reliable way to, for instance, combine a knowledge base of medical information, along with a deep learning model for generating medically correct natural language responses. This is very dangerous, because it is so easy to create content which appears to a layman to be compelling, but actually is entirely incorrect. . Another concern is that context-appropriate, highly compelling responses on social media can be used at massive scale — thousands of times greater than any troll farm previously seen — to spread disinformation, create unrest, and encourage conflict. As a rule of thumb, text generation will always be technologically a bit ahead of the ability of models to recognize automatically generated text. For instance, it is possible to use a model that can recognize artificially generated content to actually improve the generator that creates that content, until the classification model is no longer able to complete its task. . Despite these issues, deep learning can be used to translate text from one language to another, summarize long documents into something which can be digested more quickly, find all mentions of a concept of interest, and many more. Unfortunately, the translation or summary could well include completely incorrect information! However, it is already good enough that many people are using the systems — for instance Google&#39;s online translation system (and every other online service we are aware of) is based on deep learning. . Combining text and images . The ability of deep learning to combine text and images into a single model is, generally, far better than most people intuitively expect. For example, a deep learning model can be trained on input images, and output captions written in English, and can learn to generate surprisingly appropriate captions automatically for new images! But again, we have the same warning that we discussed in the previous section: there is no guarantee that these captions will actually be correct. . Because of this serious issue we generally recommend that deep learning be used not as an entirely automated process, but as part of a process in which the model and a human user interact closely. This can potentially make humans orders of magnitude more productive than they would be with entirely manual methods, and actually result in more accurate processes than using a human alone. For instance, an automatic system can be used to identify potential strokes directly from CT scans, send a high priority alert to have potential/scans looked at quickly. There is only a three-hour window to treat strokes, so this fast feedback loop could save lives. At the same time, however, all scans could continue to be sent to radiologists in the usual way, so there would be no reduction in human input. Other deep learning models could automatically measure items seen on the scan, and insert those measurements into reports, warning the radiologist about findings that they may have missed, and tell the radiologist about other cases which might be relevant. . Tabular data . For analysing timeseries and tabular data, deep learning has recently been making great strides. However, deep learning is generally used as part of an ensemble of multiple types of model. If you already have a system that is using random forests or gradient boosting machines (popular tabular modelling tools that we will learn about soon) then switching to, or adding, deep learning may not result in any dramatic improvement. Deep learning does greatly increase the variety of columns that you can include, for example columns containing natural language (e.g. book titles, reviews, etc), and high cardinality categorical columns (i.e. something that contains a large number of discrete choices, such as zip code or product id). On the downside, deep learning models generally take longer to train than random forests or gradient boosting machines, although this is changing thanks to libraries such as RAPIDS, which provides GPU acceleration for the whole modeling pipeline. We cover the pros and cons of all these methods in detail in &lt;&gt; in this book.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Recommendation systems . Recommendation systems are really just a special type of tabular data. In particular, they generally have a high cardinality categorical variable representing users, and another one representing products (or something similar). A company like Amazon represents every purchase that has ever been made as a giant sparse matrix, with customers as the rows and products as the columns. Once they have the data in this format, data scientists apply some form of collaborative filtering to fill in the matrix. For example, if customer A buys products 1 and 10, and customer B buys products 1, 2, 4, and 10, the engine will recommend that A buy 2 and 4. Because deep learning models are good at handling high cardinality categorical variables they are quite good at handling recommendation systems. They particularly come into their own, just like for tabular data, when combining these variables with other kinds of data, such as natural language, or images. They can also do a good job of combining all of these types of information with additional meta data represented as tables, such as user information, previous transactions, and so forth. . However, nearly all machine learning approaches have the downside that they only tell you what products a particular user might like, rather than what recommendations would be helpful for a user. Many kinds of recommendations for products a user might like may not be at all helpful, for instance, if the user is already familiar with its products, or if they are simply different packagings of products they have already purchased (such as a boxed set of novels, where they already have each of the items in that set). Jeremy likes reading books by Terry Pratchett, and for a while Amazon was recommending nothing but Terry Pratchett books to him (see &lt;&gt;), which really wasn&#39;t helpful because he already was aware of these books!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; A not-so-useful recommendation . Other data types: Often you will find that domain-specific data types fit very nicely into existing categories. For instance, protein chains look a lot like natural language documents, in that they are long sequences of discrete tokens with complex relationships and meaning throughout the sequence. And indeed, it does turn out that using NLP deep learning methods is the current state of the art approach for many types of protein analysis. As another example: sounds can be represented as spectrograms, which can be treated as images; standard deep learning approaches for images turn out to work really well on spectrograms. . There are many accurate models that are of no use to anyone, and many inaccurate models that are highly useful. To ensure that your modeling work is useful in practice, you need to consider how your work will be used. In 2012 Jeremy, along with Margit Zwemer and Mike Loukides, introduced a method called The Drivetrain Approach for thinking about this issue. . The Drivetrain approach . The Drivetrain approach, illustrated in &lt;&gt;, was described in detail in Designing Great Data Products. The basic idea is to start with considering your objective, then think about what you can actually do to change that objective (&quot;levers&quot;), what data you have that might help you connect potential changes to levers to changes in your objective, and then to build a model of that. You can then use that model to find the best actions (that is, changes to levers) to get the best results in terms of your objective.&lt;/p&gt; Consider a model in an autonomous vehicle, you want to help a car drive safely from point A to point B without human intervention. Great predictive modeling is an important part of the solution, but it doesn&#39;t stand on its own; as products become more sophisticated, it disappears into the plumbing. Someone using a self-driving car is completely unaware of the hundreds (if not thousands) of models and the petabytes of data that make it work. But as data scientists build increasingly sophisticated products, they need a systematic design approach. . We use data not just to generate more data (in the form of predictions), but to produce actionable outcomes. That is the goal of the Drivetrain Approach. Start by defining a clear objective. For instance, Google, when creating their first search engine, considered &quot;What is the user’s main objective in typing in a search query?&quot;, and their answer was &quot;show the most relevant search result&quot;. The next step is to consider what levers you can pull (i.e. what actions could you take) to better achieve that objective. In Google&#39;s case, that was the ranking of the search results. The third step was to consider what new data they would need to produce such a ranking; they realized that the implicit information regarding which pages linked to which other pages could be used for this purpose. Only after these first three steps do we begin thinking about building the predictive models. Our objective and available levers, what data we already have and what additional data we will need to collect, determine the models we can build. The models will take both the levers and any uncontrollable variables as their inputs; the outputs from the models can be combined to predict the final state for our objective. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The Drivetrain approach . Let&#39;s consider another example: recommendation systems. The objective of a recommendation engine is to drive additional sales by surprising and delighting the customer with recommendations of items they would not have purchased without the recommendation. The lever is the ranking of the recommendations. New data must be collected to generate recommendations that will cause new sales. This will require conducting many randomized experiments in order to collect data about a wide range of recommendations for a wide range of customers. This is a step that few organizations take; but without it, you don&#39;t have the information you need to actually optimize recommendations based on your true objective (more sales!) . Finally, you could build two models for purchase probabilities, conditional on seeing or not seeing a recommendation. The difference between these two probabilities is a utility function for a given recommendation to a customer. It will be low in cases where the algorithm recommends a familiar book that the customer has already rejected (both components are small) or a book that he or she would have bought even without the recommendation (both components are large and cancel each other out). . As you can see, in practice often the practical implementation of your model will require a lot more than just training a model! You&#39;ll often need to run experiments to collect more data, and consider how to incorporate your models into the overall system you&#39;re developing. Speaking of data, let&#39;s now focus on how to find find data for your project. . Gathering data . For many types of projects, you may be able to find all the data you need online. The project we&#39;ll be completing in this chapter is a bear detector. It will discriminate between three types of bear: grizzly, black, and teddy bear. There are many images on the Internet of each type of bear we can use. We just need a way to find them and download them. We&#39;ve provided a tool you can use for this purpose, so you can follow along with this chapter, creating your own image recognition application for whatever kinds of object you&#39;re interested in. In the fast.ai course, thousands of students have presented their work on the course forums, displaying everything from Trinidad hummingbird varieties, to Panama bus types, and even an application that helped one student let his fiancee recognize his sixteen cousins during Christmas vacation! . As at the time of writing, Bing Image Search is the best option we know of for finding and downloading images. It&#39;s free for up to 1000 queries per month, and each query can download up to 150 images. However, something better might have come along between when we wrote this and when you&#39;re reading the book, so be sure to check out book.fast.ai where we&#39;ll let you know our current recommendation. . . Important: Services that can be used for creating datasets come and go all the time, and their features, interfaces, and pricing change regularly too. In this section, we&#8217;ll show how to use one particular provider, Bing Image Search, using the service they have as this book as written. We&#8217;ll be providing more options and more up to date information on the http://book.fast.ai[book website], so be sure to have a look there now to get the most current information on how to download images from the web to create a dataset for deep learning. . To download images with Bing Image Search, you should sign up at Microsoft for Bing Image Search. You will be given a key, which you can either paste here, replacing &quot;XXX&quot;: . key = &#39;XXX&#39; . ...or, if you&#39;re comfortable at the command line, you can set it in your terminal with: . export AZURE_SEARCH_KEY=your_key_here . and then restart jupyter notebooks, and finally execute in this notebook: . key = os.environ[&#39;AZURE_SEARCH_KEY&#39;] . Once you&#39;ve set key, you can use search_images_bing. This function is provided by the small utils class included in the book. Remember, if you&#39;re not sure where a symbol is defined, you can just type it in your notebook to find out (or prefix with ? to get help, including the name of the file where it&#39;s defined, or with ?? to get its source code): . search_images_bing . &lt;function utils.search_images_bing(key, term, min_sz=128)&gt; . results = search_images_bing(key, &#39;grizzly bear&#39;) ims = results.attrgot(&#39;content_url&#39;) len(ims) . 150 . We&#39;ve successfully downloaded the URLs of 150 grizzly bears (or, at least, images that Bing Image Search finds for that search term). Let&#39;s look at one: . dest = &#39;images/grizzly.jpg&#39; download_url(ims[0], dest) . im = Image.open(dest) im.to_thumb(128,128) . This seems to have worked nicely, so let&#39;s use fastai&#39;s download_images to download all the URLs from each of our search terms. We&#39;ll put each in a separate folder. . bear_types = &#39;grizzly&#39;,&#39;black&#39;,&#39;teddy&#39; path = Path(&#39;bears&#39;) . if not path.exists(): path.mkdir() for o in bear_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o} bear&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . Our folder has image files, as we&#39;d expect: . fns = get_image_files(path) fns . (#421) [Path(&#39;bears/black/00000095.jpg&#39;),Path(&#39;bears/black/00000133.jpg&#39;),Path(&#39;bears/black/00000062.jpg&#39;),Path(&#39;bears/black/00000023.jpg&#39;),Path(&#39;bears/black/00000029.jpg&#39;),Path(&#39;bears/black/00000094.jpg&#39;),Path(&#39;bears/black/00000124.jpg&#39;),Path(&#39;bears/black/00000056.jpeg&#39;),Path(&#39;bears/black/00000046.jpg&#39;),Path(&#39;bears/black/00000045.jpg&#39;)...] . j: I just love this about working in Jupyter notebooks! It&#39;s so easy to gradually build what I want, and check my work every step of the way. I make a lot of mistakes, so this is really helpful to me... . Often when we download files from the Internet, there are a few that are corrupt. Let&#39;s check: . failed = verify_images(fns) failed . (#0) [] . To remove the failed images, we can use unlink on each. Note that, like most fastai functions that return a collection, verify_images returns an object of type L, which includes the map method. This calls the passed function on each element of the collection. . failed.map(Path.unlink); . verify_images() . Sidebar: Getting help in jupyter notebooks . Jupyter notebooks are great to easily experiment and immediately see the results of each function, but there is also a lot of functionality to help figure out how to use the functions you have or even directly look at their source code. For instance, if you type in a cell . ??verify_images . a window will pop up with: . Signature: verify_images(fns) Source: def verify_images(fns): &quot;Find images in `fns` that can&#39;t be opened&quot; return L(fns[i] for i,o in enumerate(parallel(verify_image, fns)) if not o) File: ~/git/fastai/fastai/vision/utils.py Type: function . It tells us what argument the function accepts (fns) then shows us the source code and the file it comes from. Looking at that source code, we can see it applies the function verify_image in parallel and only keep the ones for which the result of that function is False, which is consistent with the doc string: it finds the images in fns that can&#39;t be opened. . Here are the commands that are very useful in Jupyter notebooks: . at any point, if you don&#39;t remember the exact spelling of a function or argument name, you can press &quot;tab&quot; to get suggestions of auto-completion. | when inside the parenthesis of a function, pressing &quot;shift&quot; and &quot;tab&quot; simultaneously will display a window with the signature of the function and a short documentation. Pressing it twice will expand the documentation and pressing it three times will open a full window with the same information at the bottom of your screen. | in a cell, typing ?func_name and executing will open a window with the signature of the function and a short documentation. | in a cell, typing ??func_name and executing will open a window with the signature of the function, a short documentation and the source code. | if you are using the fasti library, we added a doc function for you, executing doc(func_name) in a cell will open a window with the signature of the function, a short documentation and links to the source code on GitHub and the full documentation of the funciton in the documentation of the library. | unrelated to the documentation but still very useful to get help, at any point, if you get an error, type %debug in the next cell and execute to open the python debugger that will let you inspect the content of every variable. | . End sidebar . One thing to be aware of in this process: as we discussed in chapter_intro, models can only reflect the data used to train them. And the world is full of biased data, which ends up reflected in, for example, Bing Image Search (which we used to create our dataset). For instance, let&#39;s say you were interested in creating an app which could help users figure out whether they had healthy skin, so you trained a model on the results of searches for (say) healthy skin. &lt;&gt; shows you the results you would get.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Data for a healthy skin detector? . So with this as your training data, you would end up not with a healthy skin detector, but a young white woman touching her face detector! Be sure to think carefully about the types of data that you might expect to see in practice in your application, and check carefully to ensure that all these types are reflected in your model&#39;s source data.footnote:[Thanks to Deb Raji, who came up with the healthy skin example. See her paper Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products for more fascinating insights into model bias.] . Now that we have downloaded some data, we need to assemble it in a format suitable for model training. In fastai, that means creating an object called DataLoaders. . From data to DataLoaders . Now that we have downloaded and verified the data that we want to use, we need to turn it into a DataLoaders object. DataLoaders is a thin class which just stores whatever DataLoader objects you pass to it, and makes them available as train and valid . Although it&#39;s a very simple class, it&#39;s very important in fastai: it provides the data for your model. The key functionality in DataLoaders is provided with just these 4 lines of code (it has some other minor functionality we&#39;ll skip over for now): . class DataLoaders(GetAttr): def __init__(self, *loaders): self.loaders = loaders def __getitem__(self, i): return self.loaders[i] train,valid = add_props(lambda i,self: self[i]) . jargon: DataLoaders: A fastai class which stores whatever DataLoader objects you pass to it, and makes them available as properties. . A DataLoaders object (i.e. the plural) stores multiple DataLoader objects, normally a train and a valid, although it&#39;s possible to have as many as you like. (Later in the book we&#39;ll also learn about the Dataset and Datasets classes, which have the same relationship). . To turn our downloaded data into DataLoaders we need to tell fastai at least four things: . what kinds of data we are working with ; | how to get the list of items ; | how to label these items ; | how to create the validation set. | . So far we have seen a number of factory methods for particular combinations of these things, which are convenient when you have an application and data structure which happens to fit into those predefined methods. For when you don&#39;t, fastai has an extremely flexible system called the data block API. With this API you can fully customize every stage of the creation of your DataLoaders. Here is what we need to create a DataLoaders for the dataset that we just downloaded: . bears = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.3, seed=42), get_y=parent_label, item_tfms=Resize(128)) . Let&#39;s look at each of these sections in turn: . blocks=(ImageBlock, CategoryBlock) . This is a tuple where we specify what types we want for the independent and dependent variables. The independent variable is the thing we are using to make predictions from, and the dependent variable is our target. In this case, our independent variable is a set of images, and our dependent variable are the categories (type of bear) for each image. We will see many other types of block in the rest of this book. . get_items=get_image_files . For this DataLoaders our underlying items will be file paths. We have to tell fastai how to get a list of those files. The get_image_files function takes a path, and returns a list of all of the images in that path (recursively, by default). . splitter=RandomSplitter(valid_pct=0.2, seed=42) . Often, datasets that you download will already have a validation set defined. Sometimes this is done by placing the images for the training and validation sets into different folders. Sometimes it is done by providing a CSV in which each file name is listed along with which dataset it should be in. There are many ways that this can be done, and fastai provides a very general approach which allows you to use one of fastai&#39;s predefined classes for this, or to write your own. In this case, however, we simply want to split our training and validation sets randomly. However, we would like to have the same training/validation split each time we run this notebook, so we fix the random seed. (Computers don&#39;t really know how to create random numbers at all, but simply create lists of numbers which look random. If you provide the same starting point for that list each time — called the seed — then you will get the exact same list each time.) . get_y=parent_label . The independent variable is often referred to as &quot;x&quot; and the dependent variable is often referred to as &quot;y&quot;. So in this section we are telling fastai what function to call to create the labels in our dataset. parent_label is a function provided by fastai which simply gets the name of the folder which a file is in. Because we put each of our bear images into folders based on the type of bear, this is going to give us the labels that we need. . item_tfms=Resize(128) . Our images are all different sizes, and this is a problem for deep learning: we don&#39;t feed the model one image at a time but several (what we call a mini-batch) of them. To group them in a big array (usually called tensor) that is going to go through our model, they all need to be of the same size. So we need to add a transform which will resize these images to the same size. item transforms are pieces of code which run on each individual item, whether it be an image, category, or so forth. fastai includes many predefined transforms; we will use the Resize transform here. . This command has given us a DataBlock object. This is like a template for creating a DataLoaders. We still need to tell fastai the actual source of our data — in this case, the path where the images can be found. . dls = bears.dataloaders(path) . A DataLoaders includes validation and training DataLoaders. A DataLoader is a class which provides batches of a few items at a time to the GPU. We&#39;ll be learning a lot more about this class in the next chapter. When you loop through a DataLoader fastai will give you 64 (by default) items at a time, all stacked up into a single tensor. We can take a look at a few of those items by calling the show_batch method on a DataLoader: . dls.valid.show_batch(max_n=4, rows=1) . By default Resize crops the images to fit a square shape of the size requested, using the full width or height. This can result in losing some important details. Alternatively, you can ask fastai to pad the images with zeros (which is black), or squish/stretch them: . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, rows=1) . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, rows=1) . All of these approaches seem somewhat wasteful, or problematic. If we squished or stretch the images then they end up unrealistic shapes, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy. If we crop the images then we remove some of the features that allow us to recognize them. For instance, if we were trying to recognise the breed of dog or cat, we may end up cropping out a key part of the body or the face necessary to distinguish between similar breeds. If we pad the images then we have a whole lot of empty space, which is just wasted computation for our model, and results in a lower effective resolution for the part of the image we actually use. . Instead, what we normally do in practice is to randomly select part of the image, and crop to just that part. On each epoch (which is one complete pass through all of our images in the dataset) we randomly select a different part of each image. This means that our model can learn to focus on, and recognize, different features in our images. It also reflects how images work in the real world; different photos of the same thing may be framed in slightly different ways. . Here is a another copy of the previous examples, but this time we are replacing Resize with RandomResizedCrop, which is the transform that provides the behaviour described above.The most important parameter to pass in is the min_scale parameter, which determines how much of the image to select at minimum each time. . bears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) dls = bears.dataloaders(path) dls.train.get_idxs = lambda: Inf.ones dls.train.show_batch(max_n=4, rows=1) . . Note: The get_idx assignment in this code is a little bit magic, and you absolutely don&#8217;t have to understand it at this point. So feel free to ignore the entirety of this paragraph! This is just if you&#8217;re curious… Showing different randomly varied versions of the same image is not something we normally have to do in deep learning, so it&#8217;s not something that fastai provides directly. Therefore to draw the picture of data augmentation on the same image, we had to take advantage of fastai&#8217;s sophisticated customisation features. DataLoader has a method called get_idx, which is called to decide which items should be selected next. Normally when we are training, this returns a random permutation of all of the indexes in the dataset. But pretty much everything in fastai can be changed, including how the get_idx method is defined, which means we can change how we sample data. So in this case, we are replacing it with a version which always returns the number one. That way, our DataLoader shows the same image again and again! This is a great example of the flexibility that fastai provides. . In fact, an entirely untrained neural network knows nothing whatsoever about how images behave. It doesn&#39;t even recognise that when an object is rotated by one degree, then it still is a picture of the same thing! So actually training the neural network with examples of images that are in slightly different places, and slightly different sizes, helps it to understand the basic concept of what a object is, and how it can be represented in an image. . This is a specific example of a more general technique, called data augmentation. . Data augmentation . Data augmentation refers to creating random variations of our input data, such that they appear different, but are not expected to change the meaning of the data. Examples of common data augmentation for images are rotation, flipping, perspective warping, brightness changes, contrast changes, and much more. For natural photo images such as the ones we are using here, there is a standard set of augmentations which we have found work pretty well, and are provided with the aug_transforms function. Because the images are now all the same size, we can apply these augmentations to an entire batch of them using the GPU, which will save a lot of time. To tell fastai we want to use these transforms to a batch, we use the batch_tfms parameter. (Note that&#39;s we&#39;re not using RandomResizedCrop in this example, so you can see the differences more clearly; we&#39;re also using double the amount of augmentation compared to the default, for the same reason). . bears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2)) dls = bears.dataloaders(path) dls.train.get_idxs = lambda: Inf.ones dls.train.show_batch(max_n=8, rows=2) . Now that we have assembled our data in a format fit for model training, let&#39;s actually train an image classifier using it. . Training your model, and using it to clean your data . Time to use the same lined of codes as in &lt;&gt; to train our bear classifier.&lt;/p&gt; We don&#39;t have a lot of data for our problem (150 pictures of each sort of bear at most), so to train our model, we&#39;ll use RandomResizedCrop and default aug_transforms for our model, on an image size of 224px, which is fairly standard for image classification. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; bears = bears.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = bears.dataloaders(path) . We can now create our Learner and fine tune it in the usual way. . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.235733 | 0.212541 | 0.087302 | 00:05 | . epoch train_loss valid_loss error_rate time . 0 | 0.213371 | 0.112450 | 0.023810 | 00:05 | . 1 | 0.173855 | 0.072306 | 0.023810 | 00:06 | . 2 | 0.147096 | 0.039068 | 0.015873 | 00:06 | . 3 | 0.123984 | 0.026801 | 0.015873 | 00:06 | . Now let&#39;s see whether the mistakes the model is making is mainly thinking that grizzlies are teddies (that would be bad for safety!), or that grizzlies are black bears, or something else. We can create a confusion matrix: . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Each row here represents all the black, grizzly, and teddy bears in our dataset, respectively. Each column represents the images which the model predicted as black, grizzly, and teddy bears, respectively. Therefore, the diagonal of the matrix shows the images which were classified correctly, and the other, off diagonal, cells represent those which were classified incorrectly. This is called a confusion matrix and is one of the many ways that fastai allows you to view the results of your model. It is (of course!) calculated using the validation set. With the color coding, the goal is to have white everywhere, except the diagonal where we want dark blue. Our bear classifier isn&#39;t making many mistakes! . It&#39;s helpful to see where exactly our errors are occuring, to see whether it&#39;s due to a dataset problem (e.g. images that aren&#39;t bears at all, or are labelled incorrectly, etc), or a model problem (e.g. perhaps it isn&#39;t handling images taken with unusual lighting, or from a different angle, etc). To do this, we can sort out images by their loss. . The loss is a number that is higher if the model is incorrect (and especially if it&#39;s also confident of its incorrect answer), or if it&#39;s correct, but not confident of its correct answer. In a couple chapters we&#39;ll learn in depth how loss is calculated and used in training process. For now, plot_top_losses shows us the images with the highest loss in our dataset. As the title of the output says, each image is labeled with four things: prediction, actual (target label), loss, and probability. The probability here is the confidence level, from zero to one, that the model has assigned to its prediction. . interp.plot_top_losses(5, rows=1) . This output shows that the highest loss is an image that has been predicted as &quot;grizzly&quot; with high confidence. However, it&#39;s labeled (based on our Bing image search) as &quot;black&quot;. We&#39;re not bear experts, but it sure looks to us like this label is incorrect! We should probably change its label to &quot;grizzly&quot;. . The intuitive approach to doing data cleaning is to do it before you train a model. But as you&#39;ve seen in this case, a model can actually help you find data issues more quickly and easily. So we normally prefer to train a quick and simple model first, and then use it to help us with data cleaning. . fastai includes a handy GUI for data cleaning called ImageClassifierCleaner, which allows you to choose a category, and training vs validation set, and view the highest-loss images (in order), along with menus to allow any images to be selected for removal, or relabeling. . #hide_output cleaner = ImageClassifierCleaner(learn) cleaner . NameError Traceback (most recent call last) &lt;ipython-input-4-45a2af6fc334&gt; in &lt;module&gt; 1 #hide_output -&gt; 2 cleaner = ImageClassifierCleaner(learn) 3 cleaner NameError: name &#39;learn&#39; is not defined . . We can see that amongst our black bears is an image that contain two bears, one grizzly, one black. So we should choose &lt;Delete&gt; in the menu under this image. ImageClassifierCleaner doesn&#39;t actually do the deleting or changing of labels for you; it just returns the indices of items to change. So, for instance, to delete (unlink) all images selected for deletion, we would run: . for idx in cleaner.delete(): cleaner.fns[idx].unlink() . To move images where we&#39;ve selected a different category, we would run: . for idx,cat in cleaner.change(): shutil.move(cleaner.fns[idx], path/cat) . s: Cleaning the data or getting it ready for your model are two of the biggest challenges for data scientists, one they say take 90% of their time. The fastai library aims at providing tools to make it as easy as possible. We&#39;ll be seeing more examples of model-driven data cleaning throughout this book. Once we&#39;ve cleaned up our data, we can retrain our model. Try it yourself, and see if your accuracy improves! . . Note: After cleaning the dataset using the above steps, we generally are seeing 100% accuracy on this task. We even see that result when we download a lot less images than the 150 per class we&#8217;re using here. As you can see, the common complaint you need massive amounts of data to do deep learning can be a very long way from the truth! . Now that we have trained our model, let&#39;s see how we can deploy it to be used in practice. . Turning your model into an online application . We are now going to look at what it takes to take this model and turn it into a working online application. We will just go as far as creating a basic working prototype; we do not have the scope in this book to teach you all the details of web application development generally. . Using the model for inference . Once you&#39;ve got a model you&#39;re happy with, you need to save it, so that you can then copy it over to a server where you&#39;ll use it in production. Remember that a model consists of two parts: the architecture, and the trained parameters. The easiest way to save a model is to save both of these, because that way when you load a model you can be sure that you have the matching architecture and parameters. To save both parts, use the export method. . This method even saves the definition of how to create your DataLoaders. This is important, because otherwise you would have to redefine how to transform your data in order to use your model in production. fastai automatically uses your validation set DataLoader for inference by default, so your data augmentation will not be applied, which is generally what you want. . When you call export, fastai will save a file called export.pkl. . learn.export() . Let&#39;s check that file exists, by using the Path.ls method that fastai adds to Python&#39;s Path class: . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . You&#39;ll need this file wherever you deploy your app to. For now, let&#39;s try to create a simple app within our notebook. . When we use a model for getting predictions, instead of training, we call it inference. To create our inference learner from the exported file, we use load_learner (in this case, this isn&#39;t really necessary, since we already have a working Learner in our notebook; we&#39;re just doing it here so you can see the whole process end-to-end): . learn_inf = load_learner(path/&#39;export.pkl&#39;) . When we&#39;re doing inference, we&#39;re generally just getting predicitions for one image at a time. To do this, pass a filename to predict: . learn_inf.predict(&#39;images/grizzly.jpg&#39;) . (&#39;grizzly&#39;, tensor(1), tensor([9.0767e-06, 9.9999e-01, 1.5748e-07])) . This has returned three things: the predicted category in the same format you originally provided, in this case that&#39;s a string), the index of the predicted category, and the probabilities of each category. The last two are based on the order of categories in the vocab of the DataLoaders; that is, the stored list of all possible categories. At inference time, you can access the DataLoaders as an attribute of the Learner: . learn_inf.dls.vocab . (#3) [&#39;black&#39;,&#39;grizzly&#39;,&#39;teddy&#39;] . We can see here that if we index into the vocab with the integer returned by predict then we get back &quot;grizzly&quot;, as expected. Also, note that if we index into the list of probabilities, we see a nearly 1.00 probability that this is a grizzly. . We know how to make predictions from our saved model, so we have everything we need to start building our app. We can do it directly in a Jupyter Notenook. . Creating a Notebook app from the model . To use our model in an application we can simply treat the predict method as a regular function. Therefore, creating an app from the model can be done using any of the myriad of frameworks and techniques available to application developers. . However, most data scientists are not familiar with the world of web application development. So let&#39;s try using something that you do, at this point, know: Jupyter notebooks. It turns out that we can create a complete working web application using nothing but Jupyter notebooks! The two things we need to make this happen are: . IPython widgets (ipywidgets) | Voilà | . IPython widgets are GUI components that bring together JavaScript and Python functionality in a web browser, and can be created and used within a Jupyter notebook. For instance, the image cleaner that we saw earlier in this chapter is entirely written with IPython widgets. However, we don&#39;t want to require users of our application to have to run Jupyter themselves. . That is why Voilà exists. It is a system for making applications consisting of IPython widgets available to end-users, without them having to use Jupyter at all. Voila is taking advantage of the fact that a notebook already is a kind of web application, just a rather complex one that depends on another web application Jupyter itself. Essentially, it helps us automatically convert the complex web application which we&#39;ve already implicitly made (the notebook) into a simpler, easier-to-deploy web application, which functions like a normal web application rather than like a notebook. . But we still have the advantage of developing in a notebook. So with ipywidgets, we can build up our GUI step by step. We will use this approach to create a simple image classifier. First, we need a file upload widget: . #hide_output btn_upload = widgets.FileUpload() btn_upload . . Now we can grab the image: . img = PILImage.create(btn_upload.data[-1]) . . We can use an Output widget to display it: . #hide_output out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) out_pl . . Then we can get our predictions: . pred,pred_idx,probs = learn_inf.predict(img) . ...and use a Label to display them: . #hide_output lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . Prediction: grizzly; Probability: 1.0000 . We&#39;ll need a button to do the classification, it looks exactly like the upload button. . #hide_output btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . ...and a click event handler, that is, a function that will be called when it&#39;s pressed; we can just copy over the lines of code from above: . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . You can test the button now by pressing it, and you should see the image and predictions above update automatically! . We can now put them all in a vertical box (VBox) to complete our GUI: . #hide_output VBox([widgets.Label(&#39;Select your bear!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . . We have written all the code necessary for our app. The next step is to convert it in something we can deploy. . Turning your notebook into a real app . Now that we have everything working in this Jupyter notebook, we can create our application. To do this, create a notebook which contains only the code needed to create and show the widgets that you need, and markdown for any text that you want to appear. Have a look at the bear_classifier notebook in the book repo to see the simple notebook application we created. . Next, install Voila if you have not already, by copying these lines into a Notebook cell, and executing it (if you&#39;re comfortable using the command line, you can also execute these two lines in your terminal, without the ! prefix): . !pip install voila !jupyter serverextension enable voila --sys-prefix . Cells which begin with a ! do not contain Python code, but instead contain code which is passed to your shell, such as bash, power shell in windows, or so forth. If you are comfortable using the command line (which we&#39;ll be learning about later in this book), you can of course simply type these two lines (without the ! prefix) directly into your terminal. In this case, the first line installs the voila library and application, and the second connects it to your existing Jupyter notebook. . Voila runs Jupyter notebooks, just like the Jupyter notebook server you are using now does, except that it does something very important: it removes all of the cell inputs, and only shows output (including ipywidgets), along with your markdown cells. So what&#39;s left is a web application! To view your notebook as a voila web application replace the word &quot;notebooks&quot; in your browser&#39;s URL with: &quot;voila/render&quot;. You will see the same content as your notebook, but without any of the code cells. . Of course, you don&#39;t need to use Voila or ipywidgets. Your model is just a function you can call: pred,pred_idx,probs = learn.predict(img) . So you can use it with any framework, hosted on any platform. And you can take something you&#39;ve prototyped in ipywidgets and Voila and later convert it into a regular web application. We&#39;re showing you this approach in the book because we think it&#39;s a great way for data scientists and other folks that aren&#39;t web development experts to create applications from their models. . We have our app, now let&#39;s deploy it! . Deploying your app . As we now know, you need a GPU to train nearly any useful deep learning model. So, do you need a GPU to use that model in production? No! You almost certainly do not need a GPU to serve your model in production. There&#39;s a few reasons for this: . As we&#39;ve seen, GPUs are only useful when they do lots of identical work in parallel. If you&#39;re doing (say) image classification, then you&#39;ll normally be classifying just one user&#39;s image at a time, and there isn&#39;t normally enough work to do in a single image to keep a GPU busy for long enough for it to be very efficient. So a CPU will often be more cost effective. | An alternative could be to wait for a few users to submit their images, and then batch them up, and do them all at once on a GPU. But then you&#39;re asking your users to wait, rather than getting answers straight away! And you need a high volume site for this to be workable. If you do need this functionality, you can use a tool such as Microsoft&#39;s ONNX Runtime, or AWS Sagemaker | The complexities of dealing with GPU inference are significant. In particular, the GPU&#39;s memory will need careful manual management, and you&#39;ll need some careful queueing system to ensure you only do one batch at a time | There&#39;s a lot more market competition in CPU servers than GPU, as a result of which there&#39;s much cheaper options available for CPU servers. | . Because of the complexity of GPU serving, many systems have sprung up to try to automate this. However, managing and running these systems is themselves complex, and generally requires compiling your model into a different form that&#39;s specialized for that system. It doesn&#39;t make sense to deal with this complexity until/unless your app gets popular enough that it makes clear financial sense for you to do so. . For at least the initial prototype of your application, and for any hobby projects that you want to show off, you can easily host them for free. The best place and the best way to do this will vary over time so check the book website for the most up-to-date recommendations. As we&#39;re writing this book in 2020 the simplest (and free!) approach is called Binder. To publish your web app on Binder, you follow these steps: . Add your notebook to a GitHub repository, | Paste the URL of that repo in the URL field of Binder as shown in &lt;&gt;, &lt;/li&gt; Change the &quot;File&quot; dropdown to instead select &quot;URL&quot;, | In the Path field, enter /voila/render/name.ipynb (replacing name.ipynb as appropriate for your notebook): | Click the &quot;Copy the URL&quot; button and paste it somewhere safe. | Click &quot;Launch&quot;. | &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Deploying to Binder . The first time you do this Binder will take around 5 minutes to build your site. In other words, is it finding a virtual machine which can run your app, allocating storage, collecting the files needed for Jupyter, for your notebook, and for presenting your notebook as a web application. It&#39;s doing all of this behind the scenes. . Finally, once it has started the app running, it will navigate your browser to your new web app. You can share the URL you copied to allow others to access your app as well. . For other (both free and paid) options for deploying your web app, be sure to take a look at the book web site. . You may well want to deploy your application onto mobile devices, or edge devices such as a Raspberry Pi. There are a lot of libraries and frameworks to allow you to integrate a model directly into a mobile application. However these approaches tend to require a lot of extra steps and boilerplate, and do not always support all the PyTorch and fastai layers that your model might use. In addition, the work you do will depend on what kind of mobile devices you are targeting for deployment. So you might need to do some work to run on iOS devices, different work to run on newer Android devices, different work for older Android devices, etc.. Instead, we recommend wherever possible that you deploy the model itself to a server, and have your mobile or edge application connect to it as a web service. . There is quite a few upsides to this approach. The initial installation is easier, because you only have to deploy a small GUI application, which connects to the server to do all the heavy lifting. More importantly perhaps, upgrades of that core logic can happen on your server, rather than needing to be distributed to all of your users. Your server can have a lot more memory and processing capacity than most edge devices, and it is far easier to scale those resources if your model becomes more demanding. The hardware that you will have on a server is going to be more standard and more easily supported by fastai and PyTorch, so you don&#39;t have to compile your model into a different form. . There are downsides too, of course. Your application will require a network connection, and there will be some latency each time the model is called. It takes a while for a neural network model to run anyway, so this additional network latency may not make a big difference to your users in practice. In fact, since you can use better hardware on the server, the overall latency may even be less! If your application uses sensitive data then your users may be concerned about an approach which sends that data to a remote server, so sometimes privacy considerations will mean that you need to run the model on the edge device. Sometimes this can be avoided by having an on premise server, such as inside a company&#39;s firewall. Managing the complexity and scaling the server can create additional overhead, whereas if your model runs on the edge devices then each user is bringing their own compute resources, which leads to easier scaling with an increasing number of users (also known as horizontal scaling). . A: I&#39;ve had a chance to see up close how the mobile ML landscape is changing in my work. We offer an iPhone app that depends on computer vision and for years we ran our own computer vision models in the cloud. This was the only way to do it then since those models needed significant memory and compute resources and took minutes to process. This approach required building not only the models (fun!) but infrastructure to ensure a certain number of &quot;compute worker machines&quot; was absolutely always running (scary), that more machines would automatically come online if traffic increased, that there was stable storage for large inputs and outputs, that the iOS app could know and tell the user how their job was doing, etc... Nowadays, Apple provides APIs for converting models to run efficiently on device and most iOS devices have dedicated ML hardware, so we run our new models on device. So, in a few years that strategy has gone from impossible to possible but it&#39;s still not easy. In our case it&#39;s worth it, for a faster user experience and to worry less about servers. What works for you will depend, realistically, on the user experience you&#39;re trying to create and what you personally find it easy to do. If you really know how to run servers, do it. If you really know how to build native mobile apps, do that. There are many roads up the hill. Overall, we&#39;d recommend using a simple CPU-based server approach where possible, for as long as you can get away with it. If you&#39;re lucky enough to have a very successful application, then you&#39;ll be able to justify the investment in more complex deployment approaches at that time. . Congratulations, you have succesfully built a deep learning model and deployed it! Now is a good time to take a pause and think about what could go wrong. . How to avoid disaster . In practice, a deep learning model will be just one piece of a much bigger system. As we discussed at the start of this chapter, a data product requires thinking about the entire end to end process within which our model lives. In this book, we can&#39;t hope to cover all the complexity of managing deployed data products, such as managing multiple versions of models, A/B testing, canarying, refreshing the data (should we just grow and grow our datasets all the time, or should we regularly remove some of the old data), handling data labelling, monitoring all this, detecting model rot, and so forth. However, there is an excellent book that covers many deployment issues, which is Building Machine Learning Powered Applications, by Emmanuel Ameisen. In this section, we will give an overview of some of the most important issues to consider. . One of the biggest issues with this is that understanding and testing the behavior of a deep learning model is much more difficult than most code that you would write. With normal software development you can analyse the exact steps that the software is taking, and carefully study with of these steps match the desired behaviour that you are trying to create. But with a neural network the behavior emerges from the models attempt to match the training data, rather than being exactly defined. . This can result in disaster! For instance, let&#39;s say you really were rolling out a bear detection system which will be attached to video cameras around the campsite, and will warn campers of incoming bears. If we used a model trained with the dataset we downloaded, there are going to be all kinds of problems in practice, such as: . working with video data instead of images ; | handling nighttime images, which may not appear in this dataset ; | dealing with low resolution camera images ; | ensuring results are returned fast enough to be useful in practice ; | recognising bears in positions that are rarely seen in photos that people post online (for example from behind, partially covered by bushes, or when a long way away from the camera). | . A big part of the issue is that the kinds of photos that people are most likely to upload to the Internet are the kinds of photos that do a good job of clearly and artistically displaying their subject matter. So we may need to do a lot of our own data collection and labelling to create a useful system. . This is just one example of the more general problem of out of domain data. That is to say, there may be data that our model sees in production which is very different to what it saw during training. There isn&#39;t really a complete technical solution to this problem; instead we have to be careful about our approach to rolling out the technology. . There are other reasons we need to be careful too. One very common problem is domain shift; this is where the type of data that our model sees changes over time. For instance, an insurance company may use a deep learning model as part of their pricing and risk algorithm, but over time the type of customers that they attract, and the type of risks that they represent, may change so much that the original training data is no longer relevant. . Out of domain data, and domain shift, are examples of the problem that you can never fully know the entire behaviour of your neural network. They have far too many parameters to be able to analytically understand all of their possible behaviours. This is the natural downside of the thing that they&#39;re so good at — their flexibility in being able to solve complex problems where we may not even be able to fully specify our preferred solution approaches. The good news, however, is that there are ways to mitigate these risks using a carefully thought out process. The details of this will vary depending on the details of the problem you are solving, but we will attempt to lay out here a high-level approach summarized in &lt;&gt; which we hope will provide useful guidance.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Deployment process . Where possible, the first step is to use an entirely manual process, with your deep learning model approach running in parallel, but not being used directly to drive any actions. The humans involved in the manual process should look at the deep learning outputs and check whether they make sense. For instance, with our bear classifier a park ranger could have a screen displaying any time a possible bear sighting occurred in any camera, and simply highlight them in red on the screen. The park ranger would still be expected to be just as alert as before the model was deployed; the model is simply helping to check for problems at this point. . The second step is to try to limit the scope of the model, and have it carefully supervised by people. For instance, do a small geographically and time constrained trial of the model-driven approach. Rather than rolling your bear classifier out in every national park throughout the country, pick a single observation post, for a one-week period, and have a park ranger check each alert before it goes out. . Then, gradually increase the scope of your rollout. As you do so, ensure that you have really good reporting systems in place, to make sure that you are aware of any significant changes to the actions being taken compared to your manual process. For instance, if the number of bear alerts doubles or halves after rollout of the new system in some location we should be very concerned. Try to think about all the ways in which your system could go wrong, and then think about what measure or report or picture could reflect that problem, and then ensure that your regular reporting includes that information. . j: I started a company 20 years ago called Optimal Decisions which used machine learning and optimisation to help giant insurance companies set their pricing, impacting tens of billions of dollars of risks. We used the approaches described above to manage the potential downsides of something that might go wrong. Also, before we worked with our clients to put anything in production, we tried to simulate the impact by testing the end to end system on their previous year&#39;s data. It was always quite a nerve-wracking process, putting these new algorithms in production, but every rollout was successful. . Unforeseen consequences and feedback loops . One of the biggest challenges in rolling out a model is that your model may change the behaviour of the system it is a part of. For instance, consider a &quot;predictive policing&quot; algorithm that predicts more crime in certain neighborhoods, causing more police officers to be sent to those neighborhoods, which can result in more crime being recorded in those neighborhoods, and so on. In the Royal Statiscal Society paper To predict and serve, Kristian Lum and William Isaac write: &quot;predictive policing is aptly named: it is predicting future policing, not future crime&quot;. . Part of the issue in this case is that in the presence of bias (which we&#39;ll discuss in depth in the next chapter), feedback loops can result in negative implications of that bias getting worse and worse. For instance, there are concerns that this is already happening in the US, where there is significant bias in arrest rates on racial grounds. According to the ACLU, &quot;despite roughly equal usage rates, Blacks are 3.73 times more likely than whites to be arrested for marijuana&quot;. The impact of this bias, along with the roll-out of predictive policing algorithms in many parts of the US, led Bärí Williams to write in the NY Times: &quot;The same technology that’s the source of so much excitement in my career is being used in law enforcement in ways that could mean that in the coming years, my son, who is 7 now, is more likely to be profiled or arrested — or worse — for no reason other than his race and where we live.&quot; . A helpful exercise prior to rolling out a significant machine learning system is to consider this question: &quot;what would happen if it went really, really well?&quot; In other words, what if the predictive power was extremely high, and its ability to influence behaviour was extremely significant? In that case, who would be most impacted? What would the most extreme results potentially look like? How would you know what was really going on? . Such a thought exercise might help you to construct a more careful rollout plan, ongoing monitoring systems, and human oversight. Of course, human oversight isn&#39;t useful if it isn&#39;t listened to; so make sure that there are reliable and resilient communication channels so that the right people will be aware of issues, and will have the power to fix them. . Get writing! . One of the things our students have found most helpful to solidify their understanding of this material is to write it down. There is no better test of your understanding of a topic than attempting to teach it to somebody else. This is helpful even if you never show your writing to anybody — but it&#39;s even better if you share it! So we recommend that, if you haven&#39;t already, you start a blog. Now that you&#39;ve finished chapter 2, and have learned how to train and deploy models, you&#39;re well placed to write your first blog post about your deep learning journey. What&#39;s surprised you? What opportunities do you see for deep learning in your field? What obstacles do you see? . Rachel Thomas, co-founder of fast.ai, wrote in the article Why you (yes, you) should blog: . asciidoc ____ The top advice I would give my younger self would be to start blogging sooner. Here are some reasons to blog: * It’s like a resume, only better. I know of a few people who have had blog posts lead to job offers! * Helps you learn. Organizing knowledge always helps me synthesize my own ideas. One of the tests of whether you understand something is whether you can explain it to someone else. A blog post is a great way to do that. * I’ve gotten invitations to conferences and invitations to speak from my blog posts. I was invited to the TensorFlow Dev Summit (which was awesome!) for writing a blog post about how I don’t like TensorFlow. * Meet new people. I’ve met several people who have responded to blog posts I wrote. * Saves time. Any time you answer a question multiple times through email, you should turn it into a blog post, which makes it easier for you to share the next time someone asks. ____ . Perhaps her most important tip is this: &quot;You are best positioned to help people one step behind you. The material is still fresh in your mind. Many experts have forgotten what it was like to be a beginner (or an intermediate) and have forgotten why the topic is hard to understand when you first hear it. The context of your particular background, your particular style, and your knowledge level will give a different twist to what you’re writing about.&quot; . We&#39;ve provided full details on how to set up a blog in an appendix &quot;Creating a blog&quot;. If you don&#39;t have a blog already, jump over to that chapter now, because we&#39;ve got a really great approach set up for you to start blogging, for free, with no ads--and you can even use Jupyter Notebook! . Questionnaire . Provide an example of where the bear classification model might work poorly, due to structural or style differences to the training data | Where do text models currently have a major deficiency? | What are possible negative societal implications of text generation models? | In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process? | What kind of tabular data is deep learning particularly good at? | What&#39;s a key downside of directly using a deep learning model for recommendation systems? | What are the steps of the Drivetrain approach? | How do the steps of the Drivetrain approach map to a recommendation system? | Create an image recognition model using data you curate, and deploy it on the web. | What is DataLoaders? | What four things do we need to tell fastai to create DataLoaders? | What does the splitter parameter to DataBlock do? | How do we ensure a random split always gives the same validation set? | What letters are often used to signify the independent and dependent variables? | What&#39;s the difference between crop, pad, and squish resize approaches? When might you choose one over the other? | What is data augmentation? Why is it needed? | What is the difference between item_tfms and batch_tfms? | What is a confusion matrix? | What does export save? | What is it called when we use a model for getting predictions, instead of training? | What are IPython widgets? | When might you want to use CPU for deployment? When might GPU be better? | What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC? | What are 3 examples of problems that could occur when rolling out a bear warning system in practice? | What is &quot;out of domain data&quot;? | What is &quot;domain shift&quot;? | What are the 3 steps in the deployment process? | For a project you&#39;re interested in applying deep learning to, consider the thought experiment &quot;what would happen if it went really, really well?&quot; | Start a blog, and write your first blog post. For instance, write about what you think deep learning might be useful for in a domain you&#39;re interested in. | Further research . Consider how the Drivetrain approach maps to a project or problem you&#39;re interested in. | When might it be best to avoid certain types of data augmentation? | &lt;/div&gt; . | . .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_production.html",
            "relUrl": "/2020/03/19/_production.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Chapter 5",
            "content": "[[chapter_pet_breeds]] Image classification . Now that we understand what deep learning is, what it&#39;s for, and how to create and deploy a model, it&#39;s time for us to go deeper! In an ideal world deep learning practitioners wouldn&#39;t have to know every detail of how things work under the hood… But as yet, we don&#39;t live in an ideal world. The truth is, to make your model really work, and work reliably, there&#39;s a lot of details you have to get right. And a lot of details that you have to check. This process requires being able to look inside your neural network as it trains, and as it makes predictions, find possible problems, and know how to fix them. . So, from here on in the book we are going to do a deep dive into the mechanics of deep learning. What is the architecture of a computer vision model, an NLP model, a tabular model, and so on. How do you create an architecture which matches the needs of your particular domain? How do you get the best possible results from the training process? How do you make things faster? What do you have to change as your datasets change? . We will start by repeating the same basic applications that we looked at in the first chapter, but we are going to do two things: . make them better; | apply them to a wider variety of types of data. | . In order to do these two things, we will have to learn all of the pieces of the deep learning puzzle. This includes: different types of layers, regularisation methods, optimisers, putting layers together into architectures, labelling techniques, and much more. We are not just going to dump all of these things out, but we will introduce them progressively as needed, to solve an actual problem related to the project we are working on. . From dogs and cats, to pet breeds . In our very first model we learnt how to classify dogs versus cats. Just a few years ago this was considered a very challenging task. But today, it is far too easy! We will not be able to show you the nuances of training models with this problem, because we get the nearly perfect result without worrying about any of the details. But it turns out that the same dataset also allows us to work on a much more challenging problem: figuring out what breed of pet is shown in each image. . In the first chapter we presented the applications as already solved problems. But this is not how things work in real life. We start with some dataset which we know nothing about. We have to understand how it is put together, how to extract the data we need from it, and what that data looks like. For the rest of this book we will be showing you how to solve these problems in practice, including all of these intermediate steps necessary to understand the data that we working with and test our modelling as we go. . We have already downloaded the pets dataset. We can get a path to this dataset using the same code we saw in &lt;&gt;:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from fastai2.vision.all import * path = untar_data(URLs.PETS) . Now if we are going to understand how to extract the breed of each pet from each image were going to need to understand how this data is laid out. Such details of data layout are a vital piece of the deep learning puzzle. Data is usually provided in one of these two ways: . Individual files representing items of data, such as text documents or images, possibly organised into folders or with filenames representing information about those items, or | A table of data, such as in CSV format, where each row is an item, each row which may include filenames providing a connection between the data in the table and data in other formats such as text documents and images. | . There are exceptions to these rules, particularly in domains such as genomics, where there can be binary database formats or even network streams, but overall the vast majority of the datasets your work with use some combination of the above two formats. . To see what is in our dataset we can use the ls method: . path.ls() . (#3) [Path(&#39;annotations&#39;),Path(&#39;images&#39;),Path(&#39;models&#39;)] . We can see that this dataset provides us with &quot;images&quot; and &quot;annotations&quot; directories. The website for this dataset tells us that the annotations directory contains information about where the pets are rather than what they are. In this chapter we will be doing classification, not localisation, which is to say that we care about what the pets are not where they are. Therefore we will ignore the annotations directory for now. So let&#39;s have a look inside the images directory: . (path/&quot;images&quot;).ls() . (#7394) [Path(&#39;images/great_pyrenees_173.jpg&#39;),Path(&#39;images/wheaten_terrier_46.jpg&#39;),Path(&#39;images/Ragdoll_262.jpg&#39;),Path(&#39;images/german_shorthaired_3.jpg&#39;),Path(&#39;images/american_bulldog_196.jpg&#39;),Path(&#39;images/boxer_188.jpg&#39;),Path(&#39;images/staffordshire_bull_terrier_173.jpg&#39;),Path(&#39;images/basset_hound_71.jpg&#39;),Path(&#39;images/staffordshire_bull_terrier_37.jpg&#39;),Path(&#39;images/yorkshire_terrier_18.jpg&#39;)...] . Most functions and methods in fastai which return a collection use a class called L. L can be thought of as an enhanced version of the ordinary Python list type, with added conveniences for common operations. For instance, when we display an object of this class in a notebook it appears in the format you see above. The first thing that is shown is the number of items in the collection, prefixed with a #. You&#39;ll also see in the above output that the list is suffixed with a &quot;…&quot;. This means that only the first few items are displayed — which is a good thing, because we would not want more than 7000 filenames on our screen! . By examining these filenames, we see how they appear to be structured. Each file name contains the pet breed, and then ancharacter, a number, and finally the file extension. We need to create a piece of code that extracts the breed from a single Path. Jupyter notebook makes this easy, because we can gradually build up something that works, and then use it for the entire dataset. We do have to be careful to not make too many assumptions at this point. For instance, if you look carefully you may notice that some of the pet breeds contain multiple words, so we cannot simply break at the first `` character that we find. To allow us to test our code, let&#39;s pick out one of these filenames: . fname = (path/&quot;images&quot;).ls()[0] . The most powerful and flexible way to extract information from strings like this is to use a regular expression, also known as a regex. A regular expression is a special string, written in the regular expression language, which specifies a general rule for for deciding if another string passes a test (i.e., &quot;matches&quot; the regular expression), and also possibly for plucking a particular part or parts out of that other string. . In this case, we need a regular expressin that extracts the pet breed from the file name. . We do not have the space to give you a complete regular expression tutorial here, particularly because there are so many excellent ones online. And we know that many of you will already be familiar with this wonderful tool. If you&#39;re not, that is totally fine — this is a great opportunity for you to rectify that! We find that regular expressions are one of the most useful tools in our programming toolkit, and many of our students tell us that it is one of the things they are most excited to learn about. So head over to Google and search for regular expressions tutorial now, and then come back here after you&#39;ve had a good look around. The book website also provides a list of our favorites. . a: Not only are regular expresssions dead handy, they also have interesting roots. They are &quot;regular&quot; because they they were originally examples of a &quot;regular&quot; language, the lowest rung within the &quot;Chomsky hierarchy&quot;, a grammar classification due to the same linguist Noam Chomskey who wrote Syntactic Structures, the pioneering work searching for the formal grammar underlying human language. This is one of the charms of computing: it may be that the hammer you reach for every day in fact came from a space ship. When you are writing a regular expression, the best way to start is just to try it against one example at first. Let&#39;s use the findall method to try a regular expression against the filename of the fname object: . re.findall(r&#39;(.+)_ d+.jpg$&#39;, fname.name) . [&#39;great_pyrenees&#39;] . This regular expression plucks out all the characters leading up to the last underscore character, as long as the subsequence characters are numerical digits and then the jpeg file extension. . Now that we confirmed the regular expression works for the example, let&#39;s use it to label the whole dataset. Fastai comes with many classes to help you with your labelling. For labelling with regular expressions, we can use the RegexLabeller class. We can use this in the data block API that we saw in &lt;&gt; (in fact, we nearly always use the data block API--it&#39;s so much more flexible than the simple factory methods we saw in &lt;&gt;):&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; pets = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) dls = pets.dataloaders(path/&quot;images&quot;) . One important piece of this DataBlock call that we haven&#39;t seen before is in these two lines: . item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75) . These lines implement a fastai data augmentation strategy which we call presizing. Presizing is a particular way to do image augmentation, which is designed to minimize data destruction while maintaining good performance. . Presizing . We need our images to have the same dimensions, so that they can collate into tensors to be passed to the GPU. We also want to minimize the number of distinct augmentation computations we perform. So the performance requirement suggests that we should, where possible, compose our augmentation transforms into fewer transforms (to reduce the number of computations, and reduce the number of lossy operations) and transform the images into uniform sizes (to run compute efficiently on the GPU). . The challenge is that, if performed after resizing down to the augmented size, various common data augmentation transforms might introduce spurious empty zones, degrade data, or both. For instance, rotating an image by 45 degrees fills corner regions of the new bounds with emptyness, which will not teach the model anything. Many rotation and zooming operations will require interpolating to create pixels. These interpolated pixels are derived from the original image data but are still of lower quality. . To workaround these challenges, presizing adopts two strategies that are shown in &lt;&gt;:&lt;/p&gt; First, resizing images to relatively &quot;large dimensions&quot; that is, dimensions significantly larger than the target training dimensions. | Second, composing all of the common augmentation operations (including a resize to the final target size) into one, and performing the combined operation on the GPU only once at the end of processing, rather than performing them individually and interpolating multiple times. | The first step, the resize, creates images large enough that they have spare margin to allow further augmentation transforms on their inner regions without creating empty zones. This transformation works by resizing to to a square, using a large crop size. On the training set, the crop area is chosen randomly, and the size of the crop is selected to cover the entire width or height of the image, whichever is smaller. . In the second step, the GPU is used for all data augmentation, and all of the potentially destructive operations are done together, with a single interpolation at the end. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Presizing on the training set . This picture shows the two steps: . Crop full width or height: This is in item_tfms, so it&#39;s applied to each individual image before it is copied to the GPU. It&#39;s used to ensure all images are the same size. On the training set, the crop area is chosen randomly. On the validation set, the center square of the image is always chosen | Random crop and augment: This is in batch_tfms, so it&#39;s applied to a batch all at once on the GPU, which means it&#39;s fast. On the validation set, only the resize to the final size needed for the model is done here. On the training set, the random crop and any other augmentation is done first. | To implement this process in fastai you use Resize as an item transform with a large size, and RandomResizedCrop as a batch transform with a smaller size. RandomResizedCrop will be added for you if you include the min_scale parameter in your aug_transform function, as you see in the DataBlock call above. Alternatively, you can use pad or squish instead of crop (the default) for the initial Resize. . You can see in this example the difference between an image which has been zoomed, interpolated, rotated, and then interpolated again on the right (which is the approach used by all other deep learning libraries), compared to an image which has been zoomed and rotated as one operation, and then interpolated just once on the left (the fastai approach): . You can see here that the image on the right is less well defined, and has reflection padding artifacts in the bottom left, and the grass in the top left has disappeared entirely. We find that in practice using presizing significantly improves the accuracy of models, and often results in speedups too. . Checking your data looks right is extremely important before training a model. There are simple ways to do this (and debug if needed) in the fastai library, let&#39;s look at them now. . Checking and debugging a DataBlock . We can never just assume that our code is working perfectly. Writing a DataBlock is just like writing a blueprint. You will get an error message if you have a syntax error somewhere in your code but you have no garanty that your template is going to work on your source of data as you intend. The first thing to do before we trying to train a model is to use the show_batch method and have a look at your data: . dls.show_batch(rows=1, cols=3) . Have a look at each image, and check that each one seems to have the correct label for that breed of pet. Often, data scientists work with data with which they are not familiar as domain experts me: for instance, I actually don&#39;t know what a lot of these pet breeds are. Since I am not an expert on pet breeds, I would use Google images at this point to search for a few of these breeds, and make sure the images looks similar to what I see in this output. . If you made a mistake while building your DataBlock it is very likely you won&#39;t see it before this step. To debug this, we encourage you to use the summary method. It will attempt to create a batch from the source you give it, with a lot of details. Also, if it fails, you will see exactly at which point the error happens, and the library will try to give you some help. For instance, one common mistake is to forget to put a Resize transform, ending up with pictures of different sizes and not able to batch them. Here is what the summary would look like in that case (note that the exact text may have changed since the time of writing, but it will give you an idea): . #hide_output pets1 = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;)) pets1.summary(path/&quot;images&quot;) . Setting-up type transforms pipelines Collecting items from /home/jhoward/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize Building one sample Pipeline: PILBase.create starting from /home/jhoward/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_31.jpg applying PILBase.create gives PILImage mode=RGB size=500x414 Pipeline: partial -&gt; Categorize starting from /home/jhoward/.fastai/data/oxford-iiit-pet/images/american_pit_bull_terrier_31.jpg applying partial gives american_pit_bull_terrier applying Categorize gives TensorCategory(13) Final sample: (PILImage mode=RGB size=500x414, TensorCategory(13)) Setting up after_item: Pipeline: ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor Building one batch Applying item_tfms to the first sample: Pipeline: ToTensor starting from (PILImage mode=RGB size=500x414, TensorCategory(13)) applying ToTensor gives (TensorImage of size 3x414x500, TensorCategory(13)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Error! It&#39;s not possible to collate your items in a batch Could not collate the 0-th members of your tuples because got the following shapes torch.Size([3, 414, 500]),torch.Size([3, 375, 500]),torch.Size([3, 500, 281]),torch.Size([3, 203, 300]) . - RuntimeError Traceback (most recent call last) &lt;ipython-input-18-8c0a3d421ca2&gt; in &lt;module&gt; 4 splitter=RandomSplitter(seed=42), 5 get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;)) -&gt; 6 pets1.summary(path/&#34;images&#34;) ~/git/fastai2/fastai2/data/block.py in summary(self, source, bs, **kwargs) 172 why = _find_fail_collate(s) 173 print(&#34;Make sure all parts of your samples are tensors of the same size&#34; if why is None else why) --&gt; 174 raise e 175 176 if len([f for f in dls.train.after_batch.fs if f.name != &#39;noop&#39;])!=0: ~/git/fastai2/fastai2/data/block.py in summary(self, source, bs, **kwargs) 166 print(&#34; nCollating items in a batch&#34;) 167 try: --&gt; 168 b = dls.train.create_batch(s) 169 b = retain_types(b, s[0] if is_listy(s) else s) 170 except Exception as e: ~/git/fastai2/fastai2/data/load.py in create_batch(self, b) 124 def retain(self, res, b): return retain_types(res, b[0] if is_listy(b) else b) 125 def create_item(self, s): return next(self.it) if s is None else self.dataset[s] --&gt; 126 def create_batch(self, b): return (fa_collate,fa_convert)[self.prebatched](b) 127 def do_batch(self, b): return self.retain(self.create_batch(self.before_batch(b)), b) 128 def one_batch(self): ~/git/fastai2/fastai2/data/load.py in fa_collate(t) 44 b = t[0] 45 return (default_collate(t) if isinstance(b, _collate_types) &gt; 46 else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence) 47 else default_collate(t)) 48 ~/git/fastai2/fastai2/data/load.py in &lt;listcomp&gt;(.0) 44 b = t[0] 45 return (default_collate(t) if isinstance(b, _collate_types) &gt; 46 else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence) 47 else default_collate(t)) 48 ~/git/fastai2/fastai2/data/load.py in fa_collate(t) 43 def fa_collate(t): 44 b = t[0] &gt; 45 return (default_collate(t) if isinstance(b, _collate_types) 46 else type(t[0])([fa_collate(s) for s in zip(*t)]) if isinstance(b, Sequence) 47 else default_collate(t)) ~/anaconda3/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py in default_collate(batch) 53 storage = elem.storage()._new_shared(numel) 54 out = elem.new(storage) &gt; 55 return torch.stack(batch, 0, out=out) 56 elif elem_type.__module__ == &#39;numpy&#39; and elem_type.__name__ != &#39;str_&#39; 57 and elem_type.__name__ != &#39;string_&#39;: RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 414 and 375 in dimension 2 at /opt/conda/conda-bld/pytorch_1579022060824/work/aten/src/TH/generic/THTensor.cpp:612 . Setting-up type transforms pipelines Collecting items from /home/sgugger/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize Building one sample Pipeline: PILBase.create starting from /home/sgugger/.fastai/data/oxford-iiit-pet/images/american_bulldog_83.jpg applying PILBase.create gives PILImage mode=RGB size=375x500 Pipeline: partial -&gt; Categorize starting from /home/sgugger/.fastai/data/oxford-iiit-pet/images/american_bulldog_83.jpg applying partial gives american_bulldog applying Categorize gives TensorCategory(12) Final sample: (PILImage mode=RGB size=375x500, TensorCategory(12)) Setting up after_item: Pipeline: ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor Building one batch Applying item_tfms to the first sample: Pipeline: ToTensor starting from (PILImage mode=RGB size=375x500, TensorCategory(12)) applying ToTensor gives (TensorImage of size 3x500x375, TensorCategory(12)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Error! It&#39;s not possible to collate your items in a batch Could not collate the 0-th members of your tuples because got the following shapes: torch.Size([3, 500, 375]),torch.Size([3, 375, 500]),torch.Size([3, 333, 500]), torch.Size([3, 375, 500]) . You can see exactly how we gathered the data and split it, how we went from a filename to a sample (the tuple image, category), then what item transforms were applied and how it failed to collate those samples in a batch (because of the different shapes). . Once you think your data looks right, we generally recommend the next step should be creating a simple model. We often see people procrastinate the training of an actual model for far too long. As a result, they don&#39;t actually get to find out what their baseline results look like. Perhaps it doesn&#39;t need lots of fancy domain specific engineering. Or perhaps the data doesn&#39;t seem to train it all. These are things that you want to know as soon as possible. So we will use the same simple model that we used in &lt;&gt;:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2) . epoch train_loss valid_loss error_rate time . 0 | 1.491732 | 0.337355 | 0.108254 | 00:18 | . epoch train_loss valid_loss error_rate time . 0 | 0.503154 | 0.293404 | 0.096076 | 00:23 | . 1 | 0.314759 | 0.225316 | 0.066306 | 00:23 | . As we&#39;ve briefly discussed before, the table shown when we fit a model shows us the results after each epoch of training. Remember, an epoch is one complete pass through all of the images in the data. The columns shown are the average loss over the items of the training set, the loss on the validation set, and any metrics that you requested — in this case, the error rate. . Remember that loss is whatever function we&#39;ve decided to use to optimise the parameters of our model. But we haven&#39;t actually told fastai what loss function we want to use. So what is it doing? Fastai will generally try to select an appropriate loss function based on what kind of data and model you are using. In this case you have image data, and a categorical outcome, so fastai will default to using cross entropy loss. . Cross entropy loss . Cross entropy loss is a loss function which is similar to the loss function we used in the previous chapter, but (as we&#39;ll see) has two benefits: . It works even when our dependent variable has more than two categories | It results in faster and more reliable training. | . In order to understand how cross entropy loss works for dependent variables with more than two categories, we first have to understand what the actual data and activations that are loss function is seen look like. . Viewing activations and labels . Let&#39;s have a look at the activations of our model. To actually get a batch of real data from our DataLoaders, we can use the one_batch method: . x,y = dls.one_batch() . As you see, this returns the dependent, and the independent variables, as a mini-batch. Let&#39;s see what is actually contained in our dependent variable: . y . TensorCategory([11, 0, 0, 5, 20, 4, 22, 31, 23, 10, 20, 2, 3, 27, 18, 23, 33, 5, 24, 7, 6, 12, 9, 11, 35, 14, 10, 15, 3, 3, 21, 5, 19, 14, 12, 15, 27, 1, 17, 10, 7, 6, 15, 23, 36, 1, 35, 6, 4, 29, 24, 32, 2, 14, 26, 25, 21, 0, 29, 31, 18, 7, 7, 17], device=&#39;cuda:5&#39;) . Our batch size is 64, so we have 64 rows in this tensor. Each row is a single integer between zero and 36, representing our 37 possible pet breeds. We can view the predictions (that is, the activations of the final layer of our neural network) using Learner.get_preds. This function either takes a dataset index (0 for train and 1 for valid) or an iterator of batches. Thus, we can pass it a simple list with our batch to get our predictions. It returns predictions and targets by default, but since we already have the targets, we can effectively ignore them by assigning to the special variable _: . preds,_ = learn.get_preds(dl=[(x,y)]) preds[0] . tensor([7.9069e-04, 6.2350e-05, 3.7607e-05, 2.9260e-06, 1.3032e-05, 2.5760e-05, 6.2341e-08, 3.6400e-07, 4.1311e-06, 1.3310e-04, 2.3090e-03, 9.9281e-01, 4.6494e-05, 6.4266e-07, 1.9780e-06, 5.7005e-07, 3.3448e-06, 3.5691e-03, 3.4385e-06, 1.1578e-05, 1.5916e-06, 8.5567e-08, 5.0773e-08, 2.2978e-06, 1.4150e-06, 3.5459e-07, 1.4599e-04, 5.6198e-08, 3.4108e-07, 2.0813e-06, 8.0568e-07, 4.3381e-07, 1.0069e-05, 9.1020e-07, 4.8714e-06, 1.2734e-06, 2.4735e-06]) . The actual predictions are 37 probabilities between zero and one, which add up to 1 in total. . len(preds[0]),preds[0].sum() . (37, tensor(1.0000)) . To transform the activations of our model into predictions like this, we used something called the softmax activation function. . Softmax . In our classification model, an activation function called softmax in the final layer is used to ensure that the activations are between zero and one, and that they sum to one. . Softmax is similar to the sigmoid function, which we saw earlier; sigmoid looks like this: . plot_function(torch.sigmoid, min=-4,max=4) . We can apply this function to a single column of activations from a neural network, and get back a column of numbers between zero and one. So it&#39;s a very useful activation function for our final layer. . Now think about what happens if we want to have more categories in our target (such as our 37 pet breeds). That means we&#39;ll need more activations than just a single column: we need an activation per category. We can create, for instance, a neural net that predicts &quot;3&quot;s and &quot;7&quot;s that returns two activations, one for each class--this will be a good first step towards creating the more general approach. Let&#39;s just use some random numbers with a standard deviation of 2 (so we multiply randn by 2) for this example, assuming we have six images and two possible categories (where the first columns represents &quot;3&quot;s and the second is &quot;7&quot;s): . acts = torch.randn((6,2))*2 acts . tensor([[ 0.6734, 0.2576], [ 0.4689, 0.4607], [-2.2457, -0.3727], [ 4.4164, -1.2760], [ 0.9233, 0.5347], [ 1.0698, 1.6187]]) . We can&#39;t just take the sigmoid of this directly, since we don&#39;t get rows that add to one (i.e we want the probability of being a &quot;3&quot; plus the probability of being a &quot;7&quot; to add to one): . acts.sigmoid() . tensor([[0.6623, 0.5641], [0.6151, 0.6132], [0.0957, 0.4079], [0.9881, 0.2182], [0.7157, 0.6306], [0.7446, 0.8346]]) . In &lt;&gt;, the neural net created a single activation per image, which we passed through the sigmoid function. That single activation represented the confidence that the input was a &quot;3&quot;. Binary problems are a special case of classification problems, because the target can be treated as a single boolean value, as we did in mnist_loss. Binary problems can also be thought of as part of the more general group of classifiers with any number of categories--where in this case we happen to have 2 categories. As we saw in the bear classifier, our neural net will return one activation per category.&lt;/p&gt; So in the binary case, what do those activations really indicate? A single pair of activations simply indicates the relative confidence of being a &quot;3&quot; versus being a &quot;7&quot;. The overall values, whether they are both high, or both low, don&#39;t matter--all that matters it which is higher, and by how much. . We would expect that since this is just another way of representing the same problem (in the binary case) that we would be able to use sigmoid directly on the two-activation version of our neural net. And indeed we can! We can just take the difference between the neural net activations, because that reflects how much more sure we are of being a &quot;3&quot; vs a &quot;7&quot;, and then take the sigmoid of that: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; (acts[:,0]-acts[:,1]).sigmoid() . tensor([0.6025, 0.5021, 0.1332, 0.9966, 0.5959, 0.3661]) . The second column (the probability of being a &quot;7&quot;) will then just be that subtracted from one. We need a way to do all this that also works for more than two columns. It turns out that this function, called softmax, is exactly that: . def softmax(x): return exp(x) / exp(x).sum(dim=1, keepdim=True) . jargon: Exponential function (exp): Literally defined as e**x, where e is a special number approximately equal to 2.718. It is the inverse of the natural logarithm function. Note that exp is always positive, and it increases very rapidly! . Let&#39;s check that softmax returns the same values as sigmoid for the first column, and that subtracted from one for the second column: . sm_acts = torch.softmax(acts, dim=1) sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . Softmax is the multi-category equivalent of sigmoid--we have to use it any time we have more than two categories, and the probabilities of the categories must add to one. (We often use it even when there&#39;s just two categories, just to make things a bit more consistent.) We could create other functions that have the properties that all activations are between zero and one, and sum to one; however, no other function has the same relationship to the sigmoid function, which we&#39;ve seen is smooth and symmetric. Also, we&#39;ll see shortly that the softmax function works well hand-in-hand with the loss function we will look at in the next section. . If we have three output activations, such as in our bear classifier, calculating softmax for a single bear image would then look like something like this: . . What does this function do in practice? Taking the exponential ensures all our numbers are positive, and then dividing by the sum ensures we are going to have a bunch of numbers that add up to one. The exponential also has a nice property: if one of the numbers in our activations x is slightly bigger than the others, the exponential will amplify this (since it grows, well... exponentially) which means that in the softmax, that number will be closer to 1. . Intuitively, the Softmax function really wants to pick one class among the others, so it&#39;s ideal for training a classifier when we know each picture has a definite label. (Note that it may be less ideal during inference, as you might want your model to sometimes tell you it doesn&#39;t recognize any of the classes is has seen during training, and not pick a class because it has a slightly bigger activation score. In this case, it might be better to train a model using multiple binary output columns, each using a sigmoid activation.) . Softmax is the first part of the cross entropy loss, the second part is log likeklihood. . Log likelihood . When we calculated the loss for our MNIST example in the last chapter we used. . def mnist_loss(inputs, targets): inputs = inputs.sigmoid() return torch.where(targets==1, 1-inputs, inputs).mean() . Just like we moved from sigmoid to softmax, we need to extend the loss function to work with more than just binary classification, to classifying any number of categories (in this case, we have 37 categories). Our activations, after softmax, are between zero and one, and sum to one for each row in the batch of predictions. Our targets are integers between 0 and 36. . In the binary case, we used torch.where to select between inputs and 1-inputs. When we treat a binary classification as a general classification problem with two categories, it actually becomes even easier, because (as we saw in the softmax section) we now have two columns, containing the equivalent of inputs and 1-inputs. So all we need to do is select from the appropriate column. Let&#39;s try to implement this in PyTorch. For our synthetic &quot;3&quot;s and &quot;7&quot; example, let&#39;s say these are our labels: . targ = tensor([0,1,0,1,1,0]) . ...and these are the softmax activations: . sm_acts . tensor([[0.6025, 0.3975], [0.5021, 0.4979], [0.1332, 0.8668], [0.9966, 0.0034], [0.5959, 0.4041], [0.3661, 0.6339]]) . Then for each item of targ we can use that to select that column of sm_acts using tensor indexing, like so: . idx = range(6) sm_acts[idx, targ] . tensor([0.6025, 0.4979, 0.1332, 0.0034, 0.4041, 0.3661]) . To see exactly what&#39;s happening here, let&#39;s put all the columns together in a table. Here, the first two columns are out activations, then we have the targets, the row index, and finally the result shown immediately above: . 3 7 targ idx loss . 0.602469 | 0.397531 | 0 | 0 | 0.602469 | . 0.502065 | 0.497935 | 1 | 1 | 0.497935 | . 0.133188 | 0.866811 | 0 | 2 | 0.133188 | . 0.99664 | 0.00336017 | 1 | 3 | 0.00336017 | . 0.595949 | 0.404051 | 1 | 4 | 0.404051 | . 0.366118 | 0.633882 | 0 | 5 | 0.366118 | . Looking at this table, you can see that the final column can be calculated by taking the targ and idx columns as indices into the 2-column matrix containing the 3 and 7 columns. That&#39;s what sm_acts[idx, targ] is actually doing. . The really interesting thing here is that this actually works just as well with more than two columns. To see this, consider what would happen if we added a activation column above for every digit (zero through nine), and then targ contained a number from zero to nine. As long as the activation columns sum to one (as they will, if we use softmax), then we&#39;ll have a loss function that shows how well we&#39;re predicting each digit. . We&#39;re only picking the loss from the column containing the correct label. We don&#39;t to consider the other columns, because by the definition of softmax, they add up to one minus the activation corresponding to the correct label. Therefore, making the activation for the correct label as high as possible, must mean we&#39;re also decreasing the activations of the remaining columns. . PyTorch provides a function that does exactly the same thing as sm_acts[range(n), targ] (except it takes the negative, because when applying the log afterward, we will have negative numbers), called nll_loss (NLL stands for negative log likelihood): . -sm_acts[idx, targ] . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . F.nll_loss(sm_acts, targ, reduction=&#39;none&#39;) . tensor([-0.6025, -0.4979, -0.1332, -0.0034, -0.4041, -0.3661]) . Despite the name being negative log likelihood, this PyTorch function does not take the log (we will see why in the next section). First, let&#39;s see why taking the logarithm can be useful. . Taking the log . This does work quite well as a loss function, but we can make it a bit better. The problem is that we are using probabilities, and probabilities cannot be smaller than zero, or greater than one. But that means that our model will not care about whether it predicts 0.99 versus 0.999, because those numbers are so close together. But in another sense, 0.999 is 10 times more confident than 0.99. So we wish to transform our numbers between zero and one to instead be between negative infinity and infinity. There is a function available in maths which does exactly this: the logarithm (available as torch.log). It is not defined for numbers less than zero, and looks like this: . plot_function(torch.log, min=0,max=4) . Does &quot;logarithm&quot; ring a bell? The logarithm function has this identity: . y = a**b a = log(y,b) . In this case, we&#39;re assuming that log(y,b) returns log y base b. However, PyTorch actually doesn&#39;t define log this way: log in Python uses the special number e (2.718...) as the base. . Perhaps a logarithm is something that you have not thought about for the last 20 years or so. But it&#39;s a mathematical idea which is going to be really critical for many things in deep learning, so now would be a great time to refresh your memory. The key thing to know about logarithms is this relationship: . log(a*b) = log(a)+log(b) . When we see it in that format looks a bit boring; but have a think about what this really means. It means that logarithms increase linearly when the underlying signal increases exponentially or multiplicatively. This is used for instance in the Richter scale of earthquake severity, and the dB scale of noise levels. It&#39;s also often used on financial charts, where we want to show compound growth rates more clearly. Computer scientists love using logarithms, because it means that modification, which can create really really large and really really small numbers, can be replaced by addition, which is much less likely to result in scales which are difficult for our computer to handle. . s: It&#39;s not just computer scientists that love logs! Until computers came along, engineers and scientists used a special ruler called a &quot;slide rule&quot; that did multiplication by adding logarithms. Logarithms are widely used in physics, for multiplying very big or very small numbers, and many other fields. . Taking the mean of the positive or negative log of our probabilities (depending on whether it&#39;s the correct or incorrect class) gives us the negative log likelihood loss. In PyTorch, nll_loss assumes that you already took the log of the softmax, so it doesn&#39;t actually do the logarithm for you. . . Warning: The &quot;NLL&quot; in &quot;nll_loss&quot; stands for &quot;negative log likelihood&quot;, but it doesn&#8217;t actually take the log at all! It assumes you have already taken the log. PyTorch has a function called &quot;log_softmax&quot; which combines &quot;log&quot; and &quot;softmax&quot; in a fast and accurate way. . When we first take the softmax, and then the log likelihood of that, that combination is called cross entropy loss. In PyTorch, this is available as nn.CrossEntropyLoss (which, in practice, actually does log_softmax and then nll_loss). . loss_func = nn.CrossEntropyLoss() . As you see, this is a class. Instantiating it gives you an object which behaves like a function: . loss_func(acts, targ) . tensor(1.8045) . All PyTorch loss functions are provided in two forms: the class form seen above, and also a plain functional form, available in the F namespace: . F.cross_entropy(acts, targ) . tensor(1.8045) . Either one works fine and can be used in any situation. We&#39;ve noticed that most people tend to use the class version, and that&#39;s more often used in PyTorch official docs and examples, so we&#39;ll tend to use that too. . By default PyTorch loss functions take the mean of the loss of all items. You can use reduction=&#39;none&#39; to disable that: . nn.CrossEntropyLoss(reduction=&#39;none&#39;)(acts, targ) . tensor([0.5067, 0.6973, 2.0160, 5.6958, 0.9062, 1.0048]) . s: An interesting feature about cross entropy loss appears when we consider its gradient. The gradient of cross_entropy(a,b) is just softmax(a)-b. Since softmax(a) is just the final activation of the model, that means that the gradient is proportional to the difference between the prediction and the target. This is the same as mean squared error in regression (assuming there&#39;s no final activation function such as that added by y_range), since the gradient of (a-b)**2 is 2*(a-b). Since the gradient is linear, that means that we won&#39;t see sudden jumps or exponential increases in gradients, which should lead to smoother training of models. . We have now seen all the pieces hidden behind our loss function. While it gives us a number on how well (or bad) our model is doing, it does nothing to help us know if it&#39;s actually any good. Let&#39;s now see some ways to interpret our model predictions. . Model Interpretation . It&#39;s very hard to interpret loss functions directly, because they are designed to be things which computers can differentiate and optimise, not things that people can understand. That&#39;s why we have metrics. These are not used in the optimisation process, but just used to help us poor humans understand what&#39;s going on. In this case, our accuracy is looking pretty good already! So where are we making mistakes? . We saw in &lt;&gt; that we can use a confusion matrix to see where our model is doing well, and where it&#39;s doing badly:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; #width 600 interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . Oh dear, in this case, a confusion matrix is very hard to read. We have 37 different breeds of pet, which means we have 37×37 entries in this giant matrix! Instead, we can use the most_confused method, which just shows us the cells of the confusion matrix with the most incorrect predictions (here with at least 5 or more): . interp.most_confused(min_val=5) . [(&#39;american_pit_bull_terrier&#39;, &#39;staffordshire_bull_terrier&#39;, 10), (&#39;Ragdoll&#39;, &#39;Birman&#39;, 6)] . Since we are not pet breed experts, it is hard for us to know whether these category errors reflect actual difficulties in recognising breeds. So again, we turn to Google. A little bit of googling tells us that the most common category errors shown here are actually breed differences which even expert breeders sometimes disagree about. So this gives us some comfort that we are on the right track. . So we seem to have a good baseline. What can we do now ot make it even better? . Improving our model . We will now look a at a range of techniques to improve the training of our model and make it better. While doing so, we will explain a little bit more about transfer learning and how to fine-tune our pretrained model as best as possible, without breaking the pretrained weights. . The first thing we need to set when training a model is the learning rate. We saw in the previous chapter that it needed to be just right to train as efficiently as possible, so how do we pick a good one? fastai provides something called the Learning rate finder for this. . Learning rate finder . One of the most important things we can do when training a model is to make sure that we have the right learning rate. If our learning rate is too low, it&#39;s can take many many epochs. Not only does this waste time, but it also means that we may have problems with overfitting, because every time we do a complete pass through the data, we give our model a chance to memorise it. . So let&#39;s just make our learning rate really high, right? Sure, let&#39;s try that and see what happens: . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1, base_lr=0.1) . epoch train_loss valid_loss error_rate time . 0 | 8.946717 | 47.954632 | 0.893775 | 00:20 | . epoch train_loss valid_loss error_rate time . 0 | 7.231843 | 4.119265 | 0.954668 | 00:24 | . That did not look good. Here&#39;s what happened. The optimiser stepped in the correct direction, but it stepped so far that it totally overshot the minimum loss. Repeating that multiple times makes it get further and further away, not closer and closer! . What do we to find the perfect learning rate, not too high, and not too low? In 2015 the researcher Leslie Smith came up with a brilliant idea, called the learning rate finder. His idea was to start with a very very small learning rate, something so small that we would never expect it to be too big to handle. We use that for one mini batch, find what the losses afterwards, and then increase the learning rate by some percentage (e.g. doubling it each time). Then we do another mini batch, track the loss, and double the learning rate again. We keep doing this until the loss gets worse, instead of better. This is the point where we know we have gone too far. We then select a learning rate a bit lower than this point. Our advice is to pick either: . one order of magnitude less than where the minimum loss was achieved (i.e. the minimum divided by 10) | the last point where the loss was clearly decreasing. | . The Learning Rate Finder computes those points on the curve to help you. Both these rules usually give around the same value. In the first chapter, we didn&#39;t specified a learning rate, using the default value from the fastai library (which is 1e-3). . learn = cnn_learner(dls, resnet34, metrics=error_rate) lr_min,lr_steep = learn.lr_find() . print(f&quot;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&quot;) . Minimum/10: 8.32e-03, steepest point: 6.31e-03 . We can see on this plot that in the range 1e-6 to 1e-3, nothing really happens and the model doesn&#39;t train. Then the loss starts to decrease until it reaches a minimum then increases again. We don&#39;t want a learning rate greater than 1e-1 as it will give a training that diverges (you can try for yourself) but 1e-1 is already too high: at this stage we left the period where the loss was decreasing steadily. . In this learning rate plot it appears that a learning rate around 3e-3 would be appropriate, so let&#39;s choose that. . . Note: The learning rate finder plot has a logarithmic scale, which is why the middle point between 1e-3 and 1e-2 is between 3e-3 and 4e-3. This is because we care mostly about the order of magnitude of the learning rate. . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2, base_lr=3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.071820 | 0.427476 | 0.133965 | 00:19 | . epoch train_loss valid_loss error_rate time . 0 | 0.738273 | 0.541828 | 0.150880 | 00:24 | . 1 | 0.401544 | 0.266623 | 0.081867 | 00:24 | . Something really interesting about the learning rate finder is that it was only discovered in 2015. Neural networks have been under development since the 1950s. Throughout that time finding a good learning rate has been, perhaps, the most important and challenging issue for practitioners. The idea does not require any advanced maths, giant computing resources, huge datasets, or anything else that would make it inaccessible to any curious researcher. Furthermore, Leslie Smith, was not part of some exclusive Silicon Valley lab, but was working as a naval researcher. All of this is to say: breakthrough work in deep learning absolutely does not require access to vast resources, elite teams, or advanced mathematical ideas. There is lots of work still to be done which requires just a bit of common sense, creativity, and tenacity. . Now that we have a good learning rate to train our model, let&#39;s look at how we can finetune the weights of a pretrained model. . Unfreezing and transfer learning . We discussed briefly in &lt;&gt; how transfer learning works. We saw that the basic idea is that a pretrained model, trained potentially on millions of data points (such as ImageNet), is fine tuned for some other task. But what does this really mean?&lt;/p&gt; We now know that a convolutional neural network consists of many layers with a non-linear activation function between each and one or more final linear layers, with an activation functions such as softmax at the very end. The final linear layer uses a matrix with enough columns such that the output size is the same as the number of classes in our model (assuming that we are doing classification). . This final linear layer is unlikely to be of any use for us, when we are fine tuning in a transfer learning setting, because it is specifically designed to classify the categories in the original pretraining dataset. So when we do transfer learning we remove it, and throw it away, and replace it with a new linear layer with the correct number of outputs for our desired task (in this case, there would be 37 activations). . This newly added linear layer will have entirely random weights. Therefore, our model prior to fine tuning has entirely random outputs. But that does not mean that it is an entirely random model! All of the layers prior to the last one have been carefully trained to be good at image classification tasks in general. As we saw in the images from the Zeiler and Fergus paper in &lt;&gt; (see &lt;&gt; and followings), the first layers encode very general concepts such as finding gradients and edges, and later layers encode concepts that are still very useful for us, such as finding eyeballs and fur.&lt;/p&gt; We want to train a model in such a way that we allow it to remember all of these generally useful ideas from the pretrained model, use them to solve our particular task (classify pet breeds), and only adjust them as required for the specifics of our particular task. . Our challenge than when fine tuning is to replace the random weights in our added linear layers with weights that correctly achieve our desired task (classifying pet breeds) without breaking the carefully pretrained weights and the other layers. There is actually a very simple trick to allow this to happen: tell the optimiser to only update the weights in those randomly added final layers. Don&#39;t change the weights in the rest of the neural network at all. This is called freezing those pretrained layers. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; When we create a model from a pretrained network fastai automatically freezes all of the pretrained layers for us. When we call the fine_tune method fastai does two things: . train the randomly added layers for one epoch, with all other layers frozen ; | unfreeze all of the layers, and train them all for the number of epochs requested. | . Although this is a reasonable default approach, it is likely that for your particular dataset you may get better results by doing things slightly differently. The fine_tune method has a number of parameters you can use to change its behaviour, but it might be easiest for you to just call the underlying methods directly if you want to get some custom behavior. Remember that you can see the source code for the method by using the following syntax: . learn.fine_tune?? . So let&#39;s try doing this manually ourselves. First of all we will train the randomly added layers for three epochs, using fit_one_cycle. As mentioned in &lt;&gt;, fit_one_cycle is the suggested way to train models without using fine_tune. We&#39;ll see why later in the book; in short, what fit_one_cycle does is to start training at a low learning rate, gradually increase it for the first section of training, and then gradually decrease it again for the last section of training.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.188042 | 0.355024 | 0.102842 | 00:20 | . 1 | 0.534234 | 0.302453 | 0.094723 | 00:20 | . 2 | 0.325031 | 0.222268 | 0.074425 | 00:20 | . And then we will unfreeze the model: . learn.unfreeze() . ...and run lr_find again, because having more layers to train, and weights that have already been trained for 3 epochs, means our previously found learning rate isn&#39;t appropriate and more: . learn.lr_find() . (1.0964782268274575e-05, 1.5848931980144698e-06) . Note that the graph is a little different from when we had random weights: we don&#39;t have that sharp descent that indicates the model is training. That&#39;s because our model has been trained already. Here we have a somewhat flat area before a sharp increase, and we should take a point well before that sharp increase, for instance 1e-5. The point with the maximum gradient isn&#39;t what we look for here and should be ignored. . Let&#39;s train at a suitable learning rate: . learn.fit_one_cycle(6, lr_max=1e-5) . epoch train_loss valid_loss error_rate time . 0 | 0.263579 | 0.217419 | 0.069012 | 00:24 | . 1 | 0.253060 | 0.210346 | 0.062923 | 00:24 | . 2 | 0.224340 | 0.207357 | 0.060217 | 00:24 | . 3 | 0.200195 | 0.207244 | 0.061570 | 00:24 | . 4 | 0.194269 | 0.200149 | 0.059540 | 00:25 | . 5 | 0.173164 | 0.202301 | 0.059540 | 00:25 | . This has improved our model a bit, but there&#39;s more we can do. The deepest layers of our pretrained model might not need as high a learning rate as the last ones, so we should probably use different learning rates for those, something called discriminative learning rates. . Discriminative learning rates . Even after we unfreeze, we still care a lot about the quality of those pretrained weights. We would not expect that the best learning rate for those pretrained parameters would be as high as the randomly added parameters — even after we have tuned those randomly added parameters for a few epochs. Remember, the pretrained weights have been trained for hundreds of epochs, on millions of images. . In addition, do you remember the images we saw in &lt;&gt;, showing what each layer learns? The first layer learns very simple foundations, like edge and gradient detectors; these are likely to be just as useful for nearly any task. The later layers learn much more complex concepts, like &quot;eye&quot; and &quot;sunset&quot;, which might not be useful in your task at all (maybe you&#39;re classifying car models, for instance). So it makes sense to let the later layers fine-tune more quickly than earlier layers.&lt;/p&gt; Therefore, fastai by default does something called discriminative learning rates. This was originally developed in the ULMFiT approach to NLP transfer learning that we introduced in &lt;&gt;. Like many good ideas in deep learning, it is extremely simple: use a lower learning rate for the early layers of the neural network, and a higher learning rate for the later layers (and especially the randomly added layers). The idea is based on insights developed by Jason Yosinski, who showed in 2014 that when transfer learning different layers of a neural network should train at different speeds, as seen in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Impact of different layers and training methods on transfer learning (Yosinski) . Fastai lets you pass a Python slice object anywhere that a learning rate is expected. The first value past will be the learning rate in the earliest layer of the neural network, and the second value will be the learning rate in the final layer. The layers in between will have learning rates that are multiplicatively equidistant throughout that range. Let&#39;s use this approach to replicate the previous training, but this time we&#39;ll only set the lowest layer of our net to a learning rate of 1e-6; the other layers will scale up to 1e-4. Let&#39;s train for a while and see what happens. . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) learn.unfreeze() learn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss error_rate time . 0 | 1.145300 | 0.345568 | 0.119756 | 00:20 | . 1 | 0.533986 | 0.251944 | 0.077131 | 00:20 | . 2 | 0.317696 | 0.208371 | 0.069012 | 00:20 | . epoch train_loss valid_loss error_rate time . 0 | 0.257977 | 0.205400 | 0.067659 | 00:25 | . 1 | 0.246763 | 0.205107 | 0.066306 | 00:25 | . 2 | 0.240595 | 0.193848 | 0.062246 | 00:25 | . 3 | 0.209988 | 0.198061 | 0.062923 | 00:25 | . 4 | 0.194756 | 0.193130 | 0.064276 | 00:25 | . 5 | 0.169985 | 0.187885 | 0.056157 | 00:25 | . 6 | 0.153205 | 0.186145 | 0.058863 | 00:25 | . 7 | 0.141480 | 0.185316 | 0.053451 | 00:25 | . 8 | 0.128564 | 0.180999 | 0.051421 | 00:25 | . 9 | 0.126941 | 0.186288 | 0.054127 | 00:25 | . 10 | 0.130064 | 0.181764 | 0.054127 | 00:25 | . 11 | 0.124281 | 0.181855 | 0.054127 | 00:25 | . Now the fine tuning is working great! . Fastai can show us a graph of the training and validation loss: . learn.recorder.plot_loss() . As you can see, the training loss keeps getting better and better. But notice that eventually the validation loss improvement slows, and sometimes even gets worse! This is the point at which the model is starting to over fit. In particular, the model is becoming overconfident of its predictions. But this does not mean that it is getting less accurate, necessarily. Have a look at the table of training results per epoch, and you will often see that the accuracy continues improving, even as the validation loss gets worse. In the end what matters is your accuracy, or more generally your chosen metrics, not the loss. The loss is just the function we&#39;ve given the computer to help us to optimise. . Another decision you have to make when training the model is for how long. . Selecting the number of epochs . Often you will find that you are limited by time, rather than generalisation and accuracy, when choosing how many epochs to train for. So your first approach to training should be to simply pick a number of epochs that will train in the amount of time that you are happy to wait for. Have a look at the training and validation loss plots, likely showed above, and in particular your metrics, and if you see that they are still getting better even in your final epochs, then you know that you have not trained for too long. . On the other hand, you may well see that the metrics you have chosen are really getting worse at the end of training. Remember, it&#39;s not just that were looking for the validation loss to get worse, but your actual metrics. Your validation loss will first of all during training get worse because it gets overconfident, and only later will get worse because it is incorrectly memorising the data. We only care in practice about the latter issue. Our loss function is just something, remember, that we used to allow our optimiser to have something it could differentiate and optimise; it&#39;s not actually the thing we care about in practice. . Before the days of 1cycle training it was very common to save the model at the end of each epoch, and then select whichever model had the best accuracy, out of all of the models saved in each epoch. This is known as early stopping. However, with one cycle training, it is very unlikely to give you the best answer, because those epochs in the middle occur before the learning rate has had a chance to reach the small values, where it can really find the best result. Therefore, if you find that you have overfit, what you should actually do is to retrain your model from scratch, and this time select a total number of epochs based on where your previous best results were found. . If we&#39;ve got the time to train for more epochs, we may want to instead use that time to train more parameters, that is use a deeper architecture. . Deeper architectures . In general, a model with more parameters can model your data more accurately. (There are lots and lots of caveats to this generalisation, and it depends on the specifics of the architectures you are using, but it is a reasonable rule of thumb for now.) For most of the architectures that we will be seeing in this book you can create larger versions of them by simply adding more layers. However, since we want to use pretrained models, we need to make sure that we choose a number of layers that has been already pretrained for us. . This is why, in practice, architectures tend to come in a small number of variants. For instance, the resnet architecture that we are using in this chapter comes in 18, 34, 50, 101, and 152 layer variants, pre-trained on ImageNet. A larger (more layers and parameters; sometimes described as the &quot;capacity&quot; of a model) version of a resnet will always be able to give us a better training loss, but it can suffer more from overfitting, because it has more parameters to over fit with. . In general, a bigger model has the ability to better capture the real underlying relationships in your data, and also to capture and memorise the specific details of your individual images. . However, using a deeper model is going to require more GPU RAM, so we may need to lower the size of our batches to avoid out-of-memory errors. This happens when you try to fit too much inside your GPU and looks like: . Cuda runtime error: out of memory . You may have to restart your notebook when this happens, and the way to solve it is to use a smaller batch size, which means we will pass smaller groups of images at any given time through our model. We can pass the batch size we want to the call creating our DataLoaders with bs=. . The other downside of deeper architectures is that they take quite a bit longer to train. One thing that can speed things up a lot is mixed precision training. This refers to using less precise numbers (half precision floating point, also called fp16) where possible during training. As we are writing this words (early 2020) nearly all current NVIDIA GPUs support a special feature called tensor cores which can dramatically (2x-3x) speed up neural network training. They also require a lot less GPU memory. To enable this feature in fastai, just add to_fp16() after your Learner creation (you also need to import the module). . You can&#39;t really know ahead of time what the best architecture for your particular problem is, until you try training some. So let&#39;s try a resnet 50 now with mixed precision: . from fastai2.callback.fp16 import * learn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16() learn.fine_tune(6, freeze_epochs=3) . epoch train_loss valid_loss error_rate time . 0 | 1.427505 | 0.310554 | 0.098782 | 00:21 | . 1 | 0.606785 | 0.302325 | 0.094723 | 00:22 | . 2 | 0.409267 | 0.294803 | 0.091340 | 00:21 | . epoch train_loss valid_loss error_rate time . 0 | 0.261121 | 0.274507 | 0.083897 | 00:26 | . 1 | 0.296653 | 0.318649 | 0.084574 | 00:26 | . 2 | 0.242356 | 0.253677 | 0.069012 | 00:26 | . 3 | 0.150684 | 0.251438 | 0.065629 | 00:26 | . 4 | 0.094997 | 0.239772 | 0.064276 | 00:26 | . 5 | 0.061144 | 0.228082 | 0.054804 | 00:26 | . You&#39;ll see here we&#39;ve gone back to using fine_tune, since it&#39;s so handy! We can pass freeze_epochs to tell fastai how many epochs to train for while frozen. It will automatically change learning rates appropriately for most datasets. . In this case, we&#39;re not seeing a clear win from the deeper model. This is useful to remember--bigger models aren&#39;t necessarily better models for your particular case! Make sure you try small models before you start scaling up. . Summary . In this chapter we learned some important practical tips, both for getting our image data ready for modeling (presizing; data block summary) and for fitting the model (learning rate finder, unfreezing, discriminative learning rates, setting the number of epochs, and using deeper architectures). Using these tools will help you to build more accurate image models, more quickly. . We also learned about cross entropy loss. This part of the book is worth spending plenty of time on. You aren&#39;t likely to need to actually implement cross entropy loss from scratch yourself in practice, but it&#39;s really important you understand the inputs to and output from that function, because it (or a variant of it, as we&#39;ll see in the next chapter) is used in nearly everything classification model. So when you want to debug a model, or put a model in production, or improve the accuracy of a model, you&#39;re going to need to be able to look at its activations and loss, and understand what&#39;s going on, and why. You can&#39;t do that properly if you don&#39;t understand your loss function. . If cross entropy loss hasn&#39;t &quot;clicked&quot; for you just yet, don&#39;t worry--you&#39;ll get there! First, go back to the last chapter and make sure you really understand mnist_loss. Then work gradually through the cells of the notebook for this chapter, where we step through each piece of cross entropy loss. Make sure you understand what each calculation is doing, and why. Try creating some small tensors yourself and pass them into the functions, to see what they return. . Remember: the choices made in cross entropy loss are not the only possible choices that could have been made. Just like when we looked at regression, we could choose between mean squared error and mean absolute difference (L1), we could change the details inside cross entropy loss too. If you have other ideas for possible functions that you think might work, feel free to give them a try in this chapter&#39;s notebook! (Fair warning though: you&#39;ll probably find that the model will be slower to train, and less accurate. That&#39;s because the gradient of cross entropy loss is proportional to the difference between the activation and the target, so SGD always gets a nicely scaled step for the weights.) . Questionnaire . Why do we first resize to a large size on the CPU, and then to a smaller size on the GPU? | If you are not familiar with regular expressions, find a regular expression tutorial, and some problem sets, and complete them. Have a look on the book website for suggestions. | What are the two ways in which data is most commonly provided, for most deep learning datasets? | Look up the documentation for L and try using a few of the new methods is that it adds. | Look up the documentation for the Python pathlib module and try using a few methods of the Path class. | Give two examples of ways that image transformations can degrade the quality of the data. | What method does fastai provide to view the data in a DataLoader? | What method does fastai provide to help you debug a DataBlock? | Should you hold off on training a model until you have thoroughly cleaned your data? | What are the two pieces that are combined into cross entropy loss in PyTorch? | What are the two properties of activations that softmax ensures? Why is this important? | When might you want your activations to not have these two properties? | Calculate the &quot;exp&quot; and &quot;softmax&quot; columns of &lt;&gt; yourself (i.e. in a spreadsheet, with a calculator, or in a notebook).&lt;/li&gt; Why can&#39;t we use torch.where to create a loss function for datasets where our label can have more than two categories? | What is the value of log(-2)? Why? | What are two good rules of thumb for picking a learning rate from the learning rate finder? | What two steps does the fine_tune method do? | In Jupyter notebook, how do you get the source code for a method or function? | What are discriminative learning rates? | How is a Python slice object interpreted when past as a learning rate to fastai? | Why is early stopping a poor choice when using one cycle training? | What is the difference between resnet 50 and resnet101? | What does to_fp16 do? | &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Further research . Find the paper by Leslie Smith that introduced the learning rate finder, and read it. | See if you can improve the accuracy of the classifier in this chapter. What&#39;s the best accuracy you can achieve? Have a look on the forums and book website to see what other students have achieved with this dataset, and how they did it. | &lt;/div&gt; | . .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_pet_breeds.html",
            "relUrl": "/2020/03/19/_pet_breeds.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Chapter 12",
            "content": "[[chapter_nlp_dive]] A language model from scratch . We&#39;re now ready to go deep... deep into deep learning! You already learned how to train a basic neural network, but how do you go from there to creating state of the art models? In this part of the book we&#39;re going to uncover all of the mysteries, starting with language models. . We saw in &lt;&gt; how to finetune a pretrained language model to build a text classifier, in this chapter, we will explain to you what exactly is inside that model, and what an RNN is. First, let&#39;s gather some data that will allow us to quickly prototype our various models.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The data . Whenever we start working on a new problem, we always first try to think of the simplest dataset we can which would allow us to try out methods quickly and easily, and interpret the results. When we started working on language modelling a few years ago, we didn&#39;t find any datasets that would allow for quick prototyping, so we made one. We call it human numbers, and it simply contains the first 10,000 words written out in English. . j: One of the most common practical mistakes I see even amongst highly experienced practitioners is failing to use appropriate datasets at appropriate times during the analysis process. In particular, most people tend to start with datasets which are too big and too complicated. . We can download, extract, and take a look at our dataset in the usual way: . from fastai2.text.all import * path = untar_data(URLs.HUMAN_NUMBERS) . path.ls() . (#2) [Path(&#39;train.txt&#39;),Path(&#39;valid.txt&#39;)] . Let&#39;s open those two files and see what&#39;s inside. At first we&#39;ll join all of those texts together and ignore the split train/valid given by the dataset, we will come back to it later on: . lines = L() with open(path/&#39;train.txt&#39;) as f: lines += L(*f.readlines()) with open(path/&#39;valid.txt&#39;) as f: lines += L(*f.readlines()) lines . (#9998) [&#39;one n&#39;,&#39;two n&#39;,&#39;three n&#39;,&#39;four n&#39;,&#39;five n&#39;,&#39;six n&#39;,&#39;seven n&#39;,&#39;eight n&#39;,&#39;nine n&#39;,&#39;ten n&#39;...] . We take all those lines and concatenate them in one big stream. To mark when we go from one number to the next, we use a &#39;.&#39; as separation: . text = &#39; . &#39;.join([l.strip() for l in lines]) text[:100] . &#39;one . two . three . four . five . six . seven . eight . nine . ten . eleven . twelve . thirteen . fo&#39; . Let&#39;s use word tokenization for this dataset, by splitting on spaces: . tokens = text.split(&#39; &#39;) tokens[:10] . [&#39;one&#39;, &#39;.&#39;, &#39;two&#39;, &#39;.&#39;, &#39;three&#39;, &#39;.&#39;, &#39;four&#39;, &#39;.&#39;, &#39;five&#39;, &#39;.&#39;] . To numericalize, we have to create a list of all the unique tokens (our vocab): . vocab = L(*tokens).unique() vocab . (#30) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;three&#39;,&#39;four&#39;,&#39;five&#39;,&#39;six&#39;,&#39;seven&#39;,&#39;eight&#39;,&#39;nine&#39;...] . Then we can convert our tokens into numbers by looking up the index of each in the vocab: . word2idx = {w:i for i,w in enumerate(vocab)} nums = L(word2idx[i] for i in tokens) nums . (#63095) [0,1,2,1,3,1,4,1,5,1...] . Now that we have some small dataset on which language modelling should be an easy task, we can build our first model. . Our first language model from scratch . One simple way to turn this into a neural network would be to specify that we are going to predict each word based on the previous three words. Therefore, we could create a list of every sequence of three words as independent variables, and the next word after each sequence as the dependent variable. . We can do that with plain Python. Let us do it first with tokens just to confirm what it looks like: . L((tokens[i:i+3], tokens[i+3]) for i in range(0,len(tokens)-4,3)) . (#21031) [([&#39;one&#39;, &#39;.&#39;, &#39;two&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;three&#39;, &#39;.&#39;], &#39;four&#39;),([&#39;four&#39;, &#39;.&#39;, &#39;five&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;six&#39;, &#39;.&#39;], &#39;seven&#39;),([&#39;seven&#39;, &#39;.&#39;, &#39;eight&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;nine&#39;, &#39;.&#39;], &#39;ten&#39;),([&#39;ten&#39;, &#39;.&#39;, &#39;eleven&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;twelve&#39;, &#39;.&#39;], &#39;thirteen&#39;),([&#39;thirteen&#39;, &#39;.&#39;, &#39;fourteen&#39;], &#39;.&#39;),([&#39;.&#39;, &#39;fifteen&#39;, &#39;.&#39;], &#39;sixteen&#39;)...] . Now we will do it with tensors of the numericalized values, which is what the model will actually use: . seqs = L((tensor(nums[i:i+3]), nums[i+3]) for i in range(0,len(nums)-4,3)) seqs . (#21031) [(tensor([0, 1, 2]), 1),(tensor([1, 3, 1]), 4),(tensor([4, 1, 5]), 1),(tensor([1, 6, 1]), 7),(tensor([7, 1, 8]), 1),(tensor([1, 9, 1]), 10),(tensor([10, 1, 11]), 1),(tensor([ 1, 12, 1]), 13),(tensor([13, 1, 14]), 1),(tensor([ 1, 15, 1]), 16)...] . Then we can batch those easily using the DataLoader class. For now we will split randomly the sequences. . bs = 64 cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(seqs[:cut], seqs[cut:], bs=64, shuffle=False) . We can now create a neural network architecture that takes three words as input, and returns a prediction of the probability of each possible next word in the vocab. We will use three standard linear layers, but with two tweaks. . The first tweak is that the first linear layer will use only the first word&#39;s embedding as activations, the second layer will use the second word&#39;s embedding plus the first layer&#39;s output activations, and the third layer will use the third word&#39;s embedding plus the second layer&#39;s output activations. The key effect of this is that every word is interpreted in the information context of any words preceding it. . The second tweak is that each of these three layers will use the same weight matrix. The way that one word impacts the activations from previous words should not change depending on the position of a word. In other words, activation values will change as data moves through the layers, but the layer weights themselves will not change from layer to layer. So a layer does not learn one sequence position; it must learn to handle all positions. . Since layer weights do not change, you might think of the sequential layers as the &quot;same layer&quot; repeated. In fact PyTorch makes this concrete; we can just create one layer, and use it multiple times. . Our language model in PyTorch . We can now create the language model module that we described earlier: . class LMModel1(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) def forward(self, x): h = F.relu(self.h_h(self.i_h(x[:,0]))) h = h + self.i_h(x[:,1]) h = F.relu(self.h_h(h)) h = h + self.i_h(x[:,2]) h = F.relu(self.h_h(h)) return self.h_o(h) . As you see, we have created three layers: . The embedding layer (i_h for input to hidden) | The linear layer to create the activations for the next word (h_h for hidden to hidden) | A final linear layer to predict the fourth word (h_o for hidden to output) | . This might be easier to represent in pictorial form. Let&#39;s define a simple pictorial representation of basic neural networks. &lt;&gt; shows how we&#39;re going to represent a neural net with one hidden layer.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Pictorial representation of simple neural network . Each shape represents activations: rectangle for input, circle for hidden (inner) layer activations, and triangle for output activations. We will use those shapes (summarized in &lt;&gt;) in all the diagrams of this chapter.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Shapes used in our pictorial representations . An arrow represents the actual layer computation—i.e. the linear layer followed by the activation layers. Using this notation, &lt;&gt; shows what our simple language model looks like.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Representation of our basic language model . To simplify things, we&#39;ve removed the details of the layer computation from each arrow. We&#39;ve also color-coded the arrows, such that all arrows with the same color have the same weight matrix. For instance, all the input layers use the same embedding matrix, so they all have the same color (green). . Let&#39;s try training this model and see how it goes: . learn = Learner(dls, LMModel1(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(4, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.824297 | 1.970941 | 0.467554 | 00:02 | . 1 | 1.386973 | 1.823242 | 0.467554 | 00:02 | . 2 | 1.417556 | 1.654497 | 0.494414 | 00:02 | . 3 | 1.376440 | 1.650849 | 0.494414 | 00:02 | . To see if this is any good, let&#39;s check what would a very simple model give us. In this case we could always predict the most common token, so let&#39;s find out which token is the most often the target in our validation set: . n,counts = 0,torch.zeros(len(vocab)) for x,y in dls.valid: n += y.shape[0] for i in range_of(vocab): counts[i] += (y==i).long().sum() idx = torch.argmax(counts) idx, vocab[idx.item()], counts[idx].item()/n . (tensor(29), &#39;thousand&#39;, 0.15165200855716662) . The most common token has the index 29, which corresponds to the token &#39;thousand&#39;. Always predicting this token would give us an accuracy of roughly 15 %, so we are faring way better! . A: My first guess was that the separator would be the most common token, since there is one for every number. But looking at tokens reminded me that large numbers are written with many words, so on the way to 10,000 you write &quot;thousand&quot; a lot: five thousand, five thousand and one, five thousand and two, etc.. Oops! Looking at your data is great for noticing subtle features and also embarrassingly obvious ones. . This is a nice first baseline. Let&#39;s see how we can refactor this with a loop. . Our first recurrent neural network . Looking at the code for our module, we could simplify it by replacing the duplicated code that calls the layers with a for loop. As well as making our code simpler, this will also have the benefit that we could apply our module equally well to token sequences of different lengths; we would not be restricted to token lists of length three. . class LMModel2(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) def forward(self, x): h = 0 for i in range(3): h = h + self.i_h(x[:,i]) h = F.relu(self.h_h(h)) return self.h_o(h) . Let&#39;s check that we get the same results using this refactoring: . learn = Learner(dls, LMModel2(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(4, 1e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.816274 | 1.964143 | 0.460185 | 00:02 | . 1 | 1.423805 | 1.739964 | 0.473259 | 00:02 | . 2 | 1.430327 | 1.685172 | 0.485382 | 00:02 | . 3 | 1.388390 | 1.657033 | 0.470406 | 00:02 | . We can also refactor our pictorial representation in exactly the same way, see &lt;&gt; (we&#39;re also removing the details of activation sizes here, and using the same arrow colors as in &lt;&gt;).&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Basic recurrent neural network . You will see that there is a set of activations which are being updated each time through the loop, and are stored in the variable h — this is called the hidden state. . Jargon: hidden state: the activations that are updated at each step of a recurrent neural network . A neural network which is defined using a loop like this is called a recurrent neural network, also known as an RNN. It is important to realise that an RNN is not a complicated new architecture, but is simply a refactoring of a multilayer neural network using a for loop. . A: My true opinion: if they were called &quot;looping neural networks&quot;, or LNNs, they would seem 50% less daunting! . Now that we know what an RNN is, let&#39;s try to make it a little bit beter. . Improving the RNN . Looking at the code for our RNN, one thing that seems problematic is that we are initialising our hidden state to zero for every new input sequence. Why is that a problem? We made our sample sequences short so they would fit easily into batches. But if we order those samples correctly, those sample sequences will be read in order by the model, exposing the model to long stretches of the original sequence. . Another thing we can look at is havin more signal: why only predict the fourth word when we could use the intermediate predictions to also predict the second and third words? . We&#39;ll see how we can implement those changes, starting with adding some state. . Maintaining the state of an RNN . Because we initialize the model&#39;s hidden state to zero for each new sample, we are throwing away all the information we have about the sentences we have seen so far, which means that our model doesn&#39;t actually know where we are up to in the overall counting sequence. This is easily fixed; we can simply move the initialisation of the hidden state to __init__. . But this fix will create its own subtle, but important, problem. It effectively makes our neural network as deep as the entire number of tokens in our document. For instance, if there were 10,000 tokens in our dataset, we would be creating a 10,000 layer neural network. . To see this, consider the original pictorial representation of our recurrent neural network in &lt;&gt;, before refactoring it with a for loop. You can see each layer corresponds with one token input. When we talk about the representation of a recurrent neural network before refactoring with the for loop, we call this the unrolled representation. It is often helpful to consider the unrolled representation when trying to understand an RNN.&lt;/p&gt; The problem with a 10,000 layer neural network is that if and when you get to the 10,000th word of the dataset, you will still need to calculate the derivatives all the way back to the first layer. This is going to be very slow indeed, and very memory intensive. It is unlikely that you could store even one mini batch on your GPU. . The solution to this is to tell PyTorch that we do not want to back propagate the derivatives through the entire implicit neural network. Instead, we will just keep the last three layers of gradients. To remove all of the gradient history in PyTorch, we use the detach method. . Here is the new version of our RNN. It is now stateful, because it remembers its activations between different calls to forward, which represent its use for different samples in the batch: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; class LMModel3(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) self.h = 0 def forward(self, x): for i in range(3): self.h = self.h + self.i_h(x[:,i]) self.h = F.relu(self.h_h(self.h)) out = self.h_o(self.h) self.h = self.h.detach() return out def reset(self): self.h = 0 . If you think about it, this model will have the same activations whatever the sequence length we pick, because the hidden state will remember the last activation from the previous batch. The only thing that will be different are the gradients computed at each step: they will only be calculated on sequence length tokens in the past, instead of the whole stream. That is why this sequence length is often called bptt for back-propagation through time. . jargon: Back propagation through time (BPTT): Treating a neural net with effectively one layer per time step (usually refactored using a loop) as one big model, and calculating gradients on it in the usual way. To avoid running out of memory and time, we usually use truncated BPTT, which &quot;detaches&quot; the history of computation steps in the hidden state every few time steps. | . To use LMModel3, we need to make sure the samples are going to be seen in a certain order. As we saw in &lt;&gt;, if the first line of the first batch is our dset[0] then the second batch should have dset[1] as the first line, so that the model sees the text flowing.&lt;/p&gt; LMDataLoader was doing this for us in &lt;&gt;. This time we&#39;re going to do it ourselves.&lt;/p&gt; To do this, we are going to rearrange our dataset. First we divide the samples into m = len(dset) // bs groups (this is the equivalent of splitting the whole concatenated dataset into, for instance, 64 equally sized pieces, since we&#39;re using bs=64 here). m is the length of each of these pieces. For instance, if we&#39;re using our whole dataset (although we&#39;ll actually split it into train vs valid in a moment), that will be: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; m = len(seqs)//bs m,bs,len(seqs) . (328, 64, 21031) . The first batch will be composed of the samples: . (0, m, 2*m, ..., (bs-1)*m) . then the second batch of the samples: . (1, m+1, 2*m+1, ..., (bs-1)*m+1) . and so forth. This way, at each epoch, the model will see a chunk of contiguous text of size 3*m (since each text is of size 3) on each line of the batch. . The following function does that reindexing: . def group_chunks(ds, bs): m = len(ds) // bs new_ds = L() for i in range(m): new_ds += L(ds[i + m*j] for j in range(bs)) return new_ds . Then we just pass drop_last=True when building our DataLoaders to drop the last batch that has not a shape of bs, we also pass shuffle=False to make sure the texts are read in order. . cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets( group_chunks(seqs[:cut], bs), group_chunks(seqs[cut:], bs), bs=bs, drop_last=True, shuffle=False) . The last thing we add is a little tweak of the training loop via a Callback. We will talk more about callbacks in &lt;&gt;; this one will call the reset method of our model at the beginning of each epoch and before each validation phase. Since we implemented that method to zero the hidden state of the model, this will make sure we start we a clean state before reading those continuous chunks of text. We can also start training a bit longer:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn = Learner(dls, LMModel3(len(vocab), 64), loss_func=F.cross_entropy, metrics=accuracy, cbs=ModelReseter) learn.fit_one_cycle(10, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.677074 | 1.827367 | 0.467548 | 00:02 | . 1 | 1.282722 | 1.870913 | 0.388942 | 00:02 | . 2 | 1.090705 | 1.651793 | 0.462500 | 00:02 | . 3 | 1.005092 | 1.613794 | 0.516587 | 00:02 | . 4 | 0.965975 | 1.560775 | 0.551202 | 00:02 | . 5 | 0.916182 | 1.595857 | 0.560577 | 00:02 | . 6 | 0.897657 | 1.539733 | 0.574279 | 00:02 | . 7 | 0.836274 | 1.585141 | 0.583173 | 00:02 | . 8 | 0.805877 | 1.629808 | 0.586779 | 00:02 | . 9 | 0.795096 | 1.651267 | 0.588942 | 00:02 | . This is already better! The next step is to use more targets and compare them to the intermediate predictions. . Creating more signal . Another problem with our current approach is that we only predict one output word for each three input words. That means that the amount of signal that we are feeding back to update weights with is not as large as it could be. It would be better if we predicted the next word after every single word, rather than every three words, as shown in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; RNN predicting after every token . This is easy enough to add. We need to first change our data so that the dependent variable has each of the three next words after each of our three input words. Instead of 3, we use an attribute, sl (for sequence length) and make it a bit bigger: . sl = 16 seqs = L((tensor(nums[i:i+sl]), tensor(nums[i+1:i+sl+1])) for i in range(0,len(nums)-sl-1,sl)) cut = int(len(seqs) * 0.8) dls = DataLoaders.from_dsets(group_chunks(seqs[:cut], bs), group_chunks(seqs[cut:], bs), bs=bs, drop_last=True, shuffle=False) . Looking at the first element of seqs, we can see that it contains two lists of the same size. The second list is the same as the first, but offset by one element: . [L(vocab[o] for o in s) for s in seqs[0]] . [(#16) [&#39;one&#39;,&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;...], (#16) [&#39;.&#39;,&#39;two&#39;,&#39;.&#39;,&#39;three&#39;,&#39;.&#39;,&#39;four&#39;,&#39;.&#39;,&#39;five&#39;,&#39;.&#39;,&#39;six&#39;...]] . Now we need to modify our model so that it outputs a prediction after every word, rather than just at the end of a three word sequence: . class LMModel4(Module): def __init__(self, vocab_sz, n_hidden): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.h_h = nn.Linear(n_hidden, n_hidden) self.h_o = nn.Linear(n_hidden,vocab_sz) self.h = 0 def forward(self, x): outs = [] for i in range(sl): self.h = self.h + self.i_h(x[:,i]) self.h = F.relu(self.h_h(self.h)) outs.append(self.h_o(self.h)) self.h = self.h.detach() return torch.stack(outs, dim=1) def reset(self): self.h = 0 . This model will return outputs of shape bs x sl x vocab_sz (since we stacked on dim=1). Our targets are of shape bs x sl, so we need to flatten those before using them in F.cross_entropy: . def loss_func(inp, targ): return F.cross_entropy(inp.view(-1, len(vocab)), targ.view(-1)) . We can now use this loss function to train the model: . learn = Learner(dls, LMModel4(len(vocab), 64), loss_func=loss_func, metrics=accuracy, cbs=ModelReseter) learn.fit_one_cycle(15, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 3.103298 | 2.874341 | 0.212565 | 00:01 | . 1 | 2.231964 | 1.971280 | 0.462158 | 00:01 | . 2 | 1.711358 | 1.813547 | 0.461182 | 00:01 | . 3 | 1.448516 | 1.828176 | 0.483236 | 00:01 | . 4 | 1.288630 | 1.659564 | 0.520671 | 00:01 | . 5 | 1.161470 | 1.714023 | 0.554932 | 00:01 | . 6 | 1.055568 | 1.660916 | 0.575033 | 00:01 | . 7 | 0.960765 | 1.719624 | 0.591064 | 00:01 | . 8 | 0.870153 | 1.839560 | 0.614665 | 00:01 | . 9 | 0.808545 | 1.770278 | 0.624349 | 00:01 | . 10 | 0.758084 | 1.842931 | 0.610758 | 00:01 | . 11 | 0.719320 | 1.799527 | 0.646566 | 00:01 | . 12 | 0.683439 | 1.917928 | 0.649821 | 00:01 | . 13 | 0.660283 | 1.874712 | 0.628581 | 00:01 | . 14 | 0.646154 | 1.877519 | 0.640055 | 00:01 | . We need to train for longer, since the task has changed a bit and is more complicated now. But we end up with a good result... At least, sometimes. If you run it a few times, you&#39;ll see that you can get quite different results on different runs. That&#39;s because effectively we have a very deep network here, which can result in very large or very small gradients. We&#39;ll see in the next part of to deal with this. . Now, the obvious way to get a better model is to go deeper: we only have one linear layer between the hidden state and the output activations in our basic RNN, so maybe we would get better results with more. . Multilayer RNNs . In a multilayer RNN, we pass the activations from our recurrent neural network into a second recurrent neural network, like in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; 2-layer RNN . …or in an unrolled representation in &lt;&gt; (the same way as in &lt;&gt; last section).&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; 2-layer unrolled RNN . Let&#39;s see how to implement this in practice. . The model . Let&#39;s save some time by using PyTorch&#39;s RNN class, which implements exactly what we have created above, but also gives us the option to stack multiple RNNs, as we have discussed: . class LMModel5(Module): def __init__(self, vocab_sz, n_hidden, n_layers): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = nn.RNN(n_hidden, n_hidden, n_layers, batch_first=True) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h = torch.zeros(n_layers, bs, n_hidden) def forward(self, x): res,h = self.rnn(self.i_h(x), self.h) self.h = h.detach() return self.h_o(res) def reset(self): self.h.zero_() . learn = Learner(dls, LMModel5(len(vocab), 64, 2), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelReseter) learn.fit_one_cycle(15, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 3.055853 | 2.591640 | 0.437907 | 00:01 | . 1 | 2.162359 | 1.787310 | 0.471598 | 00:01 | . 2 | 1.710663 | 1.941807 | 0.321777 | 00:01 | . 3 | 1.520783 | 1.999726 | 0.312012 | 00:01 | . 4 | 1.330846 | 2.012902 | 0.413249 | 00:01 | . 5 | 1.163297 | 1.896192 | 0.450684 | 00:01 | . 6 | 1.033813 | 2.005209 | 0.434814 | 00:01 | . 7 | 0.919090 | 2.047083 | 0.456706 | 00:01 | . 8 | 0.822939 | 2.068031 | 0.468831 | 00:01 | . 9 | 0.750180 | 2.136064 | 0.475098 | 00:01 | . 10 | 0.695120 | 2.139140 | 0.485433 | 00:01 | . 11 | 0.655752 | 2.155081 | 0.493652 | 00:01 | . 12 | 0.629650 | 2.162583 | 0.498535 | 00:01 | . 13 | 0.613583 | 2.171649 | 0.491048 | 00:01 | . 14 | 0.604309 | 2.180355 | 0.487874 | 00:01 | . Now that&#39;s disappointing... we are doing more poorly than the single-layer RNN from the end of last section. The reason is that we have a deeper model, leading to exploding or disappearing activations. . Exploding or disappearing activations . In practice, creating accurate models from this kind of RNN is difficult. We will get better results if we call detach less often, and have more layers — this gives our RNN a longer time horizon to learn from, and richer features to create. But it also means we have a deeper model to train. The key challenge in the development of deep learning has been figuring out how to train these kinds of models. . The reason this is challenging is because of what happens when you multiply by a matrix many times. Think about what happens when you multiply by a number many times. For example, if you multiply by two, starting at one, you get the sequence 1, 2, 4, 8,… after 32 steps you are already at 4,294,967,296. A similar issue happens if we multiply by 0.5: we get 0.5, 0.25, 0.125… and after 32 steps it&#39;s 0.00000000023. As you can see, a number even slightly higher or lower than one results in an explosion or disappearance of our number, after just a few repeated multiplications. . Because matrix multiplication is just multiplying numbers and adding them up, exactly the same thing happens with repeated matrix multiplications. And a deep neural network is just repeated matrix multiplications--each extra layer is another matrix multiplication. This means that it is very easy for a deep neural network to end up with extremely large, or extremely small numbers. . This is a problem, because the way computers store numbers (known as &quot;floating point&quot;) means that they become less and less accurate the further away the numbers get from zero. The diagram in &lt;&gt;, from the excellent article What you never wanted to know about floating point but will be forced to find out, shows how the precision of floating point numbers varies over the number line:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Precision of floating point numbers . This inaccuracy means that often the gradients calculated for updating the weights end up as zero or infinity for deep networks. This is commonly refered to as vanishing gradients or exploding gradients. That means that in SGD, the weights are updated either not at all, or jump to infinity. Either way, they won&#39;t improve with training. . Researchers have developed a number of ways to tackle this problem, which we will be discussing later in the book. One way to tackle the problem is to change the definition of a layer in a way that makes it less likely to have exploding activations. We&#39;ll look at the details of how this is done in &lt;&gt;, when we discuss batch normalization, and &lt;&gt;, when we discuss ResNets, although these details don&#39;t generally matter in practice (unless you are a researcher that is creating new approaches to solving this problem). Another way to deal with this is by being careful about initialization, which is a topic we&#39;ll investigate in &lt;&gt;.&lt;/p&gt; For RNNs, there are two types of layers frequently used to avoid exploding activations, and they are: gated recurrent units (GRU), and Long Short-Term Memory (LSTM). Both of these are available in PyTorch, and are drop-in replacements for the RNN layer. We will only cover LSTMs in this book, there are plenty of good tutorials online explaining GRUs, which are a minor variant on the LSTM design. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; LSTM . LSTM (for long short-term memory) is an architecture that was introduced back in 1997 by Jurgen Schmidhuber and Sepp Hochreiter. In this architecture, there are not one but two hidden states. In our base RNN, the hidden state is the output of the RNN at the previous time step. That hidden state is then responsible for doing two things at a time: . having the right information for the output layer to predict the correct next token | retaining memory of everything that happened in the sentence | . Consider, for example, the sentences &quot;Henry has a dog and he likes his dog very much&quot; and &quot;Sophie has a dog and she likes her dog very much&quot;. It&#39;s very clear that the RNN needs to remember the name at the beginning of the sentence to be able to predict he/she or his/her. . In practice, RNNs are really bad at retaining memory of what happened much earlier in the sentence, which is the motivation to have another hidden state (called cell state) in the LSTM. The cell state will be responsible for keeping long short-term memory, while the hidden state will focus on the next token to predict. Let&#39;s have a closer look and how this is achieved and build one LSTM from scratch. . Building an LSTM from scratch . In order to build an LSTM, we first have to understand its architecture. &lt;&gt; shows us its inner structure.&lt;/p&gt; Architecture of an LSTM . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; In this picture, our input $x_{t}$ enters on the bottom with the previous hidden state ($h_{t-1}$) and cell state ($c_{t-1}$). The four orange boxes represent four layers with the activation being either sigmoid (for $ sigma$) or tanh. tanh is just a sigmoid rescaled to the range -1 to 1. Its mathematical expression can be written like this: . $$ tanh(x) = frac{e^{x} + e^{-x}}{e^{x}-e^{-x}} = 2 sigma(2x) - 1$$where $ sigma$ is the sigmoid function. The green boxes are elementwise operations. What goes out is the new hidden state ($h_{t}$) and new cell state ($c_{t}$) on the left, ready for our next input. The new hidden state is also use as output, which is why the arrow splits to go up. . Let&#39;s go over the four neural nets (called gates) one by one and explain the diagram, but before this, notice how very little the cell state (on the top) is changed. It doesn&#39;t even go directly through a neural net! This is exactly why it will carry on a longer-term state. . First, the arrows for input and old hidden state are joined together. In the RNN we wrote before in this chapter, we were adding them together. In the LSTM, we stack them in one big tensor. This means the dimension of our embeddings (which is the dimension of $x_{t}$) can be different than the dimension of our hidden state. If we call those n_in and n_hid, the arrow at the bottom is of size n_in + n_hid, thus all the neural nets (orange boxes) are linear layers with n_in + n_hid inputs and n_hid outputs. . The first gate (looking from the left to right) is called the forget gate. Since it&#39;s a linear layer followed by a sigmoid, its output will have scalars between 0 and 1. We multiply this result by the cell gate, so for all the values close to 0, we will forget what was inside that cell state (and for the values close to 1 it doesn&#39;t do anything). This gives the ability to the LSTM to forget things about its longterm state. For instance, when crossing a period or an xxbos token, we would expect to it to (have learned to) reset its cell state. . The second gate is called the input gate. It works with the third gate (which doesn&#39;t really have a name but is sometimes called the cell gate) to update the cell state. For instance we may see a new gender pronoun, so we must replace the information about gender that the forget gate removed by the new one. Like the forget gate, the input gate ends up on a product, so it jsut decides which element of the cell state to update (valeus close to 1) or not (values close to 0). The third gate will then fill those values with things between -1 and 1 (thanks to the tanh). The result is then added to the cell state. . The last gate is the output gate. It will decides which information take in the cell state to generate the output. The cell state goes through a tanh before this and the output gate combined with the sigmoid decides which values to take inside it. . In terms of code, we can write the same steps like this: . class LSTMCell(Module): def __init__(self, ni, nh): self.forget_gate = nn.Linear(ni + nh, nh) self.input_gate = nn.Linear(ni + nh, nh) self.cell_gate = nn.Linear(ni + nh, nh) self.output_gate = nn.Linear(ni + nh, nh) def forward(self, input, state): h,c = state h = torch.stack([h, input], dim=1) forget = torch.sigmoid(self.forget_gate(h)) c = c * forget inp = torch.sigmoid(self.input_gate(h)) cell = torch.tanh(self.cell_gate(h)) c = c + inp * cell out = torch.sigmoid(self.output_gate(h)) h = outgate * torch.tanh(c) return h, (h,c) . In practice, we can then refactor the code. Also, in terms of performance, it&#39;s better to do one big matrix multiplication than four smaller ones (that&#39;s because we only launch the special fast kernel on GPU once, and it gives the GPU more work to do in parallel). The stacking takes a bit of time (since we have to move one of the tensors around on the GPU to have it all in a contiguous array), so we use two separate layers for the input and the hidden state. The optimized and refactored code then looks like that: . class LSTMCell(Module): def __init__(self, ni, nh): self.ih = nn.Linear(ni,4*nh) self.hh = nn.Linear(nh,4*nh) def forward(self, input, state): h,c = state #One big multiplication for all the gates is better than 4 smaller ones gates = (self.ih(input) + self.hh(h)).chunk(4, 1) ingate,forgetgate,outgate = map(torch.sigmoid, gates[:3]) cellgate = gates[3].tanh() c = (forgetgate*c) + (ingate*cellgate) h = outgate * c.tanh() return h, (h,c) . Here we use the PyTorch chunk method to split our tensor into 4 pieces, e.g.: . t = torch.arange(0,10); t . tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) . t.chunk(2) . (tensor([0, 1, 2, 3, 4]), tensor([5, 6, 7, 8, 9])) . Let&#39;s now use this architecture to train a language model! . Training a language model using LSTMs . Here is the same network as LMModel5, using a two-layer LSTM. We can train it at a higher learning rate, for a shorter time, and get better accuracy: . class LMModel6(Module): def __init__(self, vocab_sz, n_hidden, n_layers): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h = [torch.zeros(2, bs, n_hidden) for _ in range(n_layers)] def forward(self, x): res,h = self.rnn(self.i_h(x), self.h) self.h = [h_.detach() for h_ in h] return self.h_o(res) def reset(self): for h in self.h: h.zero_() . learn = Learner(dls, LMModel6(len(vocab), 64, 2), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=ModelReseter) learn.fit_one_cycle(15, 1e-2) . epoch train_loss valid_loss accuracy time . 0 | 3.000821 | 2.663942 | 0.438314 | 00:02 | . 1 | 2.139642 | 2.184780 | 0.240479 | 00:02 | . 2 | 1.607275 | 1.812682 | 0.439779 | 00:02 | . 3 | 1.347711 | 1.830982 | 0.497477 | 00:02 | . 4 | 1.123113 | 1.937766 | 0.594401 | 00:02 | . 5 | 0.852042 | 2.012127 | 0.631592 | 00:02 | . 6 | 0.565494 | 1.312742 | 0.725749 | 00:02 | . 7 | 0.347445 | 1.297934 | 0.711263 | 00:02 | . 8 | 0.208191 | 1.441269 | 0.731201 | 00:02 | . 9 | 0.126335 | 1.569952 | 0.737305 | 00:02 | . 10 | 0.079761 | 1.427187 | 0.754150 | 00:02 | . 11 | 0.052990 | 1.494990 | 0.745117 | 00:02 | . 12 | 0.039008 | 1.393731 | 0.757894 | 00:02 | . 13 | 0.031502 | 1.373210 | 0.758464 | 00:02 | . 14 | 0.028068 | 1.368083 | 0.758464 | 00:02 | . Now that&#39;s better than a multilayer RNN! We can still see there is a bit of overfitting, which is a sign that a bit of regularization might help. . Regularizing an LSTM . Recurrent neural networks, in general, are hard to train, because of the problems of vanishing activations and gradients we saw before. Using LSTMs (or GRUs) cell make training easier than vanilla RNNs, but there are still very prone to overfitting. Data augmentation, while it exists for text data, is less often used because in most cases, it requires another model to generate random augmentation (by translating in another language and back to the language used for instance). Overall, data augmentation for text data is currently not a well explored space. . However, there are other regularization techniques we can use instead to reduce overfitting, which were thoroughly studied for use with LSTMs in the paper Regularizing and Optimizing LSTM Language Models. This paper showed how effective use of dropout, activation regularization, and temporal activation regularization could allow an LSTM to beat state of the art results that previously required much more complicated models. They called an LSTM using these techniques an AWD LSTM. We&#39;ll look at each of these techniques in turn. . Dropout . Dropout is a regularization technique that was introduce by Geoffrey Hinton et al. in Improving neural networks by preventing co-adaptation of feature detectors. The basic idea is to randomly change some activations to zero at training time. This makes sure all neurons actively work toward the output as seen in &lt;&gt; which is a screenshot from the original paper.&lt;/p&gt; A screenshot from the dropout paper . Hinton used a nice metaphor when he explained, in an interview, the inspiration for dropout: . : &quot;I went to my bank. The tellers kept changing and I asked one of them why. He said he didn’t know but they got moved around a lot. I figured it must be because it would require cooperation between employees to successfully defraud the bank. This made me realize that randomly removing a different subset of neurons on each example would prevent conspiracies and thus reduce overfitting&quot; In the same interview, he also explained that neuroscience provided additional inspiration: : &quot;We don&#39;t really know why neurons spike. One theory is that they want to be noisy so as to regularize, because we have many more parameters than we have data points. The idea of dropout is that if you have noisy activations, you can afford to use a much bigger model.&quot; . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; TK add takeaway form those citations before moving on. . We can see there that if we just zero those activations without doing anything else, our model will have problems to train: if we go from the sum of 5 activations (that are all positive numbers since we apply a ReLU) to just 2, this won&#39;t have the same scale. Therefore if we dropout with a probability p, we rescale all activation by dividing them by 1-p (on average p will be zeroed, so it leaves 1-p), as shown in &lt;&gt; which is a diagram from the original paper.&lt;/p&gt; . This is a full implementation of the dropout layer in PyTorch (although PyTorch&#39;s native layer is actually written in C, not Python): . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; class Dropout(Module): def __init__(self, p): self.p = p def forward(self, x): if not self.training: return x mask = x.new(*x.shape).bernoulli_(1-p) return x * mask.div_(1-p) . The bernoulli_ method is creating a tensor with random zeros (with probability p) and ones (with probability 1-p), which is then multiplied with our input before dividing by 1-p. Note the use of the training attribute, which is available in any PyTorch nn.Module, and tells us if we are doing training or inference. . Note: In previous chapters of the book we&#8217;d be adding a code example for bernoulli_ here, so you can see exactly how it works. But now that you know enough to do this yourself, we&#8217;re going to be doing fewer and fewer examples for you, and instead expecting you to do your own experiments to see how things work. In this case, you&#8217;ll see in the end-of-chapter questionnaire that we&#8217;re asking you to experiment with bernoulli_--but don&#8217;t wait for us to ask you to experiment to develop your understanding of the code we&#8217;re studying, go ahead and do it anyway! Using dropout before passing the output of our LSTM to the final layer will help reduce overfitting. Dropout is also used in many other models, including the default CNN head used in fastai.vision, and is also available in fastai.tabular by passing the ps parameter (where each &quot;p&quot; is passed to each added Dropout layer), as we&#39;ll see in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Dropout has a different behavior in training and validation mode, which we achieved using the training attribute in Dropout above. Calling the train() method on a Module sets training to True (both for the module you call the method on, and for every module it recursively contains), and eval() sets it to False. This is done automatically when calling the methods of Learner, but if you are not using that class, remember to switch from one to the other as needed. . AR and TAR regularization . AR (for activation regularization) and TAR (for temporal activation regularization) are two regularization methods very similar to weight decay. When applying weight decay, we add a small penalty to the loss that aims at making the weights as small as possible. For the activation regularization, it&#39;s the final activations produced by the LSTM that we will try to make as small as possible, instead of the weights. . To regularize the final activations, we have to store those somewhere, then add the means of the squares of them to the loss (along with a multiplier alpha, which is just like wd for weight decay): . loss += alpha * activations.pow(2).mean() . Temporal activation regularization is linked to the fact we are predicting tokens in a sentence. That means it&#39;s likely that the outputs of our LSTMs should somewhat make sense when we read them in order. TAR is there to encourage that behavior by adding a penalty to the loss to make the difference between two consecutive activations as small as possible: our activations tensor has a shape bs x sl x n_hid, and we read consecutive activation on the sequence length axis (so the dimension in the middle). With this, TAR can be expressed as: . loss += beta * (activations[:,1:] - activations[:,:-1]).pow(2).mean() . alpha and beta are then two hyper-parameters to tune. To make this work, we need our model with dropout to return three things: the proper output, the activations of the LSTM pre-dropout and the activations of the LSTM post-dropout. AR is often applied on the dropped out activations (to not penalize the activations we turned in 0s afterward) while TAR is applied on the non-dropped out activations (because those 0s create big differences between two consecutive timesteps). There is then a callback called RNNRegularizer that will apply this regularization for us. . Training a weight-tied regularized LSTM . We can combine dropout (applied before we go in our output layer) with the AR and TAR regularization to train our previous LSTM. We just need to return three things instead of one: the normal output of our LSTM, the dropped-out activations and the activations from our LSTMs. Those last two will be picked up by the callback RNNRegularization for the contributions it has to make to the loss. . Another useful trick we can add from the AWD LSTM paper is weight tying. In a language model, the input embeddings represent a mapping from English words to activations, and the output hidden layer represents a mapping from activations to English words. We might expect, intuitively, that these mappings could be the same. We can represent this in PyTorch by assigning the same weight matrix to each of these layers: . self.h_o.weight = self.i_h.weight . In LMMModel7, we include these final tweaks: . class LMModel7(Module): def __init__(self, vocab_sz, n_hidden, n_layers, p): self.i_h = nn.Embedding(vocab_sz, n_hidden) self.rnn = nn.LSTM(n_hidden, n_hidden, n_layers, batch_first=True) self.drop = nn.Dropout(p) self.h_o = nn.Linear(n_hidden, vocab_sz) self.h_o.weight = self.i_h.weight self.h = [torch.zeros(2, bs, n_hidden) for _ in range(n_layers)] def forward(self, x): raw,h = self.rnn(self.i_h(x), self.h) out = self.drop(raw) self.h = [h_.detach() for h_ in h] return self.h_o(out),raw,out def reset(self): for h in self.h: h.zero_() . We can create a regularized Learner using the RNNRegularizer callback: . learn = Learner(dls, LMModel7(len(vocab), 64, 2, 0.5), loss_func=CrossEntropyLossFlat(), metrics=accuracy, cbs=[ModelReseter, RNNRegularizer(alpha=2, beta=1)]) . A TextLearner automatically adds those two callbacks for us (with default for alpha and beta as above) so we can simplify the line above to: . learn = TextLearner(dls, LMModel7(len(vocab), 64, 2, 0.4), loss_func=CrossEntropyLossFlat(), metrics=accuracy) . We can the train the model, and add additional regularization by increasing the weight decay to 0.1: . learn.fit_one_cycle(15, 1e-2, wd=0.1) . epoch train_loss valid_loss accuracy time . 0 | 2.693885 | 2.013484 | 0.466634 | 00:02 | . 1 | 1.685549 | 1.187310 | 0.629313 | 00:02 | . 2 | 0.973307 | 0.791398 | 0.745605 | 00:02 | . 3 | 0.555823 | 0.640412 | 0.794108 | 00:02 | . 4 | 0.351802 | 0.557247 | 0.836100 | 00:02 | . 5 | 0.244986 | 0.594977 | 0.807292 | 00:02 | . 6 | 0.192231 | 0.511690 | 0.846761 | 00:02 | . 7 | 0.162456 | 0.520370 | 0.858073 | 00:02 | . 8 | 0.142664 | 0.525918 | 0.842285 | 00:02 | . 9 | 0.128493 | 0.495029 | 0.858073 | 00:02 | . 10 | 0.117589 | 0.464236 | 0.867188 | 00:02 | . 11 | 0.109808 | 0.466550 | 0.869303 | 00:02 | . 12 | 0.104216 | 0.455151 | 0.871826 | 00:02 | . 13 | 0.100271 | 0.452659 | 0.873617 | 00:02 | . 14 | 0.098121 | 0.458372 | 0.869385 | 00:02 | . Now this is far better than our previous model! . Conclusion . You have now seen everything that is inside the AWD-LSTM architecture we used in text classification in &lt;&gt;. It uses dropouts in a lot more places:&lt;/p&gt; embedding dropout (just after the embedding layer) | input dropout (after the embedding layer) | weight dropout (applied to the weights of the LSTM at each training step) | hidden dropout (applied to the hidden state between two layers) | . which makes it even more regularized. Since fine-tuning those five dropout values (adding the dropout before the output layer) is complicated, so we have determined good defaults, and allow the magnitude of dropout to be tuned overall with the drop_mult parameter you saw (which is multiplied by each dropout). . Another architecture that is very powerful, especially in &quot;sequence to sequence&quot; problems (that is, problems where the dependent variable is itself a variable length sequence, such as language translation), is the Transformers architecture. You can find it in an online bonus chapter on the book website. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Questionnaire . If the dataset for your project is so big and complicated that working with it takes a significant amount of time, what should you do? | Why do we concatenating the documents in our dataset before creating a language model? | To use a standard fully connected network to predict the fourth word given the previous three words, what two tweaks do we need to make? | How can we share a weight matrix across multiple layers in PyTorch? | Write a module which predicts the third word given the previous two words of a sentence, without peeking. | What is a recurrent neural network? | What is hidden state? | What is the equivalent of hidden state in LMModel1? | To maintain the state in an RNN why is it important to pass the text to the model in order? | What is an unrolled representation of an RNN? | Why can maintaining the hidden state in an RNN lead to memory and performance problems? How do we fix this problem? | What is BPTT? | Write code to print out the first few batches of the validation set, including converting the token IDs back into English strings, as we showed for batches of IMDb data in &lt;&gt;.&lt;/li&gt; What does the ModelReseter callback do? Why do we need it? | What are the downsides of predicting just one output word for each three input words? | Why do we need a custom loss function for LMModel4? | Why is the training of LMModel4 unstable? | In the unrolled representation, we can see that a recurrent neural network actually has many layers. So why do we need to stack RNNs to get better results? | Draw a representation of a stacked (multilayer) RNN. | Why should we get better results in an RNN if we call detach less often? Why might this not happen in practice with a simple RNN? | Why can a deep network result in very large or very small activations? Why does this matter? | In a computer&#39;s floating point representation of numbers, which numbers are the most precise? | Why do vanishing gradients prevent training? | Why does it help to have two hidden states in the LSTM architecture? What is the purpose of each one? | What are these two states called in an LSTM? | What is tanh, and how is it related to sigmoid? | What is the purpose of this code in LSTMCell?: h = torch.stack([h, input], dim=1) | What does chunk to in PyTorch? | Study the refactored version of LSTMCell carefully to ensure you understand how and why it does the same thing as the non-refactored version. | Why can we use a higher learning rate for LMModel6? | What are the three regularisation techniques used in an AWD-LSTM model? | What is dropout? | Why do we scale the weights with dropout? Is this applied during training, inference, or both? | What is the purpose of this line from Dropout?: if not self.training: return x | Experiment with bernoulli_ to understand how it works. | How do you set your model in training mode in PyTorch? In evaluation mode? | Write the equation for activation regularization (in maths or code, as you prefer). How is it different to weight decay? | Write the equation for temporal activation regularization (in maths or code, as you prefer). Why wouldn&#39;t we use this for computer vision problems? | What is &quot;weight tying&quot; in a language model? | &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Further research . In LMModel2 why can forward start with h=0? Why don&#39;t we need to say h=torch.zeros(…)? | Write the code for an LSTM from scratch (but you may refer to &lt;&gt;).&lt;/li&gt; Search on the Internet for the GRU architecture and implement it from scratch, and try training a model. See if you can get the similar results as we saw in this chapter. Compare it to the results of PyTorch&#39;s built in GRU module. | Have a look at the source code for AWD-LSTM in fastai, and try to map each of the lines of code to the concepts shown in this chapter. | &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; | | . . . .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_nlp_dive.html",
            "relUrl": "/2020/03/19/_nlp_dive.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Chapter 10",
            "content": "[[chapter_nlp]] NLP deep dive: RNNs . In &lt;&gt; we saw that deep learning can be used to get great results with natural language datasets. Our example relied on using a pretrained language model and fine-tuning it to classify those reviews. One thing is a bit different from the transfer learning we have in computer vision: the pretrained model was not trained on the same task as the model we used to classify reviews.&lt;/p&gt; What we call a language model is a model that has been trained to guess what the next word in a text is (having read the ones before). This kind of task is called self-supervised learning: we do not need to give labels to our model, just feed it lots and lots of texts. It has a process to automatically get labels from the data, and this task isn&#39;t trivial: to properly guess the next word in a sentence, the model will have to get an understanding of the English-- or other--language. Self-supervised learning can also be used in other domains; for instance, see Self-supervised learning and computer vision for an introduction to vision applications. Self-supervised learning is not usually used for the model that is trained directly, but instead is used for pre-training a model used for transfer learning. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; jargon: Self-supervised learning: Training a model using labels that are embedded in the independent variable, rather than requiring external labels. For instance, training a model to predict the next word in a text. . The language model we used in &lt;&gt; to classify IMDb reviews was pretrained on Wikipedia. We got great results by directly fine-tuning this language model to a movie review classifier, but with one extra step, we can do even better: the Wikipedia English is slightly different from the IMDb English. So instead of jumping directly to the classifier, we could finetune our pretrained language model to the IMDb corpus and then use that as the base for our classifier.&lt;/p&gt; Even if our language model knows the basics of the language we are using in the task (e.g., our pretrained model is in English), it helps to get used to the style of the corpus we are targetting. It may be more informal language, or more technical, with new words to learn or different ways of composing sentences. In the case of IMDb, there will be lots of names of movie directors and actors, and often a less formal style of language that seen in Wikipedia. . We saw that with fastai, we can download a pre-trained language model for English, and use it to get state-of-the-art results for NLP classification. (We expect pre-trained models in many more languages to be available soon — they might well be available by the time you are reading this book, in fact.) So, why are we learning how to train a language model in detail? . One reason, of course, is that it is helpful to understand the foundations of the models that you are using. But there is another very practical reason, which is that you get even better results if you fine tune the (sequence-based) language model prior to fine tuning the classification model. For instance, for the IMDb sentiment analysis task, the dataset includes 50,000 additional movie reviews that do not have any positive or negative labels attached. So that is 100,000 movie reviews altogether (since there are also 25,000 labelled reviews in the training set, and 25,000 in the validation set). We can use all 100,000 of these reviews to fine tune the pretrained language model — this will result in a language model that is particularly good at predicting the next word of a movie review. In contrast, the pretrained model was trained only on Wikipedia articles. . The ULMFiT paper showed that this extra stage of language model fine tuning, prior to transfer learning to a classification task, resulted in significantly better predictions. Using this approach, we have three stages for transfer learning in NLP, as summarised in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The ULMFiT process . Now have a think about how you would turn this language modelling problem into a neural network, given what you have learned so far. We&#39;ll be able to use concepts that we&#39;ve seen in the last two chapters. . Text preprocessing . It&#39;s not at all obvious how we&#39;re going to use what we&#39;ve learned so far to build a language model. Sentences can be different lengths, and documents can be very long. So, how can we predict the next word of a sentence using a neural network? Let&#39;s find out! . We&#39;ve already seen how categorical variables can be used as independent variables for a neural network. The approach we took for a single categorical variable was to: . Make a list of all possible levels of that categorical variable (let us call this list the vocab) | Replace each level with its index in the vocab | Create an embedding matrix for this containing a row for each level (i..e, for each item of the vocab) | Use this embedding matrix as the first layer of a neural network. (A dedicated embedding matrix can take as inputs the raw vocab indexes created in step two; this is equivalent to, but faster and more efficient, than a matrix which takes as input one-hot encoded vectors representing the indexes) | We can do nearly the same thing with text! What is new is the idea of a sequence. First we concatenate all of the documents in our dataset into one big long string and split it into words, giving us a very long list of words. Our independent variable will be the sequence of words starting with the first word in our very long list and ending with the second last, and our dependent variable would be the sequence of words starting with the second word and ending with the last word. . When creating our vocab, we will have very common words that will probably be in the vocabulary of our pretrained model, but we will also have new words specific to our corpus (cinematographic terms, or actor names for instance). Our embedding matrix will be built accordingly: for words that are in the vocabulary of our pretrained model, we will take the corresponding row in the embedding matrix of this pretrained model; but for new words, we won&#39;t have anything, so we will just initialize the corresponding row with a random vector. . Each of the steps necessary to create a language model has jargon associated with it from the world of natural language processing, and fastai and PyTorch classes available to help. The steps are: . Tokenization:: convert the text into a list of words (or characters, or substrings, depending on the granularity of your model) | Numericalization:: make a list of all of the unique words which appear (the vocab), and convert each word into a number, by looking up its index in the vocab | Language model data loader creation:: fastai provides an LMDataLoader class which automatically handles creating a dependent variable which is offset from the independent variable buy one token. It also handles some important details, such as how to shuffle the training data in such a way that the dependent and independent variables maintain their structure as required | Language model creation:: we need a special kind of model which does something we haven&#39;t seen before: handles input lists which could be arbitrarily big or small. There are a number of ways to do this; in this chapter we will be using a recurrent neural network. We will get to the details of this in the &lt;&gt;, but for now, you can think of it as just another deep neural network.&lt;/li&gt; &lt;/ul&gt; Let&#39;s take a look at how each step works in detail. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Tokenization . When we said, convert the text into a list of words, we left out a lot of details. For instance, what do we do with punctuation? How do we deal with a word like &quot;don&#39;t&quot;? Is it one word, or two? What about long medical or chemical words? Should they be split into their separate pieces of meaning? How about hyphenated words? What about languages like German and Poland where we can create really long words from many, many pieces? What about languages like Japanese and Chinese which don&#39;t use bases at all, and don&#39;t really have a well-defined idea of word? . Because there is no one correct answer to these questions, there is no one approach to tokenization. Each element of the list created by the tokenisation process is called a token. There are three main approaches: . Word-based:: split a sentence on spaces, as well as applying language specific rules to try to separate parts of meaning, even when there are no spaces, such as turning &quot;don&#39;t&quot; into &quot;do n&#39;t&quot;. Generally, punctuation marks are also split into separate tokens | Subword based:: split words into smaller parts, based on the most commonly occurring substrings. For instance, &quot;occasion&quot; might be tokeniser as &quot;o c ca sion&quot; | Character-based:: split a sentence into its individual characters. | . We&#39;ll be looking at word and subword tokenization here, and we&#39;ll leave character-based tokenization for you to implement in the questionnaire at the end of this chapter. . jargon: token: one element of a list created by the tokenisation process. It could be a word, part of a word (a subword), or a single character. . Word tokenization with fastai . Rather than providing its own tokenizers, fastai instead provides a consistent interface to a range of tokenisers in external libraries. Tokenization is an active field of research, and new and improved tokenizers are coming out all the time, so the defaults that fastai uses change too. However, the API and options shouldn&#39;t change too much, since fastai tries to maintain a consistent API even as the underlying technology changes. . Let&#39;s try it out with the IMDb dataset that we used in &lt;&gt;:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from fastai2.text.all import * path = untar_data(URLs.IMDB) . We&#39;ll need to grab the text files in order to try out a tokenizer. Just like get_image_files, which we&#39;ve used many times already, gets all the image files in a path, get_text_files gets all the text files in a path. We can also optionally pass folders to restrict the search to a particular list of subfolders: . files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) . Here&#39;s a review that we&#39;ll tokenize (we&#39;ll just print the start of it here to save space): . txt = files[0].open().read(); txt[:75] . &#39;This movie, which I just discovered at the video store, has apparently sit &#39; . As we write this book, the default English word tokenizer for fastai uses a library called spaCy. This uses a sophisticated rules engine that has special rules for URLs, individual special English words, and much more. Rather than directly using SpacyTokenizer, however, we&#39;ll use WordTokenizer, since that will always point to fastai&#39;s current default word tokenizer (which may not always be Spacy, depending when you&#39;re reading this). . Let&#39;s try it out. We&#39;ll use fastai&#39;s coll_repr(collection,n) function to display the results; this displays the first n items of collection, along with the full size--it&#39;s what L uses by default. Not that fastai&#39;s tokenizers take a collection of documents to tokenize, so we have to wrap txt in a list: . spacy = WordTokenizer() toks = first(spacy([txt])) print(coll_repr(toks, 30)) . (#201) [&#39;This&#39;,&#39;movie&#39;,&#39;,&#39;,&#39;which&#39;,&#39;I&#39;,&#39;just&#39;,&#39;discovered&#39;,&#39;at&#39;,&#39;the&#39;,&#39;video&#39;,&#39;store&#39;,&#39;,&#39;,&#39;has&#39;,&#39;apparently&#39;,&#39;sit&#39;,&#39;around&#39;,&#39;for&#39;,&#39;a&#39;,&#39;couple&#39;,&#39;of&#39;,&#39;years&#39;,&#39;without&#39;,&#39;a&#39;,&#39;distributor&#39;,&#39;.&#39;,&#39;It&#39;,&#34;&#39;s&#34;,&#39;easy&#39;,&#39;to&#39;,&#39;see&#39;...] . As you see, spaCy has mainly just separated out the words and punctuation. But it does something else here too: it has split &quot;it&#39;s&quot; into &quot;it&quot; and &quot;&#39;s&quot;. That makes intuitive sense--these are separate words, really. Tokenization is a surprisingly subtle task, when you think about all the little details that have to be handled. spaCy handles these for us, for instance, here we see that &quot;.&quot; is separated when it terminates a sentence, but not in an acronym or number: . first(spacy([&#39;The U.S. dollar $1 is $1.00.&#39;])) . (#9) [&#39;The&#39;,&#39;U.S.&#39;,&#39;dollar&#39;,&#39;$&#39;,&#39;1&#39;,&#39;is&#39;,&#39;$&#39;,&#39;1.00&#39;,&#39;.&#39;] . fastai then adds some additional functionality to the tokenization process with the Tokenizer class: . tkn = Tokenizer(spacy) print(coll_repr(tkn(txt), 31)) . (#228) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;this&#39;,&#39;movie&#39;,&#39;,&#39;,&#39;which&#39;,&#39;i&#39;,&#39;just&#39;,&#39;discovered&#39;,&#39;at&#39;,&#39;the&#39;,&#39;video&#39;,&#39;store&#39;,&#39;,&#39;,&#39;has&#39;,&#39;apparently&#39;,&#39;sit&#39;,&#39;around&#39;,&#39;for&#39;,&#39;a&#39;,&#39;couple&#39;,&#39;of&#39;,&#39;years&#39;,&#39;without&#39;,&#39;a&#39;,&#39;distributor&#39;,&#39;.&#39;,&#39;xxmaj&#39;,&#39;it&#39;,&#34;&#39;s&#34;,&#39;easy&#39;...] . There are now some tokens added that start with the characters &quot;xx&quot;, which is not a common word prefix in English. These are special tokens. . For example, the first item in the list, &quot;xxbos&quot;, is a special token that indicates the start of a new text (&quot;BOS&quot; is a standard NLP acronym which means &quot;beginning of stream&quot;). By recognizing this start token, the model will be able to learn it needs to &quot;forget&quot; what was said previously and focus on upcoming words. . These special tokens don&#39;t come from spaCy directly. They are there because fastai adds them by default, by applying a number of rules when processing text. These rules are designed to make it easier for a model to recognise the important parts of a sentence. In a sense, we are translating the original English language sequence into a simplified tokenised language, a language which is designed to be easy for a model to learn. . For instance, the rules will replace a sequence of four exclamation points with a single exclamation point, followed by a special repeated character token, and then the number four. In this way, the model&#39;s embedding matrix can encode information about general concepts such as repeated punctuation rather than requiring a separate token for every number of repetitions of every punctuation mark. Similarly, a capitalised word will be replaced with a special capitalisation token, followed by the lower case version of the word. This way, the embedding matrix only needs the lower case version of the words, saving compute and memory, but can still learn the concept of capitalisation. . Here are some of the main special tokens you&#39;ll see: . xxbos:: indicates the beginning of a text (here a review) | xxmaj:: indicates the next word begins with a capital (since we lower-cased everything) | xxunk:: indicates the next word is unknown | . To see the rules that were used, you can check the default rules: . defaults.text_proc_rules . [&lt;function fastai2.text.core.fix_html(x)&gt;, &lt;function fastai2.text.core.replace_rep(t)&gt;, &lt;function fastai2.text.core.replace_wrep(t)&gt;, &lt;function fastai2.text.core.spec_add_spaces(t)&gt;, &lt;function fastai2.text.core.rm_useless_spaces(t)&gt;, &lt;function fastai2.text.core.replace_all_caps(t)&gt;, &lt;function fastai2.text.core.replace_maj(t)&gt;, &lt;function fastai2.text.core.lowercase(t, add_bos=True, add_eos=False)&gt;] . As always, you can look at the source code of each of them in a notebook by typing . ??replace_rep . Here is a brief summary of what each does: . fix_html:: replace special HTML characters by a readable version (IMDb reviwes have quite a few of them for instance) ; | replace_rep:: replace any character repeated three times or more by a special token for repetition (xxrep), the number of times it&#39;s repeated, then the character ; | replace_wrep:: replace any word repeated three times or more by a special token for word repetition (xxwrep), the number of times it&#39;s repeated, then the word ; | spec_add_spaces:: add spaces around / and # ; | rm_useless_spaces:: remove all repetitions of the space character ; | replace_all_caps:: lowercase a word written in all caps and adds a special token for all caps (xxcap) in front of it ; | replace_maj:: lowercase a capitalized word and adds a special token for capitalized (xxmaj) in front of it ; | lowercase:: lowercase all text and adds a special token at the beginning (xxbos) and/or the end (xxeos). | . Let&#39;s take a look at a few of them in action: . coll_repr(tkn(&#39;&amp;copy; Fast.ai www.fast.ai/INDEX&#39;), 31) . &#34;(#11) [&#39;xxbos&#39;,&#39;©&#39;,&#39;xxmaj&#39;,&#39;fast.ai&#39;,&#39;xxrep&#39;,&#39;3&#39;,&#39;w&#39;,&#39;.fast.ai&#39;,&#39;/&#39;,&#39;xxup&#39;,&#39;index&#39;...]&#34; . Now let&#39;s have a look at how subword tokenization would work. . Subword tokenization . In addition to the word tokenization approach seen in the last section, another popular tokenization method is subword tokenization. Word tokenization relies on an assumption that spaces provide a useful separation of components of meaning in a sentence. However, this assumption is not always appropriate. For instance, consider this sentence: 我的名字是郝杰瑞 (which means &quot;My name is Jeremy Howard&quot; in Chinese). That&#39;s not going to work very well with a word tokenizer, because there are no spaces in it! Languages like Chinese and Japanese don&#39;t use spaces, and in fact they don&#39;t even have a well-defined concept of a &quot;word&quot;. There are also languages, like Turkish and Hungarian, which can add many bits together without spaces, to create very long words which include a lot of separate pieces of information. . To handle these cases, it&#39;s generally best to use subword tokenization. This proceeds in two steps: . Analyze a corpus of documents to find the most commonly occurring groups of letters. These become the vocab. | Tokenize the corpus using this vocab of subword units. | Let&#39;s look at an example. For our corpus, we&#39;ll use the first 2000 movie reviews: . txts = L(o.open().read() for o in files[:2000]) . We instantiate our tokenizer, passing in the size of the vocab we want to create, and then we need to &quot;train&quot; it. That is, we need to have it read our documents, and find the common sequences of characters, to create the vocab. This is done with setup. As we&#39;ll see shortly, setup is a special fastai method that is called automatically in our usual data processing pipelines. Since we&#39;re doing everything manually at the moment, however, we have to call it ourselves. Here&#39;s a function that does these steps for a given vocab size, and shows an example output: . def subword(sz): sp = SubwordTokenizer(vocab_sz=sz) sp.setup(txts) return &#39; &#39;.join(first(sp([txt]))[:40]) . Let&#39;s try it out: . subword(1000) . &#39;▁This ▁movie , ▁which ▁I ▁just ▁dis c over ed ▁at ▁the ▁video ▁st or e , ▁has ▁a p par ent ly ▁s it ▁around ▁for ▁a ▁couple ▁of ▁years ▁without ▁a ▁dis t ri but or . ▁It&#39; . When using fastai&#39;s subword tokenizer, the special character ▁ represents a space character in the original text. . If we use a smaller vocab, then each token will represent fewer characters, and it will take more tokens to represent a sentence: . subword(200) . &#39;▁ T h i s ▁movie , ▁w h i ch ▁I ▁ j us t ▁ d i s c o ver ed ▁a t ▁the ▁ v id e o ▁ st or e , ▁h a s&#39; . On the other hand, if we use a larger vocab, then most common English words will end up in the vocab themselves, and we will not need as many to represent a sentence: . subword(10000) . &#34;▁This ▁movie , ▁which ▁I ▁just ▁discover ed ▁at ▁the ▁video ▁store , ▁has ▁apparently ▁sit ▁around ▁for ▁a ▁couple ▁of ▁years ▁without ▁a ▁distributor . ▁It &#39; s ▁easy ▁to ▁see ▁why . ▁The ▁story ▁of ▁two ▁friends ▁living&#34; . Picking a subword vocab size represents a compromise: a larger vocab means more fewer tokens per sentence, which means faster training, less memory, and less state for the model to remember; but on the downside, it means larger embedding matrices, which require more data to learn. . Overall, subword tokenization provides a way to easily scale between character tokenization (i.e. use a small subword vocab) and word tokenization (i.e. use a large subword vocab), and handles every human language without needing language-specific algorithms to be developed. It can even handle other &quot;languages&quot; such as genomic sequences or MIDI music notation! For this reason, in the last year its popularity has soared, and it seems likely to become the most common tokenization approach (it may well already be, by the time you read this!) . Once our texts have been split into tokens, we need to convert them to numbers. . Numericalization with fastai . Numericalization is the process of mapping tokens to integers. It&#39;s basically identical to the steps necessary to create a Category variable, such as the dependent variable of digits in MNIST: . Make a list of all possible levels of that categorical variable (the vocab) | Replace each level with its index in the vocab | We&#39;ll take a look at this in action on the word-tokenized text we saw earlier: . toks = tkn(txt) print(coll_repr(tkn(txt), 31)) . (#228) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;this&#39;,&#39;movie&#39;,&#39;,&#39;,&#39;which&#39;,&#39;i&#39;,&#39;just&#39;,&#39;discovered&#39;,&#39;at&#39;,&#39;the&#39;,&#39;video&#39;,&#39;store&#39;,&#39;,&#39;,&#39;has&#39;,&#39;apparently&#39;,&#39;sit&#39;,&#39;around&#39;,&#39;for&#39;,&#39;a&#39;,&#39;couple&#39;,&#39;of&#39;,&#39;years&#39;,&#39;without&#39;,&#39;a&#39;,&#39;distributor&#39;,&#39;.&#39;,&#39;xxmaj&#39;,&#39;it&#39;,&#34;&#39;s&#34;,&#39;easy&#39;...] . Just like SubwordTokenizer, we need to call setup on Numericalize; this is how we create the vocab. That means we&#39;ll need our tokenized corpus first. Since tokenization takes a while, it&#39;s done in parallel by fastai; but for this manual walk-thru, we&#39;ll use a small subset: . toks200 = txts[:200].map(tkn) toks200[0] . (#228) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;this&#39;,&#39;movie&#39;,&#39;,&#39;,&#39;which&#39;,&#39;i&#39;,&#39;just&#39;,&#39;discovered&#39;,&#39;at&#39;...] . We can pass this to setup to create our vocab: . num = Numericalize() num.setup(toks200) coll_repr(num.vocab,20) . &#34;(#2000) [&#39;xxunk&#39;,&#39;xxpad&#39;,&#39;xxbos&#39;,&#39;xxeos&#39;,&#39;xxfld&#39;,&#39;xxrep&#39;,&#39;xxwrep&#39;,&#39;xxup&#39;,&#39;xxmaj&#39;,&#39;the&#39;,&#39;.&#39;,&#39;,&#39;,&#39;a&#39;,&#39;and&#39;,&#39;of&#39;,&#39;to&#39;,&#39;is&#39;,&#39;in&#39;,&#39;i&#39;,&#39;it&#39;...]&#34; . Our special rules tokens appear first, and then every word appears once, in frequency order. The defaults to Numericalize are min_freq=3,max_vocab=60000. max_vocab=60000 results in fastai replacing all words other than the most common 60000 with a special unknown word token xxunk. This is useful to avoid having an overly large embedding matrix, since that can slow down training, use up too much memory, and can also mean that there isn&#39;t enough data to train useful representations for rare words. However, this last issue is better handled by setting min_freq; the default min_freq=3 means that any word appearing less than three times is replaced with xxunk. . Fastai can also numericalize your dataset using a vocab that you provide, by passing a list of words as the vocab parameter. . Once we&#39;ve created our Numericalize object, we can use it as if it&#39;s a function: . nums = num(toks)[:20]; nums . tensor([ 2, 8, 21, 28, 11, 90, 18, 59, 0, 45, 9, 351, 499, 11, 72, 533, 584, 146, 29, 12]) . This time, our tokens have been converted to a tensor of integers that our model can receive. We can check that they map back to the original text: . &#39; &#39;.join(num.vocab[o] for o in nums) . &#39;xxbos xxmaj this movie , which i just xxunk at the video store , has apparently sit around for a&#39; . Now that we have numbers, we need to put them in batches for our model. . Putting our texts into batches for a language model . When dealing with images, we needed to resize them all to the same height and width before grouping them together in a mini-batch so they could stack together efficiently in a single tensor. Here it&#39;s going to be a little different, because one cannot simply resize text to a desired length. Also, we want our language model to read text in order, so that it can efficiently predict what the next word is. All the difficulty of a language model loader is that each new batch should begin precisely where the previous left off. . Let&#39;s start with an example and imagine our text is the following: . : In this chapter, we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface. First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we&#39;ll have another example of the PreProcessor used in the data block API. nThen we will study how we build a language model and train it for a while. The tokenization process will add special tokens and deal with punctuation to return this text: : xxbos xxmaj in this chapter , we will go back over the example of classifying movie reviews we studied in chapter 1 and dig deeper under the surface . xxmaj first we will look at the processing steps necessary to convert text into numbers and how to customize it . xxmaj by doing this , we &#39;ll have another example of the preprocessor used in the data block xxup api . n xxmaj then we will study how we build a language model and train it for a while . We have separated the 90 tokens by spaces. Let&#39;s say we want a batch size of 6, then we need to break this text in 6 contiguous parts of length 15: . TK: add title . In a perfect world, we could then give this one batch to our model. But that doesn&#39;t work, because this would very likely not fit in our GPU memory (here we have 90 tokens, but all the IMDb reviews together give several millions of tokens). . So in fact we will need to divide this array more finely into subarrays of a fixed sequence length. It is important to maintain order within and across these subarrays, because we will use a model that maintains state in order so that it remembers what it read previously when predicting what comes next. . Going back to our previous example with 6 batches of length 15, if we chose sequence length of 5, that would mean we first feed the following array: . xxbos | xxmaj | in | this | chapter | . movie | reviews | we | studied | in | . first | we | will | look | at | . how | to | customize | it | . | . of | the | preprocessor | used | in | . will | study | how | we | build | . Then . , | we | will | go | back | . chapter | 1 | and | dig | deeper | . the | processing | steps | necessary | to | . xxmaj | by | doing | this | , | . the | data | block | xxup | api | . a | language | model | and | train | . And finally . over | the | example | of | classifying | . under | the | surface | . | xxmaj | . convert | text | into | numbers | and | . we | &#39;ll | have | another | example | . . | n | xxmaj | then | we | . it | for | a | while | . | . Going back to our dataset, the first step is to transform the individual texts into a stream by concatenating them together. As with images, it&#39;s best to randomize the order in which the inputs come, so at the beginning of each epoch we will shuffle the entries to make a new stream (we shuffle the order of the documents, not the order of the words inside, otherwise the text would not make sense anymore). . We will then cut this stream into a certain number of batches (which is our batch size). For instance, if the stream has 50,000 tokens and we set a batch size of 10, this will give us 10 mini-streams of 5,000 tokens. What is important is that we preserve the order of the tokens (so from 1 to 5,000 for the first mini-stream, then from 5,001 to 10,000...) because we want the model to read continuous rows of text (as in our example above). This is why each text has been added a xxbos token during preprocessing, so that the model knows when it reads the stream we are beginning a new entry. . So to recap, at every epoch we shuffle our collection of documents to pick one document, and then we transform that one into a stream of tokens. We then cut that stream into a batch of fixed-size consecutive mini-streams. Our model will then read the mini-streams in order, and thanks to an inner state, it will produce the same activation whatever sequence length you picked. . This is all done behind the scenes by the fastai library when we create a LMDataLoader. We can create one by first applying our Numericalize object to the tokenized texts: . nums200 = toks200.map(num) . ...and then passing that to LMDataLoader: . dl = LMDataLoader(nums200) . Let&#39;s confirm that this gives the expected results, by grabbing the first batch: . x,y = first(dl) x.shape,y.shape . (torch.Size([64, 72]), torch.Size([64, 72])) . ...and then looking at the first row of the independent variable, which should be the start of the first text: . &#39; &#39;.join(num.vocab[o] for o in x[0][:20]) . &#39;xxbos xxmaj this movie , which i just xxunk at the video store , has apparently sit around for a&#39; . ...and the first row of the dependent variable, which is the same thing offset by one token: . &#39; &#39;.join(num.vocab[o] for o in y[0][:20]) . &#39;xxmaj this movie , which i just xxunk at the video store , has apparently sit around for a couple&#39; . This concludes all the preprocessing steps we need to apply to our data. We are now ready to train out text classifier. . Training a text classifier . As we have seen at the beginning of this chapter to train a state-of-the-art text classifier using transfer learning will take two steps: first we need to fine-tune our langauge model pretrained on Wikipedia to the corpus of IMDb reviews, then we can use that model to train a classifier. . As usual, let&#39;s start with assemblng our data. . Language model using DataBlock . fastai handles tokenization and numericalization automatically when TextBlock is passed to DataBlock. All of the arguments that can be passed to Tokenize and Numericalize can also be passed to TextBlock. In the next chapter we&#39;ll discuss the easiest ways to run each of these steps separately, to ease debugging--but you can always just debug by running them manually on a subset of your data as shown in the previous sections. And don&#39;t forget about DataBlock&#39;s handy summary method, which is very useful for debugging data issues. . Here&#39;s how we use TextBlock to create a language model, using fastai&#39;s defaults: . get_imdb = partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;, &#39;unsup&#39;]) dls_lm = DataBlock( blocks=TextBlock.from_folder(path, is_lm=True), get_items=get_imdb, splitter=RandomSplitter(0.1) ).dataloaders(path, path=path, bs=128, seq_len=80) . One thing that&#39;s different to previous types used in DataBlock is that we&#39;re not just using the class directly (i.e. TextBlock(...), but instead are calling a class method. A class method is a Python method which, as the name suggests, belongs to a class rather than an object. (Be sure to search online for more information about class methods if you&#39;re not familiar with them, since they&#39;re commonly used in many Python libraries and applications; we&#39;ve used them a few times previously in the book, but haven&#39;t called attention to them.) The reason that TextBlock is special is that setting up the numericalizer&#39;s vocab can take a long time (we have to read every document and tokenize it to get the vocab); to be as efficient as possible fastai does things such as: . Save the tokenized documents in a temporary folder, so fastai doesn&#39;t have to tokenize more than once | Runs multiple tokenization processes in parallel, to take advantage of your computer&#39;s CPUs. | . Therefore we need to tell TextBlock how to access the texts, so that it can do this initial preprocessing--that&#39;s what from_folder does. . show_batch then works in the usual way: . dls_lm.show_batch(max_n=2) . text text_ . 0 xxbos xxmaj it &#39;s awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk to pro . xxmaj you have to complete goals that involve skating , driving , and walking . xxmaj you create your own skater and give it a name , and you can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout the game until he betrays you and gets you kicked off of the skateboard | xxmaj it &#39;s awesome ! xxmaj in xxmaj story xxmaj mode , your going from punk to pro . xxmaj you have to complete goals that involve skating , driving , and walking . xxmaj you create your own skater and give it a name , and you can make it look stupid or realistic . xxmaj you are with your friend xxmaj eric throughout the game until he betrays you and gets you kicked off of the skateboard xxunk | . 1 what xxmaj i &#39;ve read , xxmaj death xxmaj bed is based on an actual dream , xxmaj george xxmaj barry , the director , successfully transferred dream to film , only a genius could accomplish such a task . n n xxmaj old mansions make for good quality horror , as do portraits , not sure what to make of the killer bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also , this | xxmaj i &#39;ve read , xxmaj death xxmaj bed is based on an actual dream , xxmaj george xxmaj barry , the director , successfully transferred dream to film , only a genius could accomplish such a task . n n xxmaj old mansions make for good quality horror , as do portraits , not sure what to make of the killer bed with its killer yellow liquid , quite a bizarre dream , indeed . xxmaj also , this is | . Now that our data is ready, we can fine-tune the pretrained language model. . Fine tuning the language model . For converting the integer word indices into activations that we can use for our neural network, we will use embeddings, just like we did for collaborative filtering and tabular modelling. Then those embeddings are fed in a Recurrent Neural Network (RNN), using an architecture called AWD_LSTM (we will show how to write such a model from scratch in &lt;&gt;). As we discussed earlier, the embeddings in the pretrained model are merged with random embeddings added for words that weren&#39;t in the pretraining vocabulary. This is handled automatically inside language_model_learner:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn = language_model_learner( dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy, Perplexity()]).to_fp16() . The loss function used by default is cross entropy loss, since we essentially have a classification problem (the different categories being the words in our vocab). A metric often used in NLP for language models is called perplexity. It is the exponential of the loss (i.e. torch.exp(cross_entropy)). We will also add accuracy, to see how many times our model is right when trying to predict the next word, since cross entropy (as we&#39;ve seen) is both hard to interpret, and also tells you more about the model&#39;s confidence, rather than just its accuracy . The grey first arrow in our overall picture has been done for us and made available as a pretrained model in fastai; we&#39;ve now built the DataLoaders and Learner for the second stage, and we&#39;re ready to fine-tune it! . . It takes quite a while to train each epoch, so we&#39;ll be saving the intermediate model results during the training process. Since fine_tune doesn&#39;t do that for us, we&#39;ll just use fit_one_cycle. Just like cnn_learner, language_model_learner automatically calls freeze when using a pretrained model (which is the default), so this will only train the embeddings (which is the only part of the model that contains randomly initialized weights--i.e. embeddings for words that are in our IMDb vocab, but aren&#39;t in the pretrained model vocab): . learn.fit_one_cycle(1, 2e-2) . epoch train_loss valid_loss accuracy perplexity time . 0 | 4.120048 | 3.912788 | 0.299565 | 50.038246 | 11:39 | . This model takes a while to train, so it&#39;s a good opportunity to talk about saving intermediary results. . Saving and loading models . You can easily save the state of your model like so: . learn.save(&#39;1epoch&#39;) . It will create a file in learn.path/models/ named &quot;1epoch.pth&quot;. If you want to load your model in another machine after creating your Learner the same way, or resume training later, you can load the content of this file with: . learn = learn.load(&#39;1epoch&#39;) . We can them finetune the model after unfreezing: . learn.unfreeze() learn.fit_one_cycle(10, 2e-3) . epoch train_loss valid_loss accuracy perplexity time . 0 | 3.893486 | 3.772820 | 0.317104 | 43.502548 | 12:37 | . 1 | 3.820479 | 3.717197 | 0.323790 | 41.148880 | 12:30 | . 2 | 3.735622 | 3.659760 | 0.330321 | 38.851997 | 12:09 | . 3 | 3.677086 | 3.624794 | 0.333960 | 37.516987 | 12:12 | . 4 | 3.636646 | 3.601300 | 0.337017 | 36.645859 | 12:05 | . 5 | 3.553636 | 3.584241 | 0.339355 | 36.026001 | 12:04 | . 6 | 3.507634 | 3.571892 | 0.341353 | 35.583862 | 12:08 | . 7 | 3.444101 | 3.565988 | 0.342194 | 35.374371 | 12:08 | . 8 | 3.398597 | 3.566283 | 0.342647 | 35.384815 | 12:11 | . 9 | 3.375563 | 3.568166 | 0.342528 | 35.451500 | 12:05 | . Once this is done, we save all of our model except the final layer that converts activations to probabilities of picking each token in our vocabulary. The model not including the final layer is called the encoder. We can save it with save_encoder: . learn.save_encoder(&#39;finetuned&#39;) . jargon: Encoder: The model not including the task-specific final layer(s). It means much the same thing as body when applied to vision CNNs, but tends to be more used for NLP and generative models. . This completes the second stage of the text classification process: fine-tuning the language model. We can now fine tune this language model using the IMDb sentiment labels. . Text generation . Before using this to fine-tune a classifier on the review, we can use our model to generate random reviews: since it&#39;s trained to guess what the next word of the sentence is, we can use it to write new reviews: . TEXT = &quot;I liked this movie because&quot; N_WORDS = 40 N_SENTENCES = 2 preds = [learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)] . print(&quot; n&quot;.join(preds)) . i liked this movie because of its story and characters . The story line was very strong , very good for a sci - fi film . The main character , Alucard , was very well developed and brought the whole story i liked this movie because i like the idea of the premise of the movie , the ( very ) convenient virus ( which , when you have to kill a few people , the &#34; evil &#34; machine has to be used to protect . As you can see, we add some randomness (we pick a random word based on the probabilities returned by the model) so you don&#39;t get exactly the same review twice. Our model doesn&#39;t have any programmed knowledge of the structure of a sentence or grammar rules, yet it has clearly learned a lot about English sentences: we can see it capitalized properly (I is just transformed to i with our rules -- they require two characters or more to consider a word is capitalized -- so it&#39;s normal to see it lowercased), and is using consistent tense. The general review make sense at first glance, and it&#39;s only if you read carefully you can notice something is a bit off. Not bad for a model trained in a couple of hours! . Our end goal wasn&#39;t to train a model to generate reviews, but to classify them... so let&#39;s use this model to do just that. . Creating the classifier DataLoaders . We&#39;re now moving from language model fine tuning, to classifier fine tuning. To re-cap, a language model predicts the next word of a document, so it doesn&#39;t need any external labels. A classifier, however, predicts some external label--in the case of IMDb, it&#39;s the sentiment of a document. . This means that the structure of our DataBlock for NLP classification will look very familiar; it&#39;s actually nearly the same as we&#39;ve seen for the many image classification datasets we&#39;ve worked with: . dls_clas = DataBlock( blocks=(TextBlock.from_folder(path, vocab=dls_lm.vocab),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path, path=path, bs=128, seq_len=72) . Just like with image classification, show_batch shows the dependent variable (sentiment, in this case) with each independent variable (movie review text): . dls_clas.show_batch(max_n=3) . text category . 0 xxbos i rate this movie with 3 skulls , only coz the girls knew how to scream , this could &#39;ve been a better movie , if actors were better , the twins were xxup ok , i believed they were evil , but the eldest and youngest brother , they sucked really bad , it seemed like they were reading the scripts instead of acting them … . spoiler : if they &#39;re vampire &#39;s why do they freeze the blood ? vampires ca n&#39;t drink frozen blood , the sister in the movie says let &#39;s drink her while she is alive … .but then when they &#39;re moving to another house , they take on a cooler they &#39;re frozen blood . end of spoiler n n it was a huge waste of time , and that made me mad coz i read all the reviews of how | neg | . 1 xxbos i have read all of the xxmaj love xxmaj come xxmaj softly books . xxmaj knowing full well that movies can not use all aspects of the book , but generally they at least have the main point of the book . i was highly disappointed in this movie . xxmaj the only thing that they have in this movie that is in the book is that xxmaj missy &#39;s father comes to xxunk in the book both parents come ) . xxmaj that is all . xxmaj the story line was so twisted and far fetch and yes , sad , from the book , that i just could n&#39;t enjoy it . xxmaj even if i did n&#39;t read the book it was too sad . i do know that xxmaj pioneer life was rough , but the whole movie was a downer . xxmaj the rating | neg | . 2 xxbos xxmaj this , for lack of a better term , movie is lousy . xxmaj where do i start … … n n xxmaj cinemaphotography - xxmaj this was , perhaps , the worst xxmaj i &#39;ve seen this year . xxmaj it looked like the camera was being tossed from camera man to camera man . xxmaj maybe they only had one camera . xxmaj it gives you the sensation of being a volleyball . n n xxmaj there are a bunch of scenes , haphazardly , thrown in with no continuity at all . xxmaj when they did the &#39; split screen &#39; , it was absurd . xxmaj everything was squished flat , it looked ridiculous . n n xxmaj the color tones were way off . xxmaj these people need to learn how to balance a camera . xxmaj this &#39; movie &#39; is poorly made , and | neg | . Looking at the DataBlock definition above, every piece is familiar from previous data blocks we&#39;ve built, with two important exceptions: . TextBlock.from_folder no longer has the is_lm=True parameter, and | We pass the vocab we created for the language model fine-tuning. | . The reason that we pass the vocab of the language model is to make sure we use the same correspondence of token to index. Otherwise the embeddings we learned in our fine-tuned language model won&#39;t make any sense to this model, and the fine-tuning step won&#39;t be of any use. . By passing is_lm=False (or not passing is_lm at all, since it defaults to False) we tell TextBlock that we have regular labeled data, rather than using the next tokens as labels. There is one challenge we have to deal with, however, which is to do with collating multiple documents into a minibatch. Let&#39;s see with an example, by trying to create a minibatch containing the first 10 documents. First we&#39;ll numericalize them: . nums_samp = toks200[:10].map(num) . Let&#39;s now look at how many tokens each of these 10 movie reviews have: . nums_samp.map(len) . (#10) [228,238,121,290,196,194,533,124,581,155] . Remember, PyTorch DataLoaders need to collate all the items in a batch into a single tensor, and that a single tensor has a fixed shape (i.e. it has some particular length on every axis, and all items must be consistent). This should look a bit familiar: we had the same issue with images. In that case, we use cropping, padding, and/or squishing to make everything the same size. Cropping might not be a good idea for documents, because it seems likely we&#39;d remove some key information (having said that, the same issue is true for images, and we use cropping there; data augmentation hasn&#39;t been well explored for NLP yet, so perhaps there are actually opportunities to use cropping in NLP too!) You can&#39;t really &quot;squish&quot; a document. So that leaves padding! . We will expand the shortest texts to make them all the same size. To do this, we use a special token that will be ignored by our model. This is called padding (just like in vision). Additionally, to avoid memory issues and improve performance, we will batch together texts that are roughly the same lengths (with some shuffling for the training set). We do this by (approximately, for the training set) sorting the documents by length prior to each epoch. The result of this is that the documents collated into a single batch will tend of be of similar lengths. We won&#39;t make every batch, therefore, the same size, but will instead use the size of the largest document in each batch. (It is possible to do something similar with images, which is especially useful for irregularly sized rectangular images, although as we write these words, no library provides good support for this yet, and there aren&#39;t any papers covering it. It&#39;s something we&#39;re planning to add to fastai soon however, so have a look on the book website, where we&#39;ll add information about this if and when it&#39;s working well.) . The padding and sorting is automatically done by the data block API for us when using a TextBlock, with is_lm=False. (We don&#39;t have this same issue for language model data, since we concatenate all the documents together first, and then split them into equally sized sections.) . We can now create a model to classify our texts: . learn = text_classifier_learner(dls_clas, AWD_LSTM, drop_mult=0.5, metrics=accuracy).to_fp16() . The final step prior to training the classifier is to load the encoder from our fine-tuned language model. We use load_encoder instead of load because we only have pretrained weights available for the encoder; load by default raises an exception if an incomplete model is loaded. . learn = learn.load_encoder(&#39;finetuned&#39;) . Fine tuning the classifier . The last step is to train with discriminative learning rates and gradual unfreezing. In computer vision, we often unfreeze the model all at once, but for NLP classifiers, we find that unfreezing a few layers at a time makes a real difference. . learn.fit_one_cycle(1, 2e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.347427 | 0.184480 | 0.929320 | 00:33 | . In just one epoch we get the same result as our training in &lt;&gt;, not too bad! We can pass -2 to freeze_to to freeze all except the last two parameter groups:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn.freeze_to(-2) learn.fit_one_cycle(1, slice(1e-2/(2.6**4),1e-2)) . epoch train_loss valid_loss accuracy time . 0 | 0.247763 | 0.171683 | 0.934640 | 00:37 | . Then we can unfreeze a bit more, and continue training: . learn.freeze_to(-3) learn.fit_one_cycle(1, slice(5e-3/(2.6**4),5e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.193377 | 0.156696 | 0.941200 | 00:45 | . And finally, the whole model! . learn.unfreeze() learn.fit_one_cycle(2, slice(1e-3/(2.6**4),1e-3)) . epoch train_loss valid_loss accuracy time . 0 | 0.172888 | 0.153770 | 0.943120 | 01:01 | . 1 | 0.161492 | 0.155567 | 0.942640 | 00:57 | . We reach 94.3% accuracy, which was state-of-the-art just three years ago. By training a model on all the texts read backwards and averaging the predictions of those two models, we can even get to 95.1% accuracy, which was the state of the art introduced by the ULMFiT paper. It was only beaten a few months ago, fine-tuning a much bigger model and using expensive data augmentation (translating sentences in another language and back, using another model for translation). . Using a pretrained model let us build a fine-tuned language model that was pretty powerful, to either generate fake reviews or help classify them. It is good to remember that this technology can also be used for malign purposes. . Disinformation and language models . Even simple algorithms based on rules, before the days of widely available deep learning language models, could be used to create fraudulent accounts and try to influence policymakers. Jeff Kao, now a computational journalist at ProPublica, analysed the comments that were sent to the FCC in the USA regarding a 2017 proposal to repeal net neutrality. In his article More than a Million Pro-Repeal Net Neutrality Comments were Likely Faked&quot;, he discovered a large cluster of comments opposing net neutrality that seemed to have been generated by some sort of Madlibs-style mail merge. In &lt;&gt;, the fake comments have been helpfully color-coded by Kao to highlight their formulaic nature.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Comments received during the neutral neutrality debate . Kao estimated that &quot;less than 800,000 of the 22M+ comments… could be considered truly unique&quot; and that &quot;more than 99% of the truly unique comments were in favor of keeping net neutrality.&quot; . Given advances in language modeling that have occurred since 2017, such fraudulent campaigns could be nearly impossible to catch now. You now have all the tools at your disposal necessary to create and compelling language model. That is, something that can generate context appropriate believable text. It won&#39;t necessarily be perfectly accurate or correct, but it will be believable. Think about what this technology would mean when put together with the kinds of disinformation campaigns we have learned about. Take a look at this conversation on Reddit shown in &lt;&gt;, where a language model based on OpenAI&#39;s GPT-2 algorithm is having a conversation with itself about whether the US government should cut defense spending:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; An algorithm talking to itself on Reddit . In this case, the use of the algorithm is being done explicitly. But imagine what would happen if a bad actor decided to release such an algorithm across social networks. They could do it slowly and carefully, allowing the algorithms to gradually develop followings and trust over time. It would not take many resources to have literally millions of accounts doing this. In such a situation we could easily imagine it getting to a point where the vast majority of discourse online was from bots, and nobody would have any idea that it was happening. . We are already starting to see examples of machine learning being used to generate identities. For example, &lt;&gt; shows us a LinkedIn profile for Katie Jones.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Katie Jones&#39; LinkedIn profile . Katie Jones was connected on LinkedIn to several members of mainstream Washington think tanks. But she didn&#39;t exist. That image you see is auto generated by a generative adversarial network, and somebody named Katie Jones has not, in fact, graduated from the Centre for Strategic and International Studies. . Many people assume or hope that algorithms will come to our defence here. The hope is that we will develop classification algorithms which can automatically recognise auto generated content. The problem, however, is that this will always be an arms race, in which better classification (or discriminator) algorithms can be used to create better generation algorithms. . Questionnaire . What is self-supervised learning? | What is a language model? | Why is a language model considered self-supervised learning? | What are self-supervised models usually used for? | What do we fine-tune language models? | What are the three steps to create a state-of-the-art text classifier? | How do the 50,000 unlabeled movie reviews help create a better text classifier for the IMDb dataset? | What are the three steps to prepare your data for a language model? | What is tokenization? Why do we need it? | Name three different approaches to tokenization. | What is &#39;xxbos&#39;? | List 4 rules that fastai applies to text during tokenization. | Why are repeated characters replaced with a token showing the number of repetitions, and the character that&#39;s repeated? | What is numericalization? | Why might there be words that are replaced with the &quot;unknown word&quot; token? | With a batch size of 64, the first row of the tensor representing the first batch contains the first 64 tokens for the dataset. What does the second row of that tensor contain? What does the first row of the second batch contain? (Careful—students often get this one wrong! Be sure to check your answer against the book website.) | Why do we need padding for text classification? Why don&#39;t we need it for language modeling? | What does an embedding matrix for NLP contain? What is its shape? | What is perplexity? | Why do we have to pass the vocabulary of the language model to the classifier data block? | What is gradual unfreezing? | Why is text generation always likely to be ahead of automatic identification of machine generated texts? | Further research . See what you can learn about language models and disinformation. What are the best language models today? Have a look at some of their outputs. Do you find them convincing? How could a bad actor best use this to create conflict and uncertainty? | Given the limitation that models are unlikely to be able to consistently recognise machine generated texts, what other approaches may be needed to handle large-scale disinformation campaigns that leveraged deep learning? | &lt;/div&gt; . | . .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_nlp.html",
            "relUrl": "/2020/03/19/_nlp.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Chapter 6",
            "content": "[[chapter_multicat]] Other computer vision problems . In the previous chapter we learnt some important practical techniques for training models in practice. Issues like selecting learning rates and the number of epochs are very important to getting good results. . In this chapter we are going to look at other types of computer vision problems, multi-label classification and regression. The first one is when you want to predict more than one label per image (or sometimes none at all) and the second one is when your labels are one (or several) number, a quantity instead of a category. . In the process will study more deeply the output activations, targets, and loss functions in deep learning models. . Multi-label classification . Multi-label classification refers to the problem of identifying the categories of objects in an image, where you may not have exactly one type of object in the image. There may be more than one kind of object, or there may be no objects at all in the classes that you are looking for. . For instance, this would have been a great approach for our bear classifier. One problem with the bear classifier that we rolled out in &lt;&gt; is that if a user uploaded something that wasn&#39;t any kind of bear, the model would still say it was either a grizzly, black, or teddy bear — it had no ability to predict &quot;not a bear at all&quot;. In fact, after we have completed this chapter, it would be a great exercise for you to go back to your image classifier application, and try to retrain it using the multi-label technique. And then, tested by passing in an image which is not of any of your recognised classes.&lt;/p&gt; In practice, we have not seen many examples of people training multi-label classifiers for this purpose. But we very often see both users and developers complaining about this problem. It appears that this simple solution is not at all widely understood or appreciated. Because in practice it is probably more common to have some images with zero matches or more than one match, we should probably expect in practice that multi-label classifiers are more widely applicable than single label classifiers. . First, we&#39;ll seee what a multi-label dataset looks like, then we&#39;ll explain how to get it ready for our model. Then we&#39;ll see that the architecture does not change from last chapter, only the loss function does. Let&#39;s start with the data. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The data . For our example we are going to use the Pascal dataset, which can have more than one kind of classified object per image. . We begin by downloading and extracting the dataset as per usual: . from fastai2.vision.all import * path = untar_data(URLs.PASCAL_2007) . This dataset is different to the ones we have seen before, and that it is not structured by file name or folder, but instead comes with a CSV (comma separated values) file telling us what labels to use for each image. We can have a look at the CSV file by reading it into a Pandas DataFrame: . df = pd.read_csv(path/&#39;train.csv&#39;) df.head() . fname labels is_valid . 0 000005.jpg | chair | True | . 1 000007.jpg | car | True | . 2 000009.jpg | horse person | True | . 3 000012.jpg | car | False | . 4 000016.jpg | bicycle | True | . We can see that the list of categories in each image is shown as a space delimited string. . Sidebar: Pandas and DataFrames . No, it’s not actually a panda! Pandas is a Python library that is used to manipulate and analysis tabular and timeseries data. The main class is DataFrame, which represents a table of rows and columns. You can get a DataFrame from a CSV file, a database table, python dictionaries, and many other sources. In Jupyter, a DataFrame is output as a formatted table, as you see above. . You can access rows and columns of a DataFrame with the iloc property, which lets you access rows and columns as if it is a matrix: . df.iloc[:,0] df.iloc[0,:] # Trailing ‘:’s are always optional (in numpy, PyTorch, pandas, etc), # so this is equivalent: df.iloc[0] . fname 000005.jpg labels chair is_valid True Name: 0, dtype: object . You can also grab a column by name by indexing into a DataFrame directly: . df[&#39;fname&#39;] . 0 000005.jpg 1 000007.jpg 2 000009.jpg 3 000012.jpg 4 000016.jpg ... 5006 009954.jpg 5007 009955.jpg 5008 009958.jpg 5009 009959.jpg 5010 009961.jpg Name: fname, Length: 5011, dtype: object . You can create new columns and do calculations using columns: . TK . Pandas is a fast and flexible library, and is an important part of every data scientist’s Python toolbox. Unfortunately, its API can be rather confusing and surprising, so it takes a while to get familiar with it. If you haven’t used Pandas before, we’d suggest going through a tutorial; we are particularly fond of the book “Python for Data Analysis” by Wes McKinney, the creator of Pandas. It also covers other important libraries like matplotlib and numpy. We will try to briefly describe Pandas functionality we use as we come across it, but will not go into the level of detail of McKinney’s book. . End sidebar . Now that we have seen what the data looks like, let&#39;s make it ready for model training. . Constructing a data block . How do we convert from a DataFrame object to a DataLoaders object? We generally suggest using the data block API for creating a DataLoaders object, where possible, since it provides a good mix of flexibility and simplicity. Here we will show you the steps that we take to use the data blocks API to construct a DataLoaders object in practice, using this dataset as an example. . As we have seen, PyTorch and fastai have two main classes for representing and accessing a training set or validation set: . Dataset:: a collection which returns a tuple of your independent and dependent variable for a single item | DataLoader:: an iterator which provides a stream of mini batches, where each mini batch is a couple of a batch of independent variables and a batch of dependent variables | . On top of these, fastai provides two classes for bringing your training and validation sets together: . Datasets:: an object which contains a training Dataset and a validation Dataset | DataLoaders:: an object which contains a training DataLoader and a validation DataLoader | . Since a DataLoader builds on top of a Dataset, and adds additional functionality to it (collating multiple items into a mini batch), it’s often easiest to start by creating and testing Datasets, and then look at DataLoaders after that’s working. . When we create a DataBlock, we build up gradually, step-by-step, and use the notebook to check our data along the way. This is a great way to make sure that you maintain momentum as you are coding, and that you keep an eye out for any problems. It’s easy to debug, because you know that if there are any problems, it is in the line of code you just typed! . Let’s start with the simplest case, which is a data block created with no parameters: . dblock = DataBlock() . We can create a Datasets object from this. The only thing needed is a source, in this case, our dataframe: . dsets = dblock.datasets(df) . this contains a train and a “valid” dataset, which we can index into: . dsets.train[0] . (fname 008663.jpg labels car person is_valid False Name: 4346, dtype: object, fname 008663.jpg labels car person is_valid False Name: 4346, dtype: object) . As you can see, this simply returns a row of the dataframe, twice. This is because by default, the datablock assumes we have two things: input and target. We are going to need to grab the appropriate fields from the DataFrame, which we can do by passing get_x and get_y functions: . dblock = DataBlock(get_x = lambda r: r[&#39;fname&#39;], get_y = lambda r: r[&#39;labels&#39;]) dsets = dblock.datasets(df) dsets.train[0] . (&#39;005620.jpg&#39;, &#39;aeroplane&#39;) . As you can see, rather than defining a function in the usual way, we are using Python’s lambda keyword. This is just a shortcut for defining and then referring to a function. The above is identical to the following more verbose approach: . def get_x(r): return r[&#39;fname&#39;] def get_y(r): return r[&#39;labels&#39;] dblock = DataBlock(get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (&#39;002549.jpg&#39;, &#39;tvmonitor&#39;) . lambda functions are great for quickly iterating, however they are not compatible with serialization, so we advise you to use the more verbose approach if you want to export your Learner after training (they are fine if you are just experimenting). . We can see that the independent variable will need to be converted into a complete path, so that we can open it as an image, and the second will need to be split on the space character (which is the default for Python’s split function) so that it becomes a list: . def get_x(r): return path/&#39;train&#39;/r[&#39;fname&#39;] def get_y(r): return r[&#39;labels&#39;].split(&#39; &#39;) dblock = DataBlock(get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (Path(&#39;train/002844.jpg&#39;), [&#39;train&#39;]) . To actually open the image and do the conversion to tensors, we will need to use a set of transforms; block types will provide us with those. We can use the same block types that we have used previously, with one exception. The ImageBlock will work fine again, because we have a path which points to a valid image, but the CategoryBlock is not going to work. The problem is: that block returns a single integer. But we need to be able to have multiple labels for each item. To solve this, we use a MultiCategoryBlock. This type of block expects to receive a list of strings, as we have in this case, so let’s test it out: . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[0] . (PILImage mode=RGB size=500x375, TensorMultiCategory([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])) . As you can see, our list of categories is not encoded in the same way that it was for the regular CategoryBlock. In that case, we had a single integer, representing which category was present, based on its location in our vocab. In this case, however, we instead have a list of zeros, with a one in any position where that category is present. For example, if there is a one in the second and fourth positions, then that means that vocab items two and four are present in this image. This is known as one hot encoding. The reason we can’t easily just use a list of category indices, is that each list would be a different length, and PyTorch requires tensors, where everything has to be the same length. . jargon: One hot encoding: using a vector of zeros, with a one in each location that is represented in the data, to encode a list of integers. . Let’s check what the categories represent for this example (we are using the convenient torch.where function, which tells us all of the indices where our condition is true or false): . idxs = torch.where(dsets.train[0][1]==1.)[0] dsets.train.vocab[idxs] . (#1) [&#39;dog&#39;] . With numpy arrays, PyTorch tensors, and fastai’s L class, you can index directly using a list or vector, which makes a lot of code (such as this example) much clearer and more concise. . We have ignored the column is_valid up until now, which means that DataBlock has been using a random split by default. To explicitly choose the elements of our validation set, we need to write a function and pass it to splitter (or use one of fastai&#39;s predefined functions or classes). It will take the items (here our whole dataframe) and must return two (or more) list of integers. . def splitter(df): train = df.index[~df[&#39;is_valid&#39;]].tolist() valid = df.index[df[&#39;is_valid&#39;]].tolist() return train,valid dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=splitter, get_x=get_x, get_y=get_y) dsets = dblock.datasets(df) dsets.train[0] . (PILImage mode=RGB size=500x333, TensorMultiCategory([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])) . As we have discussed, a DataLoader collates the items from a Dataset into a mini batch. This is a tuple of tensors, where each tensor simply stacks the items from that location in the Dataset item. Now that we have confirmed that the individual items look okay there&#39;s one more step we need to ensure we can create our DataLoaders, which is to ensure that every item is of the same size. To do this, we can use RandomResizedCrop: . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=splitter, get_x=get_x, get_y=get_y, item_tfms = RandomResizedCrop(128, min_scale=0.35)) dls = dblock.dataloaders(df) . And now we can display a sample of our data: . dls.show_batch(rows=1, cols=3) . And remember that if anything goes wrong when you create your DataLoaders from your DataBlock, or if you want to view exactly what happens with your DataBlock, you can use the summary method we presented in the last chapter. . Our data is now ready for training a model. As we will see, nothing is going to change when we create our Learner, but behind the scenes, the fastai library will pick a new loss function for us: binary cross entropy. . Binary cross entropy . Now we&#39;ll create our Learner. We saw in &lt;&gt; that a Learner object contains four main things: the model, a DataLoaders object, an Optimizer, and the loss function to use. We already how our DataLoaders, and we can leverage fastai&#39;s resnet models (which we&#39;ll learn how to create from scratch later), and we know how to create an SGD optimizer. So let&#39;s focus on ensuring we have a suitable loss function. To do this, let&#39;s use cnn_learner to create a Learner, so we can look at its activations:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn = cnn_learner(dls, resnet18) . We also saw that the model in a Learner is generally an object of a class inheriting from nn.Module, and that you can call it using parentheses and it will return the activations of a model. You should pass it your independent variable, as a mini batch. We can try it out by grabbing a mini batch from our DataLoader, and then passing it to the model: . x,y = dls.train.one_batch() activs = learn.model(x) activs.shape . torch.Size([64, 20]) . Have a think about why activs has this shape… We have a batch size of 64. And we need to calculate the probability of each of 20 categories. Here’s what one of those activations looks like: . activs[0] . tensor([-1.0028, 0.3400, -0.5906, 0.7806, 3.1160, -0.1994, 1.3180, 1.6361, -1.7553, 0.2217, 2.8052, 1.3229, 0.9369, -1.4760, -0.3204, -2.3116, -3.8615, -1.5931, 0.0745, -3.6006], device=&#39;cuda:5&#39;, grad_fn=&lt;SelectBackward&gt;) . . Note: Knowing how to manually get a mini batch and pass it into a model, and look at the activations and loss, is really important for debugging your model. It is also very helpful for learning, so that you can see exactly what is going on. . They aren’t yet scaled between zero and one. We learned in &lt;&gt; how to scale activations to be between zero and one: the sigmoid function. We also saw how to calculate a loss based on this--this is our loss function from &lt;&gt;, with the addition of log as discussed in the last chapter:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; def binary_cross_entropy(inputs, targets): inputs = inputs.sigmoid() return torch.where(targets==1, 1-inputs, inputs).log().mean() . Note that because we have a one-hot encoded dependent variable, we can&#39;t directly use nll_loss or softmax (and therefore we can&#39;t use cross_entropy): . softmax, as we saw, requires that all predictions sum to one, and tends to push one activation to be much larger than the others (due to the use of exp); however, we may well have multiple objects that we&#39;re confident appear in an image, so restricting the maximum sum of activations to one is not a good idea. By the same reasoning, we may want the sum to be less than one, if we don&#39;t think any of the categories appear in an image. | nll_loss, as we saw, returns the value of just one activation: the single activation corresponding with the single label for an item. This doesn&#39;t make sense when we have multiple labels. | . On the other hand, the binary_cross_entropy function, which is just mnist_loss along with log, provides just what we need, thanks to the magic of PyTorch&#39;s elementwise operations. Each activation will be compared to each target for each column, so we don&#39;t have to do anything to make this function work for multiple colums. . j: One of the things I really like about working with libraries like PyTorch, with broadcasting and elementwise operations, is that quite frequently I find I can write code that works equally well for a single item, or a batch of items, without changes. binary_cross_entropy is a great example of this. By using these operations, we don&#39;t have to write loops ourselves, and can rely on PyTorch to do the looping we need as appropriate for the rank of the tensors we&#39;re working with. . PyTorch already provides this function for us. In fact, it provides a number of versions, with rather confusing names! . F.binary_cross_entropy, and it&#39;s module equivalent nn.BCELoss, calculate cross entropy on a one-hot encoded target, but do not include the initial sigmoid. Normally for one-hot encoded targets you&#39;ll want F.binary_cross_entropy_with_logits (or nn.BCEWithLogitsLoss), which do both sigmoid and binary cross entropy in a single function, as in our example above. . The equivalent for single-label datasets (like MNIST or Pets), where the target is encoded as a single integer, is F.nll_loss or nn.NLLLoss for the version without the initial softmax, and F.cross_entropy or nn.CrossEntropyLoss for the version with the initial softmax. . Since we have a one-hot encoded target, we will use BCEWithLogitsLoss. . loss_func = nn.BCEWithLogitsLoss() loss = loss_func(activs, y) loss . tensor(1.0082, device=&#39;cuda:5&#39;, grad_fn=&lt;BinaryCrossEntropyWithLogitsBackward&gt;) . We don&#39;t actually need to tell fastai to use this loss function (although we can if we want) since it will be automatically chosen for us. fastai knows that the DataLoaders have multiple category labels, so it will use nn.BCEWithLogitsLoss by default. . One change compared to the last chapter is the metric we use: since we are in a multilabel problem, we can&#39;t use the accuracy function. Why is that? Well accuracy was comparing our outputs to our targets like so: . def accuracy(inp, targ, axis=-1): &quot;Compute accuracy with `targ` when `pred` is bs * n_classes&quot; pred = inp.argmax(dim=axis) return (pred == targ).float().mean() . The class predicted was the one with the highest activation (this is what argmax does). Here it doesn&#39;t work because we could have more than one prediction on a single image. After applying the sigmoid to our activations (to make them between 0 and 1), we need to decide which ones are 0s and which ones are 1s by picking a threshold. Each value above the threshold will be considered as a 1, and each value lower than the threshold will be considered a 0: . def accuracy_multi(inp, targ, thresh=0.5, sigmoid=True): &quot;Compute accuracy when `inp` and `targ` are the same size.&quot; if sigmoid: inp = inp.sigmoid() return ((inp&gt;thresh)==targ.bool()).float().mean() . If we pass accuracy_multi directly as a metric, it will use the default value for threshold, which is 0.5. We might want to adjust that default and create a new version of accuracy_multi that has a different default. To help with this, there is a function in python called partial. It allows us to bind a function with some arguments or keyword arguments, making a new version of that function that, whenever it is called, always includes those arguments. For instance, here is a simple function taking two arguments: . def say_hello(name, say_what=&quot;Hello&quot;): return f&quot;{say_what} {name}.&quot; say_hello(&#39;Jeremy&#39;),say_hello(&#39;Jeremy&#39;, &#39;Ahoy!&#39;) . (&#39;Hello Jeremy.&#39;, &#39;Ahoy! Jeremy.&#39;) . We can switch to a French version of that function by using partial: . f = partial(say_hello, say_what=&quot;Bonjour&quot;) f(&quot;Jeremy&quot;),f(&quot;Sylvain&quot;) . (&#39;Bonjour Jeremy.&#39;, &#39;Bonjour Sylvain.&#39;) . We can now train our model. Let&#39;s try setting the accuracy threshold to 0.2 for our metric: . learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2)) learn.fine_tune(3, base_lr=3e-3, freeze_epochs=4) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.903610 | 0.659728 | 0.263068 | 00:07 | . 1 | 0.724266 | 0.346332 | 0.525458 | 00:07 | . 2 | 0.415597 | 0.125662 | 0.937590 | 00:07 | . 3 | 0.254987 | 0.116880 | 0.945418 | 00:07 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.123872 | 0.132634 | 0.940179 | 00:08 | . 1 | 0.112387 | 0.113758 | 0.949343 | 00:08 | . 2 | 0.092151 | 0.104368 | 0.951195 | 00:08 | . Picking a threshold is important. If you pick a threshold that&#39;s too low, you&#39;ll often be failing to select correctly labelled objects. We can see this by changing our metric, and then calling validate, which returns the validation loss and metrics: . learn.metrics = partial(accuracy_multi, thresh=0.1) learn.validate() . (#2) [0.10436797887086868,0.93057781457901] . If you pick a threshold that&#39;s too high, you&#39;ll often be selecting correctly labelled objects: . learn.metrics = partial(accuracy_multi, thresh=0.99) learn.validate() . (#2) [0.10436797887086868,0.9416930675506592] . We can find the best threshold by trying a few levels and seeing what works best. This is much faster if we just grab the predictions once: . preds,targs = learn.get_preds() . ...and then we can call the metric directly. Note that by default get_preds applies the output activation function (sigmoid, in this case) for us, so we&#39;ll need to tell accuracy_multi to not apply it: . accuracy_multi(preds, targs, thresh=0.9, sigmoid=False) . TensorMultiCategory(0.9554) . We can now use this approach to find the best threshold level: . xs = torch.linspace(0.05,0.95,29) accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs] plt.plot(xs,accs); . In this case, we&#39;re using the validation set to pick a hyperparameter (the threshold), which is the purpose of the validation set. But sometimes students have expressed their concern that we might be overfitting to the validation set, since we&#39;re trying lots of values to see which is the best. However, as you see in the plot, changing the threshold in this case results in a smooth curve, so we&#39;re clearly not picking some inappropriate outlier. This is a good example of where you have to be careful of the difference between theory (don&#39;t try lots of hyperparameter values or you might overfit the validation set) versus practice (if the relationship is smooth, then it&#39;s fine to do this). . This concludes the part of thic chapter dedicated to multi-label classification. Let&#39;s have a look at a regression problem now. . Regression . It&#39;s easy to think of deep learning models as being classified into domains, like computer vision, NLP, and so forth. And indeed, that&#39;s how fastai classifies its applications—largely because that&#39;s how most people are used to thinking of things. . But really, that&#39;s hiding a more interesting and deeper perspective. A model is defined by its independent and dependent variables, along with its loss function. That means that there&#39;s really a far wider array of models than just the simple domain based split. Perhaps we have an independent variable that&#39;s an image, and a dependent that&#39;s text (e.g. generating a caption from an image); or perhaps we have an independent variable that&#39;s text, and dependent that&#39;s an image (e.g. generating an image from a caption—which is actually possible for deep learning to do!); or perhaps we&#39;ve got images, texts, and tabular data as independent variables, and we&#39;re trying to predict product purchases; …the possibilities really are endless. . To be able to move beyond fixed applications, to crafting your own novel solutions to novel problems, it helps to really understand the data blocks API (and maybe also the mid-tier API, which we&#39;ll see later in the book). As an example, let&#39;s consider the problem of image regression. This refers to learning from a dataset where the independent variable is an image, and the dependent variable is one or more floats. Often we see people treat image regression as a whole separate application—but as you&#39;ll see here we can treat it as just another CNN on top of the data block API. . We&#39;re going to jump straight to a somewhat tricky variant of image regression, because we know you&#39;re ready for it! We&#39;re going to do a key point model. A key point refers to a specific location represented in an image—in this case, we&#39;ll be looking for the center of the person&#39;s face in each image. That means we&#39;ll actually be predicting two values for each image: the row and column of the face center. . Assemble the data . We will use the Biwi Kinect Head Pose Dataset for this section. First thing first, let&#39;s begin by downloading the dataset as usual. . path = untar_data(URLs.BIWI_HEAD_POSE) . Let&#39;s see what we&#39;ve got! . path.ls() . (#50) [Path(&#39;13.obj&#39;),Path(&#39;07.obj&#39;),Path(&#39;06.obj&#39;),Path(&#39;13&#39;),Path(&#39;10&#39;),Path(&#39;02&#39;),Path(&#39;11&#39;),Path(&#39;01&#39;),Path(&#39;20.obj&#39;),Path(&#39;17&#39;)...] . There are 24 directories numbered from 01 to 24 (they correspond to the different persons photographed) and a corresponding .obj file (we won&#39;t need them here). We&#39;ll take a look inside one of these directories: . (path/&#39;01&#39;).ls() . (#1000) [Path(&#39;01/frame_00281_pose.txt&#39;),Path(&#39;01/frame_00078_pose.txt&#39;),Path(&#39;01/frame_00349_rgb.jpg&#39;),Path(&#39;01/frame_00304_pose.txt&#39;),Path(&#39;01/frame_00207_pose.txt&#39;),Path(&#39;01/frame_00116_rgb.jpg&#39;),Path(&#39;01/frame_00084_rgb.jpg&#39;),Path(&#39;01/frame_00070_rgb.jpg&#39;),Path(&#39;01/frame_00125_pose.txt&#39;),Path(&#39;01/frame_00324_rgb.jpg&#39;)...] . Inside the subdirectories, we have different frames, each of them come with an image ( _rgb.jpg) and a pose file ( _pose.txt). We can easily get all the image files recursively with get_image_files, then write a function that convert an image filename to its associated pose file. . img_files = get_image_files(path) def img2pose(x): return Path(f&#39;{str(x)[:-7]}pose.txt&#39;) img2pose(img_files[0]) . Path(&#39;13/frame_00349_pose.txt&#39;) . We can have a look at our first image: . im = PILImage.create(img_files[0]) im.shape . (480, 640) . im.to_thumb(160) . The Biwi dataset web site explains the format of the pose text file associated with each image, which shows the location of the center of the head. The details of this aren&#39;t important for our purposes, so we&#39;ll just show the function we use to extract the head center point: . cal = np.genfromtxt(path/&#39;01&#39;/&#39;rgb.cal&#39;, skip_footer=6) def get_ctr(f): ctr = np.genfromtxt(img2pose(f), skip_header=3) c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2] c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2] return tensor([c1,c2]) . This function returns the coordinates as a tensor of two items: . get_ctr(img_files[0]) . tensor([384.6370, 259.4787]) . We can pass this function to DataBlock as get_y, since it is responsible for labeling each item. We&#39;ll resize the images to half their input size, just to speed up training a bit. . One important point to note is that we should not just use a random splitter. The reason for this is that the same person appears in multiple images in this dataset — but we want to ensure that our model can generalise to people that it hasn&#39;t seen yet. Each folder in the dataset contains the images for one person. Therefore, we can create a splitter function which returns true for just one person, resulting in a validation set containing just that person&#39;s images. . The only other difference to previous data block examples is that the second block is a PointBlock. This is necessary so that fastai knows that the labels represent coordinates; that way, it knows that when doing data augmentation, it should do the same augmentation to these coordinates as it does to the images. . biwi = DataBlock( blocks=(ImageBlock, PointBlock), get_items=get_image_files, get_y=get_ctr, splitter=FuncSplitter(lambda o: o.parent.name==&#39;13&#39;), batch_tfms=[*aug_transforms(size=(240,320)), Normalize.from_stats(*imagenet_stats)] ) . . Important: We&#8217;re not aware of other libraries (except for fastai) that automatically and correctly apply data augmentation to coordinates. So if you&#8217;re working with another library, you may need to disable data augmentation for these kinda of problems. . Before doing any modeling, we should look at our data to confirm it seems OK: . dls = biwi.dataloaders(path) dls.show_batch(max_n=9, figsize=(8,6)) . That&#39;s looking good! As well as looking at the batch visually, it&#39;s a good idea to also look at the underlying tensors (especially as a student, it will help clarify your understanding of what your model is really seeing). . xb,yb = dls.one_batch() xb.shape,yb.shape . (torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2])) . Make sure that you understand why these are the shapes for our mini-batches. . Here&#39;s an example of one row from the dependent variable: . yb[0] . tensor([[0.0111, 0.1810]], device=&#39;cuda:5&#39;) . As you can see, we haven&#39;t had to use a separate image regression application; all we&#39;ve had to do is label the data, and tell fastai what kind of data the independent and dependent variables represent. . It&#39;s the same for creating our Learner. We will use the same function as before, this time with just a new parameter and we will ready to train our model. . Training a model . As usual we can use cnn_learner to create our Learner. Remember way back in &lt;&gt; how we used y_range to tell fastai the range of our targets? We&#39;ll do the same here; coordinates in fastai and PyTorch are always rescaled between -1 and +1.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn = cnn_learner(dls, resnet18, y_range=(-1,1)) . y_range is implemented in fastai using sigmoid_range, which is defined as: . def sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo . This is set as the final layer of the model, if y_range is defined. Take a moment to think about what this function does, and why it forces the model to output activations in the range (low,high). . Here&#39;s what it looks like: . plot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4) . We didn&#39;t specify a loss function, which means we&#39;re getting whatever fastai chooses as the default. Let&#39;s see what it picked for us: . dls.loss_func . FlattenedLoss of MSELoss() . This makes sense, since when coordinates are used a dependent variable, most of the time we&#39;re likely to be trying to predict something as close as possible; that&#39;s basically what MSELoss (mean-squared error loss) does. If you want to use a different loss function, you can pass it to cnn_learner using the loss_func parameter. . Note also that we didn&#39;t specify any metrics. That&#39;s because the MSE is already a useful metric for this task (although it&#39;s probably more interpretable after we take the square root). . We can pick a good learning rate with the Learning Rate Finder: . learn.lr_find() . We&#39;ll try an LR of 2e-2: . lr = 2e-2 learn.fit_one_cycle(5, lr) . epoch train_loss valid_loss time . 0 | 0.045840 | 0.012957 | 00:36 | . 1 | 0.006369 | 0.001853 | 00:36 | . 2 | 0.003000 | 0.000496 | 00:37 | . 3 | 0.001963 | 0.000360 | 00:37 | . 4 | 0.001584 | 0.000116 | 00:36 | . Generally when we run this we get a loss of around 0.0001, which corresponds to an average coordinate prediction error of: . math.sqrt(0.0001) . 0.01 . This sounds very accurate! But most importantly, we should have a look at our results with Learner.show_results. The left side is actual (ground truth) and the right side are our model&#39;s predictions. . learn.show_results(ds_idx=1, max_n=3, figsize=(6,8)) . It&#39;s quite amazing that with just a few minutes of computation we&#39;ve created such an accurate key points model, and without any special domain-specific application. This is the power of building on flexible APIs, and using transfer learning! It&#39;s particularly striking that we&#39;ve been able to use transfer learning so effectively even between totally different tasks; our pretrained model was trained to do image classification, and we fine-tuned for image regression. . Conclusion . In problems that are at first glance completely different (single-label classification, multi-label classification and regression) we end up using the same model with just different numbers of outputs. The different directions of those trainings is determined by the loss function, which is the one thing that changes. That&#39;s why it simportant to double-check your are using the right loss function for your problem. . In fastai, the library will automatically try to pick the right one from the data you built, but if you are using pure PyTorch to build your DataLoaders, make sure you think hard when you have to decide on your loss function, and remember that you most probably want . nn.CrossEntropyLoss for single-label classification | nn.BCEWithLogitsLoss for multi-label classification | nn.MSELoss for regression | . Questionnaire . how could multi-label classification improve the usability of the bear classifier? | How do we encode the dependent variable in a multi-label classification problem? | How do you access the rows and columns of a DataFrame as if it was a matrix? | How do you get a column by name from a DataFrame? | What is the difference between a dataset and DataLoader? | What does a Datasets object normally contain? | What does a DataLoaders object normally contain? | What does lambda do in Python? | What are the methods to customise how the independent and dependent variables are created with the data block API? | Why is softmax not an appropriate output activation function when using a one hot encoded target? | Why is nll_loss not an appropriate loss function when using a one hot encoded target? | What is the difference between nn.BCELoss and nn.BCEWithLogitsLoss? | Why can&#39;t we use regular accuracy in a multi-label problem? | When is it okay to tune an hyper-parameter on the validation set? | How is y_range implemented in fastai? (See if you can implement it yourself and test it without peaking!) | What is a regression problem? What loss function should you use for such a problem? | What do you need to do to make sure the fastai library applies the same data augmentation to your inputs images and your target point coordinates? | Further research . Read a tutorial about pandas DataFrames and experiment with a few methods that look interesting to you. Have a look at the book website for recommended tutorials. | Retrain the bear classifier using multi-label classification. See if you can make it work effectively with images that don&#39;t contain any bears, including showing that information in the web application. Try an image with two different kinds of bears. Check whether the accuracy on the single label dataset is impacted using multi-label classification. | &lt;/div&gt; .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_multicat.html",
            "relUrl": "/2020/03/19/_multicat.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Chapter 4",
            "content": "[[chapter_mnist_basics]] Under the hood: training a digit classifier . Now that we’ve seen what it looks like to actually train a variety of models, let’s now dig under the hood and see exactly what is going on. We’ll start with computer vision, and will use that to introduce many key tools and concepts of deep learning. We&#39;ll discuss the role of arrays and tensors, and of brodcasting, a powerful technique for using them expressively. We&#39;ll explain stochastic gradient descent (SGD), the mechanism for learning by updating weights automatically. We&#39;ll discuss the choice of loss function for for such a classification task, and the role of mini-batches. And we&#39;ll put these pieces together, to see how they all fit. . In future chapters we’ll do deep dives into other applications as well, and see how these concepts and tools generalize. . But for now, let&#39;s start by considering how images are represented in a computer, then we will make our way up to how to classify different type of images. . Pixels: the foundations of computer vision . In order to understand what happens in a computer vision model, we first have to understand how computers handle images. We&#39;ll use one of the most famous datasets in computer vision, MNIST, for our experiments. MNIST contains hand-written digits, collected by the National Institute of Standards and Technology, and collated into a machine learning dataset by Yann Lecun and his colleagues. Lecun used MNIST in 1998 to demonstrate Lenet 5, the first computer system to demonstrate practically useful recognition of hand-written digit sequences. This was one of the most important breakthroughs in the history of AI. . Sidebar: Tenacity and deep learning . The story of deep learning is one of tenacity and grit from a handful of dedicated researchers. After early hopes (and hype!) neural networks went out of favor in the 1990&#39;s and 2000&#39;s, and just a handful of researchers kept trying to make them work well. Three of them, Yann Lecun, Geoff Hinton, and Yoshua Bengio were awarded the highest honor in computer science, the Turing Award (generally considered the &quot;Nobel Prize of computer science&quot;) after triumphing despite the deep skepticism and disinterest of the wider machine learning and statistics community. . Left to right, Yann Lecun, Geoffrey Hinton and Yoshua Bengio . Geoff Hinton has told of how even academic papers showing dramatically better results than anything previously published would be rejected from top journals and conferences, just because they used a neural network. Yann Lecun&#39;s work on convolutional neural networks, which we will study in the next section, showed that these models could read hand-written text--something that had never been achieved before. However his breakthrough was ignored by most researchers, even as it was used commercially to read 10% of the checks in the US! . In addition to these three Turing Award winners, there are many other researchers who have battled to get us to where we are today. For instance, Jurgen Schmidhuber (who many believe should have shared in the Turing Award) pioneered many important ideas, including working on the LSTM architecture with his student Sepp Hochreiter (widely used for speech recognition and other text modeling tasks, and used in the IMDb example in &lt;&gt;). Perhaps most important of all, Paul Werbos in 1974 invented back-propagation for neural networks, the technique shown in this chapter and used universally for training neural networks (Werbos 1994). His development was almost entirely ignored for decades, but today it is the most important foundation of modern AI.&lt;/p&gt; There is a lesson here for all of us! On your deep learning journey you will face many obstacles, both technical, and (even more difficult) people around you who don&#39;t believe you&#39;ll be successful. There&#39;s one guaranteed way to fail, and that&#39;s to stop trying. We&#39;ve seen that the only consistent trait amongst every fast.ai student that&#39;s gone on to be a world-class practitioner is that they are all very tenacious. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; End sidebar . For this initial tutorial we are just going to try to create a model that can recognise &quot;3&quot;s and &quot;7&quot;s. So let&#39;s download a sample of MNIST which contains images of just these digits: . path = untar_data(URLs.MNIST_SAMPLE) . We can see what&#39;s in this directory by using ls(), a method added by fastai. This method returns an object of a special fastai class called L, which has all the same functionality of Python&#39;s builtin list, plus a lot more. One of its handy features is that, when printed, it displays the count of items, before listing the items themselves (if there&#39;s more than 10 items, it just shows the first few). . path.ls() . (#9) [Path(&#39;cleaned.csv&#39;),Path(&#39;item_list.txt&#39;),Path(&#39;trained_model.pkl&#39;),Path(&#39;models&#39;),Path(&#39;valid&#39;),Path(&#39;labels.csv&#39;),Path(&#39;export.pkl&#39;),Path(&#39;history.csv&#39;),Path(&#39;train&#39;)] . The MNIST dataset shows a very common layout for machine learning datasets: separate folders for the training set, which is used to train a model, and the validation set (and/or test set), which is used to evaluate the model (we&#39;ll be talking a lot about these concepts very soon!) Let&#39;s see what&#39;s inside the training set: . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;train/7&#39;),Path(&#39;train/3&#39;)] . There&#39;s a folder of &quot;3&quot;s, and a folder of &quot;7&quot;s. In machine learning parlance, we say that &quot;3&quot; and &quot;7&quot; are the labels in this dataset. Let&#39;s take a look in one of these folders (using sorted to ensure we all get the same order of files): . threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() threes . (#6131) [Path(&#39;train/3/10.png&#39;),Path(&#39;train/3/10000.png&#39;),Path(&#39;train/3/10011.png&#39;),Path(&#39;train/3/10031.png&#39;),Path(&#39;train/3/10034.png&#39;),Path(&#39;train/3/10042.png&#39;),Path(&#39;train/3/10052.png&#39;),Path(&#39;train/3/1007.png&#39;),Path(&#39;train/3/10074.png&#39;),Path(&#39;train/3/10091.png&#39;)...] . As we might expect, it&#39;s full of image files. Let’s take a look at one now. Here’s an image of a handwritten number ‘3’, taken from the famous MNIST dataset of handwritten numbers: . im3_path = threes[1] im3 = Image.open(im3_path) im3 . Here we are using the Image class from the Python Imaging Library (PIL), which is the most widely used Python package for opening, manipulating, and viewing images. Jupyter knows about PIL images, so it displays the image for us automatically. . In a computer, everything is represented as a number. To view the numbers that make up this image, we have to convert it to a NumPy array or a PyTorch tensor. For instance, here&#39;s a few numbers from the top-left of the image, converted to a numpy array: . array(im3)[4:10,4:10] . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . ...and the same thing as a PyTorch tensor: . tensor(im3)[4:10,4:10] . tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) . We can slice the array to pick just a part with the top of the digit in it, and then use a Pandas DataFrame to color-code the values using a gradient, which shows us clearly how the image is created from the pixel values: . #hide_output im3_t = tensor(im3) df = pd.DataFrame(im3_t[4:15,4:22]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | . 3 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | . 4 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | . 5 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | . . You can see that the background white pixels are stored as the number zero, black is the number 255, and shades of grey are between the two. The entire image contains 28 pixels across and 28 pixels down, for a total of 768 pixels. (This is much smaller than an image that you would get from a phone camera, which has millions of pixels, but is a convenient size for our initial learning and experiments. We will build up to bigger, full-colour images soon.) . So, now you&#39;ve seen what an image looks like to a computer, let&#39;s recall our goal: create a model that can recognise “3”s and “7”s. How might you go about getting a computer to do that? . stop: Before you read on, take a moment to think about how a computer might be able to recognize these two different digits. What kind of features might it be able to look at? How might it be able to identify these features? How could it combine them together? Learning works best when you try to solve problems yourself, rather than just reading somebody else&#39;s answers; so step away from this book for a few minutes, grab a piece of paper and pen, and jot some ideas down… . First try: pixel similarity . So, here is a first idea: how about we find the average pixel value for every pixel of the threes and do the same for each of the sevens. This will give us two group averages, defining what we might call the &quot;ideal&quot; 3 and 7. Then, to classify an image as digit, we see which of these two ideal digits the image is most similar to. This certainly seems like it should be better than nothing, so it will make a good baseline. . . Note: A baseline is a simple model which you are confident should perform reasonably well. It should be very simple to implement, and very easy to test, so that you can then test each of your improved ideas, and make sure they are always better than your baseline. Without starting with a sensible baseline, it is very difficult to know whether your super fancy models are actually any good. One good approach to creating a baseline is doing what we have done here: think of a simple, easy to implement model. Another good approach is to search around to find other people that have solved similar problems to yours, and download and run their code on your dataset. Ideally, try both of these! . Step one for our simple model is to get the average of pixel values for each of our two groups. In the process of doing this, we will learn a lot of neat Python numeric programming tricks! . Let&#39;s create a tensor containing all of our threes stacked together. We already know how to create a tensor containing a single image. To create a tensor containing all the images in a directory, we will first use a Python list comprehension to create a plain list of the single image tensors. . We will use Jupyter to do some little checks of our work along the way -- in this case, making sure that the number of returned items seems reasonable: . seven_tensors = [tensor(Image.open(o)) for o in sevens] three_tensors = [tensor(Image.open(o)) for o in threes] len(three_tensors),len(seven_tensors) . (6131, 6265) . . Note: List and dictionary comprehensions are a wonderful feature of Python. Many Python programmers use them every day, including all of the authors of this book—they are part of &quot;idiomatic Python&quot;. But programmers coming from other languages may have never seen them before. There are a lot of great tutorials just a web search away, so we won&#8217;t spend a long time discussing them now. Here is a quick explanation and example to get you started. A list comprehension looks like this: new_list = [f(o) for o in a_list if o&gt;0]. This would return every element of a_list that is greater than zero, after passing it to the function f. There are three parts here: the collection you are iterating over (a_list), an optional filter (if o&gt;0), and something to do to each element (f(o)). It&#8217;s not only shorter to write but way faster than the alternative ways of creating the same list with a loop. . We&#39;ll also check that one of the images looks okay. Since we now have tensors (which Jupyter by default will print as values), rather than PIL images (which Jupyter by default will display as an image), we need to use fastai&#39;s show_image function to display it: . show_image(three_tensors[1]); . For every pixel position, we want to compute the average over all the images of the intensity of that pixel. To do this we first combine all the images in this list into a single three-dimensional tensor. The most common way to describe such a tensor is to call it a rank-3 tensor. We often need to stack up individual tensors in a collection into a single tensor. Unsurprisingly, PyTorch comes with a function called stack. . Some operations in PyTorch, such as taking a mean, require us to cast our integer types to float types. Since we&#39;ll be needing this later, we&#39;ll also cast our stacked tensor to float now. Casting in PyTorch is as simple as typing the name of the type you wish to cast to, and treating it as a method. . Generally when images are floats, the pixels are expected to be be zero and one, so we will also divide by 255 here. . stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes = torch.stack(three_tensors).float()/255 stacked_threes.shape . torch.Size([6131, 28, 28]) . Perhaps the most important attribute of a tensor is its shape. This tells you the length of each axis. In this case, we can see that we have 6131 images, each of size 28 x 28 pixels. There is nothing specifically about this tensor that says that the first axis is the number of images, the second is the height, and the third is the width — the semantics of a tensor are entirely up to us, and how we construct it. As far as PyTorch is concerned, it is just a bunch of numbers in memory. . The length of a tensor&#39;s shape is its rank. . len(stacked_threes.shape) . 3 . . Important: it&#8217;s really important for you to commit to memory and practice these bits of tensor jargon: rank is the number of axes or dimensions in a tensor; shape is the size of each axis of a tensor. . You can also get a tensor&#39;s rank directly with ndim. . stacked_threes.ndim . 3 . Finally, we can compute what the ideal three looks like. We calculate the mean of all the image tensors, by taking the mean along dimension zero of our stacked, rank-3 tensor. This is the dimension which indexes over all the images. . In other words, for every pixel position, this will compute the average of that pixel over all images. So the result will be one value for every pixel position -- in other words, a single image. Here it is: . mean3 = stacked_threes.mean(0) show_image(mean3); . According to this dataset, this is the ideal number three! Let&#39;s do the same thing for the sevens, but let&#39;s put all the steps together at once to save some time: . mean7 = stacked_sevens.mean(0) show_image(mean7); . Let&#39;s now pick a &quot;3&quot;, and measure its distance from each of these &quot;ideal digits&quot;. . stop: How would you calculate how similar a particular image is from each of our ideal digits? Remember to step away from this book and jot down some ideas, before you move on! Research shows that recall and understanding improves dramatically when you are engaged with the learning process by solving problems, experimenting, and trying new ideas yourself Here&#39;s our sample &quot;3&quot;: . a_3 = stacked_threes[1] show_image(a_3); . We can&#39;t just add up the differences between the pixels of this image and the ideal digit. Why not?... . Because some differences will be positive, some will be negative, and these differences cancel out, resulting in a situation where an image which is too dark in some places and too light in others might be shown as having zero total differences from the ideal. That would be misleading! . To avoid this, there&#39;s two main ways data scientists measure distance in this context: . Take the mean of the absolute value of differences (absolute value is the function that replaces negative values with positive values). This is called the mean absolute difference or L1 norm | Take the mean of the square of differences (which makes everything positive) and then take the square root (which undoes the squaring). This is called the root mean squared error (RMSE) or L2 norm. . Important: in this book we generally assume that you have completed high school maths, and remember at least some of it... But everybody forgets some things! It all depends on what you happen to have had reason to practice in the meantime. Perhaps you have forgotten what a square root is, or exactly how they work. No problem! Any time you come across a maths concept that is not explained fully in this book, don&#8217;t just keep moving on, but instead stop and look it up. Make sure you understand the basic idea of what that maths concept is, how it works, and why we might be using it. One of the best places to refresh your understanding is Khan Academy. For instance, Khan Academy has a great introduction to square roots. | . Let&#39;s try both of these now: . dist_3_abs = (a_3 - mean3).abs().mean() dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt() dist_3_abs,dist_3_sqr . (tensor(0.1114), tensor(0.2021)) . dist_7_abs = (a_3 - mean7).abs().mean() dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt() dist_7_abs,dist_7_sqr . (tensor(0.1586), tensor(0.3021)) . In both cases, the distance between our 3 and the &quot;ideal&quot; 3 is less than the distance to the ideal 7. So our simple model will give the right prediction in this case. . s: Intuitively, the difference between L1 norm and mean squared error (MSE) is that the latter will penalize bigger mistakes more heavily than the former (and be more lenient with small mistakes). . PyTorch already provides both of these as loss functions. You&#39;ll find these inside torch.nn.functional, which the PyTorch team recommends importing as F (and is available by default under that name in fastai). Here MSE stands for mean squared error, and L1 refers to the standard mathematical jargon for mean absolute value (in math it&#39;s called the L1 norm). . F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt() . (tensor(0.1586), tensor(0.3021)) . j: When I first came across this &quot;L1&quot; thingie, I looked it up to see what on Earth it meant, found on Google that it is a vector norm using absolute value, so looked up vector norm and started reading: Given a vector space V over a field F of the real or complex numbers, a norm on V is a nonnegative-valued any function p: V → [0,+∞) with the following properties: For all a ∈ F and all u, v ∈ V, p(u + v) ≤ p(u) + p(v)... Then I stopped reading. &quot;Ugh, I&#39;ll never understand math!&quot; I thought, for the thousandth time. Since then I&#39;ve learned that every time these complex mathy bits of jargon come up in practice, it turns out I can replace them with a tiny bit of code! Like the L1 loss is just equal to (a-b).abs().mean(), where a and b are tensors. I guess mathy folks just think differently to me... I&#39;ll make sure, in this book, every time some mathy jargon comes up, I&#39;ll give you the little bit of code it&#39;s equal to as well, and explain in common sense terms what&#39;s going on. . In the above code we completed various mathematical operations on PyTorch tensors. If you&#39;ve done some numeric programming in Pytorch before, you may recognize these as being similar to Numpy arrays. Let&#39;s have a look at those two very important classes. . NumPy arrays and PyTorch tensors . Numpy is the most widely used library for scientific and numeric programming in Python, and provides very similar functionality and a very similar API to that provided by PyTorch; however, it does not support using the GPU, or calculating gradients, which are both critical for deep learning. Therefore, in this book we will generally use PyTorch tensors instead of NumPy arrays, where possible. . (Note that fastai adds some features to NumPy and PyTorch to make them a bit more similar to each other. If any code in this book doesn&#39;t work on your computer, it&#39;s possible that you forgot to include a line at the start of your notebook such as: from fastai.vision.all import *.) . But what are arrays and tensors, and why should you care? . A numpy array is multidimensional table of data, with all items of the same type. Since that can be any type at all, they could even be arrays of arrays, with the innermost arrays potentially being different sizes — this is called a &quot;jagged array&quot;. By &quot;multidimensional table&quot; we mean, for instance, a list (dimension of one), a table or matrix (dimension of two), a &quot;table of tables&quot; or a &quot;cube&quot; (dimension of three), and so forth. If the items are all of some simple type such as an integer or a float then numpy will store them as a compact C data structure in memory. This is where numpy shines. Numpy has a wide variety of operators and methods which can run computations on these compact structures at the same speed as optimized C, because they are written in optimized C. . In fact, arrays and tensors can finish computations many thousands of times faster than using pure Python. . A PyTorch tensor is nearly the same thing as a numpy array, but with an additional restriction which unlocks some additional capabilities. It&#39;s the same in that it, too, is a multidimensional table of data, with all items of the same type. However, the restriction is that a tensor cannot use just any old type — it has to use a single basic numeric type for all componentss. As a result, a tensor is not as flexible as a genuine array of arrays, which allows jagged arrays, where the inner arrays could have different sizes. So a PyTorch tensor cannot be jagged. It is always a regularly shaped multidimensional rectangular structure. . The vast majority of methods and operators supported by numpy on these structures are also supported by PyTorch. But PyTorch tensors have additional capabilities. One major capability is that these structures can live on the GPU, in which case their computation will be optimised for the GPU, and can run much faster. In addition, PyTorch can automatically calculate derivatives of these operations, including combinations of operations. As you&#39;ll see, it would be impossible to do deep learning in practice without this capability. . s: If you don&#39;t know what C is, do not worry as you won&#39;t need it at all. In a nutshell, it&#39;s a low-level (low-level means more similar to the language that computers use internally) language that is very fast compared to Python. To take advantage of its speed while programming in Python, try to avoid as much as possible writing loops and replace them by commands that work directly on arrays or tensors. Perhaps the most important new coding skill for a Python programmer to learn is how to effectively use the array/tensor APIs. We will be showing lots more tricks later in this book, but here&#39;s a summary of the key things you need to know for now. . To create an array or tensor, pass a list (or list of lists, or list of lists of lists, etc), to array() or tensor(): . data = [[1,2,3],[4,5,6]] arr = array (data) tns = tensor(data) . arr # numpy . array([[1, 2, 3], [4, 5, 6]]) . tns # pytorch . tensor([[1, 2, 3], [4, 5, 6]]) . All the operations below are shown on tensors - the syntax and results for NumPy arrays is idential. . You can select a row: . tns[1] . tensor([4, 5, 6]) . ...or a column, using : to indicate all of the first axis (we sometimes refer to the dimensions of tensors/arrays as axes): . tns[:,1] . tensor([2, 5]) . We can combine these, along with Python slice syntax ([start:end], end being excluded) . tns[1,1:3] . tensor([5, 6]) . We can use the standard operators: . tns+1 . tensor([[2, 3, 4], [5, 6, 7]]) . Tensors have a type: . tns.type() . &#39;torch.LongTensor&#39; . Tensors will automatically change from int to float if needed . tns*1.5 . tensor([[1.5000, 3.0000, 4.5000], [6.0000, 7.5000, 9.0000]]) . So, is our baseline model any good? To quantify this, we will use a metric. . Computing metrics using broadcasting . Recall that a metric is a number which is calculated from the predictions of our model, and the correct labels in our dataset, in order to tell us how good our model is. For instance, we could use either of the functions we saw in the previous section, mean squared error, or mean absolute error, and take the average of them over the whole dataset. However, neither of these are numbers that are very understandable to most people; in practice, we normally use accuracy as the metric for classification models. . As we&#39;ve discussed, we need to use a validation set to calculate our metric. That means we need to remove some of the data from training entirely, so it is not seen by the model at all. As it turns out, the creators of the MNIST dataset have already done this for us. Do you remember how there was a whole separate directory called &quot;valid&quot;? That&#39;s what this directory is for! . So to start with, let&#39;s create tensors for our threes and sevens from that directory. . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . Now we need a function that decides if a digit is a 3 or a 7. We need to know which of our &quot;ideal digits&quot; its closer to. First, we need a function that calculates the distance from a dataset to an ideal image. It turns out we can do that very simply, in this case calculating the mean absolute error: . def mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) mnist_distance(a_3, mean3) . tensor(0.1114) . Something very interesting happens when we run this function on the whole set of threes in the validation set: . valid_3_dist = mnist_distance(valid_3_tens, mean3) valid_3_dist, valid_3_dist.shape . (tensor([0.1050, 0.1526, 0.1186, ..., 0.1122, 0.1170, 0.1086]), torch.Size([1010])) . It&#39;s returned the distance for every single image, as a vector (i.e. a rank 1 tensor) of length 1010 (the number of threes in our validation set). How did that happen? Have a look again at our function mnist_distance, and you&#39;ll see we have there (a-b). . The magic trick is that PyTorch, when it sees two tensors of different ranks, will broadcast the tensor with the smaller rank to have the same size as the one with the larger rank. Broadcasting is an important capability that makes tensor code much easier to write. . Thanks to broadcasting, when PyTorch sees an operation on two tensors of the same rank, it completes the operation on each corresponding element of the two tensors, and returns the tensor result. For instance: . tensor([1,2,3]) + tensor([1,1,1]) . tensor([2, 3, 4]) . So in this case, PyTorch treats mean3, a rank 2 tensor representing a single image, as if it was 1010 copies of the same image, and then subtracts each of those copies from each &quot;three&quot; in our validation set. What shape would you expect this tensor to have? Try to figure it out yourself before you look at the answer below: . (valid_3_tens-mean3).shape . torch.Size([1010, 28, 28]) . We are calculating the difference between the &quot;ideal 3&quot; and each of 1010 threes in the validation set, for each of 28x28 images, resulting in the shape 1010,28,28. . There&#39;s a couple of really cool things to know about this operation we just did: . PyTorch doesn&#39;t actually copy mean3 1010 times. Instead, it just pretends as if it was a tensor of that shape, but doesn&#39;t actually allocate any additional memory | It does the whole calculation in C (or, if you&#39;re using a GPU, in CUDA, the equivalent of C on the GPU), tens of thousands of times faster than pure Python (up to millions of times faster on a GPU!) | . This is true of all broadcasting and elementwise operations and functions done in PyTorch. It&#39;s the most important technique for you to know to create efficient PyTorch code. . Next in mnist_distance we see abs(). You might be able to guess now what this does when applied to a tensor... It applies the method to each individual element in the tensor, and returns a tensor of the results (that is, it applies the method &quot;elementwise&quot;). So in this case, we&#39;ll get back 1010 absolute values. . Finally, our function calls mean((-1,-2)). In Python, -1 refers to the last element, and -2 refers to the second last. So in this case, this tells PyTorch that we want to take the mean of the last two axes of the tensor. After taking the mean over the last two axes, we are left with just the first axis, which is why our final size was (1010). . We&#39;ll be learning lots more about broadcasting throughout this book, especially in &lt;&gt;, and will be practising it regularly too.&lt;/p&gt; We can use this mnist_distance to figure out whether an image is a three or not by using the following logic: if the distance between the digit in question and the ideal 3 is less than the distance to the ideal 7, then it&#39;s a 3. This function will automatically do broadcasting and be applied elementwise, just like all PyTorch functions and operators. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; def is_3(x): return mnist_distance(x,mean3) &lt; mnist_distance(x,mean7) . Let&#39;s test it on our example case (note also that when we convert the boolean response to a float, we get a 1.0 for true and 0.0 for false): . is_3(a_3), is_3(a_3).float() . (tensor(True), tensor(1.)) . And testing it on the full validation set of threes: . is_3(valid_3_tens) . tensor([True, True, True, ..., True, True, True]) . Now we can calculate the accuracy for each of threes and sevens, by taking the average of that function for all threes, and it&#39;s inverse for all sevens: . accuracy_3s = is_3(valid_3_tens).float() .mean() accuracy_7s = (1 - is_3(valid_7_tens).float()).mean() accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2 . (tensor(0.9168), tensor(0.9854), tensor(0.9511)) . This looks like a pretty good start! We&#39;re getting over 90% accuracy on both threes and sevens. . But let&#39;s be honest: threes and sevens are very different looking digits. And we&#39;re only classifying two out of the ten possible digits so far. So we&#39;re going to need to do better! . To do better, perhaps it is time to try a system that doess some learning -- that is, that can automatically modify itself to improve its performance. In other words, it&#39;s time to talk about the training process, and SGD. . Stochastic Gradient descent (SGD) . Do you remember the way that Arthur Samuel described machine learning, which we quoted in &lt;&gt;:&lt;/p&gt; : Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programed would &quot;learn&quot; from its experience. As we discussed, this is the key to allowing us to have something which can get better and better — to learn. But our pixel similarity approach does not really do this. We do not have any kind of weight assignment, or any way of improving based on testing the effectiveness of a weight assignment. In other words, we can&#39;t really improve our pixel similarity approach by modifying a set of parameters (which will be the SGD part, as we will see). In order to take advantage of the power of deep learning, we will first have to represent our task in the way that Arthur Samuel described it. . Instead of trying to find the similarity between an image and a &quot;ideal image&quot; we could instead look at each individual pixel, and come up with a set of weights for each pixel, such that the highest weights are associated with those pixels most likely to be black for a particular category. For instance, pixels towards the bottom right are not very likely to be activated for a seven, so they should have a low weight for a seven, but are more likely to be activated for an eight, so they should have a high weight for an eight. This can be represented as a function for each possible category, for instance the probability of being the number eight: . def pr_eight(x,w) = (x*w).sum() . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Here we are assuming that X is the image, represented as a vector -- in other words, with all of the rows stacked up end to end into a single long line. And we are assuming that the weights are a vector W. If we have this function, then we just need some way to update the weights to make them a little bit better. With such an approach, we can repeat that step a number of times, making the weights better and better, until they are as good as we can make them. . We want to find the specific values for the vector W which causes our function to be high for those images that are actually an eight, and low for those images which are not. Searching for the best vector W is a way to search for the best function for recognising eights. (Because we are not yet using a deep neural network, we are limited by what our function can actually do — we are going to fix that constraint later in this chapter.) . To be more specific, here are the steps that we are going to require, to turn this function into a machine learning classifier: . Initialize the weights | For each image, use these weights to predict whether it appears to be a three or a seven | Based on these predictions, calculate how good the model is (its loss) | Calculate the gradient, which measures for each weight, how changing that weight would change the loss | Step (that is, change) all weights based on that calculation | Go back to the second step, and repeat the process | ...until you decide to stop the training process (for instance because the model is good enough, or you don&#39;t want to wait any longer) | &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G init init predict predict init&#45;&gt;predict loss loss predict&#45;&gt;loss gradient gradient loss&#45;&gt;gradient step step gradient&#45;&gt;step step&#45;&gt;predict repeat stop stop step&#45;&gt;stop These seven steps, illustrated in &lt;&gt; are the key to the training of all deep learning models， and we&#39;ll be using the seven terms in the above diagram throughout this book. That deep learning turns out to rely entirely on these steps is extremely surprising and counter-intuitive. It&#39;s amazing that this process can solve such complex problems. But, as you&#39;ll see, it really does!&lt;/p&gt; There are many different ways to do each of these seven steps, and we will be learning about them throughout the rest of this book. These are the details which make a big difference for deep learning practitioners. But it turns out that the general approach to each one generally follows some basic principles: . Initialize:: we initialise the parameters to random values. This may sound surprising. There are certainly other choices we could make, such as initialising them to the percentage of times that that pixel is activated for that category. But since we already know that we have a routine to improve these weights, it turns out that just starting with random weights works perfectly well | Loss:: This is the thing Arthur Samuel refered to: &quot;testing the effectiveness of any current weight assignment in terms of actual performance&quot;. We need some function that will return a number that is small if the performance of the model is good (the standard approach is to treat a small loss as good, and a large loss as bad, although this is just a convention) | Step:: A simple way to figure out whether a weight should be increased a bit, or decreased a bit, would be just to try it. Increase the weight by a small amount, and see if the loss goes up or down. Once you find the correct direction, you could then change that amount by a bit more, and a bit less, until you find an amount which works well. However, this is slow! As we will see, the magic of calculus allows us to directly figure out which direction, and roughly how much, to change each weight, without having to try all these small changes, by calculating gradients. This is just a performance optimisation, we would get exactly the same results by using the slower manual process as well | Stop:: We have already discussed how to choose how many epochs to train a model for. This is where that decision is applied. For our digit classifier, we would keep training until the accuracy of the model started getting worse, or we ran out of time. | . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Before applying these steps to our image classification problem, let&#39;s illustrate what they look like in a simpler case. First we will define a very simple function, the quadratic — let&#39;s pretend that this is our loss function, and x is a weight parameter of the function: . def f(x): return x**2 . Here is a graph of that function: . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) . The sequence of steps we described above starts by picking some random value for a parameter, and calculating the value of the loss: . plot_function(f, &#39;x&#39;, &#39;x**2&#39;) plt.scatter(-1.5, f(-1.5), color=&#39;red&#39;); . Now we look to see what would happen if we increased or decreased our parameter by a little bit — the adjustment. This is simply the slope at a particular point: . . We can change our weight by a little in the direction of the slop, calculate our loss and adjustment again, and repeat this a few times. Eventually, we will get to the lowest point on our curve: . . This basic idea goes all the way back to Isaac Newton, who pointed out that we can optimise arbitrary functions in this way. Regardless of how complicated our functions become, this basic approach of gradient descent will not significantly change. The only minor changes we will see later in this book are some handy ways we can make it faster, by finding better steps. . The gradient . The one magic step is the bit where we calculate the gradients. As we mentioned, we can use calculus as a performance optimization; it allows us to more quickly calculate whether our loss will go up or down when we adjust our parameters up or down. In other words, the gradients will tell us how much we have to change each weight to make our model better. . Perhaps you remember back to your high school calculus class: the derivative of a function tells you how much a change in the parameters of a function will change its result. Don&#39;t worry, lots of us forget our calculus once high school is behind us! But you will have to have some intuitive understanding of what a derivative is before you continue, so if this is all very fuzzy in your head, head over to Khan Academy and complete the lessons on basic derivatives. You won&#39;t have to know how to calculate them yourselves, you just have to know what a derivative is. . The key point about a derivative is this: for any function, such as the quadratic function we saw before, we can calculate its derivative. The derivative is another function. It calculates the change, rather than the value. For instance, the derivative of the quadratic function at the value three tells us how rapidly the function changes at the value three. More specifically, you may remember from high school that gradient is defined as &quot;rise/run&quot;, that is, the change in the value of the function, divided by the change in the value of the parameter. When we know how our function will change, then we know what we need to do to make it smaller. This is the key to machine learning: having a way to change the parameters of a function to make it smaller. Calculus provides us with a computational shortcut, the derivative, which lets us directly calculate the gradient of our functions. . One important thing to be aware of: our function has lots of weights that we need to adjust, so when we calculate the derivative we won&#39;t get back one number, but lots of them — a gradient for every weight. But there is nothing mathematically tricky here; you can calculate the derivative with respect to one weight, and treat all the other ones as constant. Then repeat that for each weight. This is how all of the gradients are calculated, for every weight. . We mentioned just now that you won&#39;t have to calculate any gradients yourselves. How can that be? Amazingly enough, PyTorch is able to automatically compute the derivative of nearly any function! What&#39;s more, it does it very fast. Most of the time, it will be at least as fast as any derivative function that you can create by hand. Let&#39;s see an example. . First, pick a tensor value which we want gradients at: . xt = tensor(3.).requires_grad_() . Notice the special method requires_grad_? That&#39;s the magical incantation we use to tell PyTorch that we want to calculate gradients with respect to that variable at that value. It is essentially tagging the variable, so PyTorch will remember to keep track of how to compute gradients of the other, direct calculations on it which you will ask for. . a: This API might throw you if you&#39;re coming from math or physics. In those contexts the &quot;gradient&quot; of a function is just another function (i.e., its derivative), so you might expect gradient-related API to give you a new function. But in deep learning, &quot;gradients&quot; usually means the value of a function&#39;s derivative at a particular argument value. PyTorch API also puts the focus on that argument, not the function you&#39;re actually computing gradients of. It may feel backwards at first but it&#39;s just a different perspective. Now we calculate our function with that value. Notice how PyTorch prints not just the value calculated, but also a note that it has a gradient function it&#39;ll be using to calculate our gradient when needed: . yt = f(xt) yt . tensor(9., grad_fn=&lt;PowBackward0&gt;) . Finally, we tell PyTorch to calculate the gradients for us: . yt.backward() . The &quot;backward&quot; here refers to &quot;back propagation&quot;, which is the name given to the process of calculating the derivative of each layer (we&#39;ll see how this is done exactly in chapter , when we calculate the gradients of a deep neural net from scratch). This is called the &quot;backward pass&quot; of the network, as opposed to the &quot;forward pass&quot;, which is where the activations are calculated. Life would probably be easier if backward was just called calculate_grad, but deep learning folks really do like to add jargon everywhere they can!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; We can now view the gradients by checking the grad attribute of our tensor: . xt.grad . tensor(6.) . If you remember your high school calculus rules, the derivative of x**2 is 2*x, and we have x=3, so the gradient should be 2*3=6, which is what PyTorch calculated for us! . Now we&#39;ll repeat the above steps, but with a vector argument for our function: . xt = tensor([3.,4.,10.]).requires_grad_() xt . tensor([ 3., 4., 10.], requires_grad=True) . ...and adding sum() to our function so it can take a vector (i.e. a rank-1 tensor), and return a scalar (i.e. a rank-0 tensor): . def f(x): return (x**2).sum() yt = f(xt) yt . tensor(125., grad_fn=&lt;SumBackward0&gt;) . Our gradients are 2*xt, as we&#39;d expect! . yt.backward() xt.grad . tensor([ 6., 8., 20.]) . The gradient only tells us the slope of our function, it doesn&#39;t actually tell us how far to adjust the parameters. It gives us some idea of how far to adjust them; if the slope is very large, then that may suggest that we have more adjustments to do, whereas if the slope is very small, that may suggest that we are close to the optimal value. . Stepping with a learning rate . Deciding how to change our parameters based on the value of the gradients is an important part of the deep learning process. Nearly all approaches start with the basic idea of multiplying the gradient by some small number, called the learning rate (LR). The learning rate is often a number between 0.001 and 0.1, although it could be anything. Often, people select a learning rate just by trying a few, and finding which results in the best model after training (we&#39;ll show you a better approach later in this book, called the learning rate finder). Once you&#39;ve picked a learning rate, you can adjust your parameters using this simple function: . w -= gradient(w) * lr . This is known as stepping your parameters, using a optimiser step. . If you pick a learning rate that&#39;s too low, it can mean having to do for a lot of steps. &lt;&gt; illustrates that.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Gradient descent with low LR . Although picking a learning rate that&#39;s too high is even worse--it can actually result in the loss getting worse as we see in &lt;&gt;!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Gradient descent with high LR . If the learning rate is too high, it may also &quot;bounce&quot; around, rather than actually diverging; &lt;&gt; shows how this has the result of taking many steps to train successfully.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Gradient descent with bouncy LR . Now let&#39;s apply all of this on an end-to-end example. . An end-to-end SGD example . To understand SGD, it might be easiest to start with a simple, synthetic, example. Let&#39;s imagine you were measuring the speed of a roller coaster as it went over the top of a hump. It would start fast, and then get slower as it went up the hill, and then would be slowest at the top, and it would then speed up again as it goes downhill. If you&#39;re measuring the speed manually every second for 20 seconds, it might look something like this: . time = torch.arange(0,20).float(); time . tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]) . speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1 plt.scatter(time,speed); . We&#39;ve added a bit of random noise, since measuring things manually isn&#39;t precise. This means it&#39;s not that easy to answer the question: what was the roller coaster&#39;s lowest speed? Using SGD we can try to find a function that matches our observations. We can&#39;t consider every possible function, so let&#39;s use a guess that it will be quadratic, i.e. a function of the form a*(time**2)+(b*time)+c. . We want to distinguish clearly between the function&#39;s input (the time when we are measuring the coaster&#39;s speed) and its parameters (the values that define which quadratic we&#39;re trying). So let us collect the parameters in one argument and separate the input, t, and the parameters, params, in the function&#39;s signature: . def f(t, params): a,b,c = params return a*(t**2) + (b*t) + c . In other words, we&#39;ve restricted the problem of finding the best imaginable function that fits the data, to finding the best quadratic function. This greatly simplifies the problem, since every quadratic function is fully defined by the three parameters a, b, and c. So to find the best quadratic function, we only need to find the best values for a, b, and c. . If we can solve this problem for the three parameters of a quadratic function, we&#39;ll be able to apply the same approach for other, more complex functions with more parameters--such as a neural net. So let&#39;s find the parameters for f first, and then we&#39;ll come back and do the same thing for the MNIST dataset with a neural net. . We need to define first what we mean by &quot;best&quot;. We define this precisely by choosing a loss function, which will return a value based on a prediction and a target, where lower values of the function correspond to &quot;better&quot; predictions. For continuous data, it&#39;s common to use mean squared error: . def mse(preds, targets): return ((preds-targets)**2).mean() . Now, let&#39;s work through our 7 step process. . Step 1--Initialize the parameters to random values, and tell PyTorch that we want to track their gradients, using requires_grad_: . params = torch.randn(3).requires_grad_() . Step 2--Calculate the predictions: . preds = f(time, params) . Let&#39;s create a little function to see how close our predictions are to our targets, and take a look: . def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, to_np(preds), color=&#39;red&#39;) ax.set_ylim(-300,100) . show_preds(preds) . This doesn&#39;t look very close--our random parameters suggest that the roller coaster will end up going backwards, since we have negative speeds! . Step 3--Calculate the loss: . loss = mse(preds, speed) loss . tensor(25823.8086, grad_fn=&lt;MeanBackward0&gt;) . Our goal is now to improve this. To do that, we&#39;ll need to know the gradients. . Step 4--Calculate the gradients. In other words, calculate an approximation of how the parameters need to change. . loss.backward() params.grad . tensor([-53195.8594, -3419.7146, -253.8908]) . params.grad * 1e-5 . tensor([-0.5320, -0.0342, -0.0025]) . We can use these gradients to improve our parameters. We&#39;ll need to pick a learning rate (we&#39;ll discuss how to do that in practice in the next chapter; for now we&#39;ll just pick 1.0): . params . tensor([-0.7658, -0.7506, 1.3525], requires_grad=True) . Step 5--Step the weights. In other words, update the parameters based on the gradients we just calculated. . lr = 1e-5 params.data -= lr * params.grad.data params.grad = None . Let&#39;s see if the loss has improved: . preds = f(time,params) mse(preds, speed) . tensor(5435.5366, grad_fn=&lt;MeanBackward0&gt;) . ...and take a look at the plot: . show_preds(preds) . We need to repeat this a few times, so we&#39;ll create a function to apply one step: . def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . ...now we&#39;re ready for step 6! . Step 6--Repeat the process. By looping and performing many improvements, we hope to reach a good result. . for i in range(10): apply_step(params) . 5435.53662109375 1577.4495849609375 847.3780517578125 709.22265625 683.0757446289062 678.12451171875 677.1839599609375 677.0025024414062 676.96435546875 676.9537353515625 . Looking only at these loss numbers disguises the fact that each iteration represents an entirely different quadratic function being tried, on the way to find the best possible quadratic function. We can see this process visually if, instead of printing out the loss function, we plot the function at every step. Then we can see how the shape is approaching the best possible quadratic function for our data: . _,axs = plt.subplots(1,4,figsize=(12,3)) for ax in axs: show_preds(apply_step(params, False), ax) plt.tight_layout() . Step 7 is to stop. We just decided to stop after 10 epochs arbitrarily. In practice, we watch the training and validation losses and our metrics to decide when to stop, as we&#39;ve discussed. . Summarizing gradient descent . To summarize, at the beginning, the weights of our model can be random (training from scratch) or come from a pretrained model (transfer learning). In the first case, the output we will get from our inputs won&#39;t have anything to do with what we want, and even in the second case, it&#39;s very likely the pretrained model won&#39;t be very good at the specific task we are targeting. So the model will need to learn better weights. . To do this, we will compare the outputs the model gives us with our targets (we have labelled data, so we know what result the model should give) using a loss function, which returns a number that needs to be as low as possible. Our weights need to be improved. To do this, we take a few data items (such as images) that we feed to our model. After going through our model, we compare to the corresponding targets using our loss function. The score we get tells us how wrong our predictions were, and we will change the weights a little bit to make it slightly better. . To find how to change the weights to make the loss a bit better, we use calculus to calculate the gradient. (Actually, we let PyTorch do it for us!) Let&#39;s imagine you are lost in the mountains with your car parked at the lowest point. To find your way, you might wander in a random direction but that probably won&#39;t help much. Since you know your vehicle is at the lowest point, you would be better to go downhill. By always taking a step in the direction of the steepest downward slope, you should eventually arrive at your destination. We use the magnitude of the gradient (i.e., the steepness of the slope) to tell us how big a step to take; specifically, we multiply the gradient by a number we choose called the learning rate to decide on the step size. . MNIST loss function . Let&#39;s get back to our MNIST problem. As we&#39;ve seen, we need gradients in order to improve our model, and in order to calculate gradients we need some loss function that represents how good our model is. That is because the gradients are a measure of how that loss function changes with small tweaks to the weights. . The obvious approach would be to use the accuracy as our loss function. In this case, we would calculate our prediction for each image, and then calculate the overall accuracy (remember, at first we simply use random weights), and then calculate the gradients of each weight with respect to that accuracy calculation. . Unfortunately, we have a significant technical problem here. The gradient of a function is its slope, or its steepness, which can be defined as rise over run -- that is, how much the value of function goes up or down, divided by how much you changed the input. We can write this in maths: (y_new-y_old) / (x_new-x_old). Specifically, it is defined when x_new is very similar to x_old, meaning that their difference is very small. But accuracy only changes at all when a prediction changes from a 3 to a 7, or vice versa. So the problem is that a small change in weights from from x_old to x_new isn&#39;t likely to cause any prediction to change, so (y_new - y_old) will be zero. In other words, the gradient is zero almost everywhere. . As a result, a very small change in the value of a weight will often not actually change the accuracy at all. This means it is not useful to use accuracy as a loss function. When we use accuracy as a loss function, most of the time our gradients will actually be zero, and the model will not be able to learn from that number. That is not much use at all! . s: In mathematical terms, accuracy is a function that is constant almost everywhere (except at the threshold, 0.5) so its derivative is nil almost everywhere (and infinity at the threshold). This then gives gradients that are zero or infinite, so useless to do an update of gradient descent. Instead, we want a loss function which, when our weights result in slightly better predictions, gives us a slightly better loss. So what does a &quot;slightly better prediction&quot; look like, exactly? Well, in this case, it means that, if the correct answer is a 3, then the score is a little higher, or if the correct answer is a 7, then the score is a little lower. Here is a simple implementation of just such a function, assuming that inputs are numbers between zero and one: . def mnist_loss(inputs, targets): return torch.where(targets==1, 1-inputs, inputs).mean() . Here, we&#39;re assuming that targets contains 1 for any digit which is meant to be a three, and 0 otherwise. Let&#39;s look at an example: . tgt = tensor([1,0,1]) inp = tensor([0.9, 0.4, 0.2]) . torch.where(a,b,c) is the same as running the list comprehension [b[i] if a[i] else c[i] for i in range(len(a))], except it works on tensors, at C/CUDA speed. . Note: It&#8217;s important to learn about PyTorch functions like this, because looping over tensors in Python performs at Python speed, not C/CUDA speed! Try running help(torch.where) now to read the docs for this function, or, better still, look it up on the PyTorch documentation site. . torch.where(tgt==1, 1-inp, inp) . tensor([0.1000, 0.4000, 0.8000]) . You can see that this function will return a lower number if the predictions are more accurate, and more confident for accurate predictions (higher absolute values) and less confident for inaccurate predictions. In PyTorch, we always assume that a lower value of a loss function is better. . mnist_loss(inp,tgt) . tensor(0.4333) . For instance, if we change our prediction for the one &quot;false&quot; target from 0.2 to 0.8 the loss will go down, indicating that this is a better prediction. . mnist_loss(tensor([0.9, 0.4, 0.8]),tgt) . tensor(0.2333) . One problem with mnist_loss as currently defined is that it assumes that inputs are always between zero and one. We need to ensure, then, that this is actually the case! As it happens, there is a function that does exactly that--it always outputs a number between zero and one and it&#39;s called sigmoid. . Sigmoid . The function called sigmoid is defined by: . def sigmoid(x): return 1/(1+torch.exp(-x)) . Pytorch actually already defines this for us, so we don’t really need our own version. This is an important function in deep learning, since we often want to ensure values between zero and one. This is what it looks like: . plot_function(torch.sigmoid, title=&#39;Sigmoid&#39;, min=-4, max=4) . Let&#39;s update mnist_loss to first apply sigmoid to the inputs: . def mnist_loss(inputs, targets): inputs = inputs.sigmoid() return torch.where(targets==1, 1-inputs, inputs).mean() . We now have two terms which are somewhat similar: loss and metric. They are similar because they are both measures of how well your model is performing. The key difference, though, is that the loss must be a function which has a meaningful derivative. It can&#39;t have big flat sections, and large jumps, but instead must be reasonably smooth. Therefore, sometimes it does not really reflect exactly what we are trying to achieve, but is something that is a compromise between our real goal, and a function that can be optimised using its gradient. The loss function is calculated for each item in our dataset, and then at the end of an epoch these are all averaged, and the overall mean loss is reported for the epoch. . Metrics, on the other hand, are the numbers that we really care about. These are the things which are printed at the end of each epoch, and tell us how our model is really doing. It is important that we learn to focus on these metrics, rather than the loss, when judging the performance of a model. . SGD and mini-batches . In order to take an optimiser step we need to calculate the loss over one or more data items. We could calculate it for the whole dataset, and take the average, or we could calculate it for a single data item. But neither of these sounds ideal — calculating it for the whole dataset would take a very long time, but calculating it for a single item would result in a very imprecise and unstable gradient. So instead we take a compromise between the two: we calculate the average loss for a few data items at a time. This is called a mini-batch. The number of data items in the mini batch is called the batch size. A larger batch size means that you will get a more accurate and stable estimate of your datasets gradient on the loss function, but it will take longer, and you will get less mini-batches per epoch. Choosing a good batch size is one of the decisions you need to make as a deep learning practitioner to train your model quickly and accurately. We will talk about how to make this choice throughout this book. . Another good reason for using mini-batches rather than calculating the gradient on individual data items is that, in practice, we nearly always do our training on an accelerator such as a GPU. These accelerators only perform well if they have lots of work to do at a time. So it is helpful if we can give them lots of data items to work on at a time. Using mini-batches is one of the best ways to do this. (Although if you give them too much data to work on at once, they run out of memory--making GPUs happy is tricky!) . As we&#39;ve seen, in the discussion of data augmentation, we get better generalisation if we can very things during training. A simple and effective thing we can vary during training is what data items we put in each mini batch. Rather than simply enumerating our data set in order for every epoch, instead what we normally do in practice is to randomly shuffle it on every epoch, before we create mini batches. PyTorch and fastai provide a class that will do the shuffling and mini batch collation for you, called DataLoader. . A DataLoader can take any Python collection, and turn it into an iterator over many batches, like so: . coll = range(15) dl = DataLoader(coll, batch_size=5, shuffle=True) list(dl) . [tensor([9, 3, 6, 8, 0]), tensor([13, 1, 14, 4, 12]), tensor([ 7, 11, 2, 5, 10])] . For training a model, we don&#39;t just want any Python collection, but a collection containing independent and dependent variables. A collection that contains tuples of independent and dependent variables is known in PyTorch as a Dataset. Here&#39;s an example of an extremely simple Dataset: . ds = L(enumerate(string.ascii_lowercase)) ds . (#26) [(0, &#39;a&#39;),(1, &#39;b&#39;),(2, &#39;c&#39;),(3, &#39;d&#39;),(4, &#39;e&#39;),(5, &#39;f&#39;),(6, &#39;g&#39;),(7, &#39;h&#39;),(8, &#39;i&#39;),(9, &#39;j&#39;)...] . When we pass a Dataset to a DataLoader we will get back many batches which are themselves tuples of independent and dependent variable many batches: . dl = DataLoader(ds, batch_size=6, shuffle=True) list(dl) . [(tensor([ 7, 19, 17, 13, 25, 15]), (&#39;h&#39;, &#39;t&#39;, &#39;r&#39;, &#39;n&#39;, &#39;z&#39;, &#39;p&#39;)), (tensor([11, 9, 23, 21, 3, 16]), (&#39;l&#39;, &#39;j&#39;, &#39;x&#39;, &#39;v&#39;, &#39;d&#39;, &#39;q&#39;)), (tensor([12, 2, 18, 22, 14, 24]), (&#39;m&#39;, &#39;c&#39;, &#39;s&#39;, &#39;w&#39;, &#39;o&#39;, &#39;y&#39;)), (tensor([ 1, 0, 20, 4, 6, 10]), (&#39;b&#39;, &#39;a&#39;, &#39;u&#39;, &#39;e&#39;, &#39;g&#39;, &#39;k&#39;)), (tensor([8, 5]), (&#39;i&#39;, &#39;f&#39;))] . We are now read to write our first training loop for a model using SGD! . Putting it all together . it&#39;s time to implement the graph we saw in &lt;&gt;. In code, our process will be implemented something like this for each epoch:&lt;/p&gt; for x,y in dl: pred = model(x) loss = loss_func(pred, y) loss.backward() parameters -= parameters.grad * lr . We already have our xs--that&#39;s the images themselves. We&#39;ll concatenate them all into a single tensor, and also change them from a list of matrices (a rank 3 tensor) to a list of vectors (a rank 2 tensor). We can do this using view, which is a PyTorch method that changes the shape of a tensor without changing its contents. -1 is a special parameter to view. It means: make this axis as big as necessary to fit all the data. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28) . We need a label for each. We&#39;ll use 1 for threes and 0 for sevens: . train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) train_x.shape,train_y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . A Dataset in PyTorch is required to return a tuple of (x,y) when indexed. Python provides a zip function which, when combined with list, provides a simple way to get this functionality: . dset = list(zip(train_x,train_y)) x,y = dset[0] x.shape,y . (torch.Size([784]), tensor([1])) . This is enough to allow us to create a DataLoader: . dl = DataLoader(dset, batch_size=256) xb,yb = first(dl) xb.shape,yb.shape . (torch.Size([256, 784]), torch.Size([256, 1])) . We&#39;ll do the same for the validation set: . valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) valid_dl = DataLoader(valid_dset, batch_size=256) . Now we need an (initially random) weight for every pixel (this is the initialize step in our 7-step process): . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() . weights = init_params((28*28,1)) . The function weights*pixels won&#39;t be flexible enough--it is always equal to zero when the pixels are equal to zero (i.e. it&#39;s intercept is zero). You might remember from high school math that the formula for a line is y=w*x+b; we still need the b. We&#39;ll initialize it to a random number too: . bias = init_params(1) . In neural networks, the w in the equation y=w*x+b is called the weights, and the b is called the bias. Together, the weights and bias make up the parameters. . jargon: Parameters: The weights and biases of a model. The weights are the w in the equation w*x+b, and the biases are the b in that equation. . We can now calculate a prediction for one image: . (train_x[0]*weights.T).sum() + bias . tensor([4.5118], grad_fn=&lt;AddBackward0&gt;) . We need a way to do this for all the images in a mini-batch. Let&#39;s create a mini-batch of size 4 for testing: . batch = train_x[:4] batch.shape . torch.Size([4, 784]) . Whilst we could use a python for loop to calculate the prediction for each image, that would be very slow. Because Python loops don&#39;t run on the GPU, and because Python is a slow language for loops in general, we need to represent as much of the computation in a model as possible using higher-level functions. . In this case, there&#39;s an extremely convenient mathematical operation that calculates w*x for every row of a matrix--it&#39;s called matrix multiplication. &lt;&gt; show what matrix multiplication looks like (diagram from Wikipedia).&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Matrix multiplication . This image shows two matrices, A and B being multiplied together. Each item of the result, which we&#39;ll call AB, contains each item of its corresponding row of A multiplied by each item of its corresponding column of B, added together. For instance, row 1 column 2 (the orange dot with a red border) is calculated as $a_{1,1} * b_{1,2} + a_{1,2} * b_{2,2}$. If you need a refresher on matrix multiplication, we suggest you take a look at the great Introduction to Matrix Multiplication on Khan Academy, since this is the most important mathematical operation in deep learning. . In Python, matrix multiplication is represented with the @ operator. Let&#39;s try it: . def linear1(xb): return xb@weights + bias preds = linear1(batch) preds . tensor([[ 4.5118], [ 3.6536], [11.2975], [14.1164]], grad_fn=&lt;AddBackward0&gt;) . The first element is the same as we calculated before, as we&#39;d expect. This equation, batch@weights + bias, is one of the two fundamental equations of any neural network (the other one is the activation function, which we&#39;ll see in a moment). . The mnist_loss function we wrote earlier already works on a mini-batch, thanks to the magic of broadcasting! Here&#39;s the loss for our mini-batch: . loss = mnist_loss(preds, train_y[:4]) loss . tensor(0.0090, grad_fn=&lt;MeanBackward0&gt;) . Now we can calculate the gradients: . loss.backward() weights.grad.shape,weights.grad.mean(),bias.grad . (torch.Size([784, 1]), tensor(-0.0013), tensor([-0.0088])) . Let&#39;s put that all in a function: . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . ...and test it: . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-0.0025), tensor([-0.0177])) . But look what happens if we call it twice: . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-0.0038), tensor([-0.0265])) . The gradients have changed! The reason for this is that loss.backward actually adds the gradients of loss to any gradients that are currently stored. So we have to set the current gradients to zero first. . weights.grad.zero_() bias.grad.zero_(); . . Note: Methods in PyTorch that end in an underscore modify their object in-place. For instance, bias.zero_() sets all elements of the tensor bias to zero. . Our only remaining step will be to update the weights and bias based on the gradient and learning rate. When we do so, we have to tell PyTorch not to take the gradient of this step too, otherwise things will get very confusing when we try to compute the derivative at the next batch! If we assign to the data attribute of a tensor then PyTorch will not take the gradient of that step. Here&#39;s our basic training loop for an epoch: . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . We also want to know how we&#39;re doing, by looking at the accuracy of the validation set. To decide if an output represents a 3 or a 7, we can just check whether it&#39;s greater than zero. So our accuracy for each item can be calculated (using broadcasting, so no loops!) with: . (preds&gt;0.0).float() == train_y[:4] . tensor([[True], [True], [True], [True]]) . That gives us this function to calculate our validation accuracy: . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() . We can check it works: . batch_accuracy(linear1(batch), train_y[:4]) . tensor(1.) . ...and then putting the batches together: . def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . validate_epoch(linear1) . 0.4403 . That&#39;s our starting point. Let&#39;s train for one epoch, and see if the accuracy improves: . lr = 1. params = weights,bias train_epoch(linear1, lr, params) validate_epoch(linear1) . 0.4992 . for i in range(20): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.6772 0.8081 0.914 0.9453 0.9565 0.9619 0.9624 0.9633 0.9658 0.9677 0.9702 0.9716 0.9721 0.9736 0.9741 0.9745 0.9765 0.977 0.977 0.9765 . Looking good! We&#39;re already about at the same accuracy as our &quot;pixel similarity&quot; approach, and we&#39;ve created a general purpose foundation we can build on. Our next step will be to create an object that will handle the SGD step for us. In PyTorch, it&#39;s called an optimizer. . Creating an optimizer . Because this is such a general foundation, PyTorch provides some useful classes to make it easier to implement. The first we&#39;ll use is to replace our linear() function with PyTorch&#39;s nn.Linear module. A &quot;module&quot; is an object of a class that inherits from the PyTorch nn.Module class. Objects of this class behave identically to a standard Python function, in that you can call it using parentheses, and it will return the activations of a model. . nn.Linear does the same thing as our init_params and linear together. It contains both the weights and bias in a single class. Here&#39;s how we replicate our model from the previous section: . linear_model = nn.Linear(28*28,1) . Every PyTorch module knows what parameters it has that can be trained; they are available through the parameters method: . w,b = linear_model.parameters() w.shape,b.shape . (torch.Size([1, 784]), torch.Size([1])) . We can use this information to create an optimizer: . class BasicOptim: def __init__(self,params,lr): self.params,self.lr = list(params),lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr def zero_grad(self, *args, **kwargs): for p in self.params: p.grad = None . We can create our optimizer by passing in the model&#39;s parameters: . opt = BasicOptim(linear_model.parameters(), lr) . Our training loop can now be simplified to: . def train_epoch(model): for xb,yb in dl: calc_grad(xb, yb, model) opt.step() opt.zero_grad() . Our validation function doesn&#39;t need to change at all: . validate_epoch(linear_model) . 0.6714 . Let&#39;s put our little training loop in a function, to make things simpler: . def train_model(model, epochs): for i in range(epochs): train_epoch(model) print(validate_epoch(model), end=&#39; &#39;) . The results are the same as the previous section. . train_model(linear_model, 20) . 0.4932 0.7935 0.8477 0.9165 0.9346 0.9482 0.956 0.9634 0.9658 0.9673 0.9702 0.9717 0.9731 0.9751 0.9756 0.9765 0.9775 0.978 0.9785 0.9785 . fastai provides the SGD class which, by default, does the same thing as our BasicOptim: . linear_model = nn.Linear(28*28,1) opt = SGD(linear_model.parameters(), lr) train_model(linear_model, 20) . 0.4932 0.771 0.8594 0.918 0.9355 0.9492 0.9575 0.9634 0.9658 0.9682 0.9692 0.9717 0.9731 0.9751 0.9756 0.977 0.977 0.9785 0.9785 0.9785 . fastai also provides Learner.fit, which we can use instead of train_model. To create a Learner we first need to create DataLoaders, by passing in our training and validation DataLoaders: . dls = DataLoaders(dl, valid_dl) . To create a Learner without using an application (such as cnn_learner) we need to pass in all the information that we&#39;ve created in this chapter: the DataLoaders, the model, the optimization function (which will be passed the parameters), the loss function, and optionally any metrics to print: . learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . Now we can call fit: . learn.fit(10, lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.636918 | 0.503445 | 0.495584 | 00:00 | . 1 | 0.500283 | 0.192597 | 0.839549 | 00:00 | . 2 | 0.184349 | 0.182295 | 0.833660 | 00:00 | . 3 | 0.081278 | 0.107260 | 0.912169 | 00:00 | . 4 | 0.043316 | 0.078320 | 0.932777 | 00:00 | . 5 | 0.028503 | 0.062712 | 0.946025 | 00:00 | . 6 | 0.022414 | 0.052999 | 0.955348 | 00:00 | . 7 | 0.019704 | 0.046531 | 0.962218 | 00:00 | . 8 | 0.018323 | 0.041979 | 0.965653 | 00:00 | . 9 | 0.017486 | 0.038622 | 0.966634 | 00:00 | . As you can see, there&#39;s nothing magic about the PyTorch and fastai classes. They are just convenient pre-packaged pieces that make your life a bit easier! (They also provide a lot of extra functionality we&#39;ll be using in future chapters.) . With these classes, we can now replace our linear model with a neural network. . Adding a non-linearity . So far we have a general procedure for optimising the parameters of a function, and we have tried it out on a very boring function: a simple linear classifier. A linear classifier is very constrained in terms of what it can do. To make it a bit more complex (and able to handle more tasks), we need to add a non-linearity between two linear classifiers, and this is what will gived us a neural network. . Here is the entire definition of a basic neural network: . def simple_net(xb): res = xb@w1 + b1 res = res.max(tensor(0.0)) res = res@w2 + b2 return res . That&#39;s it! All we have in simple_net is two linear classifiers with a max function between them. . Here, w1 and w2 are weight tensors, and b1 and b2 are bias tensors; that is, parameters that are initially randomly initialised, just like we did in the previous section. . w1 = init_params((28*28,30)) b1 = init_params(30) w2 = init_params((30,1)) b2 = init_params(1) . The key point about this is that w1 has 30 output activations (which means that w2 must have 30 input activations, so they match). That means that the first layer can construct 30 different features, each representing some different mix of pixels. You can change that 30 to anything you like, to make the model more or less complex. . That little function res.max(tensor(0.0)) is called a rectified linear unit, also known as ReLU. We think we can all agree that rectified linear unit sounds pretty fancy and complicated... But actually, there&#39;s nothing more to it than res.max(tensor(0.0)), in other words: replace every negative number with a zero. This tiny function is also available in PyTorch as F.relu: . plot_function(F.relu) . j: There is an enormous amount of jargon in deep learning, such as: rectified linear unit. The vast vast majority of this jargon is no more complicated than can be implemented in a short line of code and Python, as we saw in this example. The reality is that for academics to get their papers published they need to make them sound as impressive and sophisticated as possible. One of the ways that they do that is to introduce jargon. Unfortunately, this has the result that the field ends up becoming far more intimidating and difficult to get into than it should be. You do have to learn the jargon, because otherwise papers and tutorials are not going to mean much to you. But that doesn&#39;t mean you have to find the jargon intimidating. Just remember, when you come across a word or phrase that you haven&#39;t seen before, it will almost certainly turn out that it is a very simple concept that it is referring to. . The basic idea is that by using more linear layers, we can have our model do more computation, and therefore model more complex functions. But there&#39;s no point just putting one linear layout directly after another one, because when we multiply things together and then at them up multiple times, that can be replaced by multiplying different things together and adding them up just once! That is to say, a series of any number of linear layers in a row can be replaced with a single linear layer with a different set of parameters. . But if we put a non-linear function between them, such as max, then this is no longer true. Now, each linear layer is actually somewhat decoupled from the other ones, and can do its own useful work. The max function is particularly interesting, because it operates as a simple &quot;if&quot; statement. For any arbitrarily wiggly function, we can approximate it as a bunch of lines joined together; to make it more close to the wiggly function, we just have to use shorter lines. . s: Mathematically, we say the composition of two linear functions is another linear function. So we can stack as many linear classifiers on top or each other, without non-linear functions between them, it will jsut be the same as one linear classifier. . Amazingly enough, it can be mathematically proven that this little function can solve any computable problem to an arbitrarily high level of accuracy, if you can find the right parameters for w1 and w2, and if you make these matrices big enough. This is known as the universal approximation theorem . The three lines of code that we have here are known as layers. The first and third are known as linear layers, and the second line of code is known variously as a nonlinearity, or activation function. . Just like the previous section, we can replace this code with something a bit simpler, by taking advantage of PyTorch: . simple_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1) ) . nn.Sequential creates a module which will call each of the listed layers or functions in turn. . F.relu is a function, not a PyTorch module. nn.ReLU is a PyTorch module that does exactly the same thing. Most functions that can appear in a model also have identical forms that are modules. Generally, it&#39;s just a case of replacing F with nn, and changing the capitalization. When using nn.Sequential PyTorch requires us to use the module version. Since modules are classes, we have to instantiate them, which is why you see nn.ReLU() above. Because nn.Sequential is a module, we can get its parameters--which will return a list of all the parameters of all modules it contains. . Let&#39;s try it out! For deeper models, we may need to use a lower learning rate and a few more epochs. . learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . #hide_output learn.fit(40, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.294820 | 0.416238 | 0.504907 | 00:00 | . 1 | 0.141692 | 0.216893 | 0.816487 | 00:00 | . 2 | 0.079073 | 0.110840 | 0.921001 | 00:00 | . 3 | 0.052444 | 0.075782 | 0.941119 | 00:00 | . 4 | 0.040078 | 0.059658 | 0.957802 | 00:00 | . 5 | 0.033729 | 0.050542 | 0.962709 | 00:00 | . 6 | 0.030057 | 0.044751 | 0.965653 | 00:00 | . 7 | 0.027653 | 0.040775 | 0.967615 | 00:00 | . 8 | 0.025914 | 0.037867 | 0.969087 | 00:00 | . 9 | 0.024563 | 0.035642 | 0.970069 | 00:00 | . 10 | 0.023465 | 0.033873 | 0.972031 | 00:00 | . 11 | 0.022547 | 0.032421 | 0.972031 | 00:00 | . 12 | 0.021761 | 0.031202 | 0.973013 | 00:00 | . 13 | 0.021081 | 0.030153 | 0.974485 | 00:00 | . 14 | 0.020482 | 0.029238 | 0.974485 | 00:00 | . 15 | 0.019949 | 0.028429 | 0.975957 | 00:00 | . 16 | 0.019472 | 0.027706 | 0.976938 | 00:00 | . 17 | 0.019039 | 0.027055 | 0.977429 | 00:00 | . 18 | 0.018645 | 0.026466 | 0.977920 | 00:00 | . 19 | 0.018283 | 0.025931 | 0.977920 | 00:00 | . 20 | 0.017950 | 0.025441 | 0.978901 | 00:00 | . 21 | 0.017641 | 0.024991 | 0.979882 | 00:00 | . 22 | 0.017353 | 0.024576 | 0.979882 | 00:00 | . 23 | 0.017084 | 0.024192 | 0.980373 | 00:00 | . 24 | 0.016832 | 0.023837 | 0.980864 | 00:00 | . 25 | 0.016595 | 0.023506 | 0.981354 | 00:00 | . 26 | 0.016371 | 0.023198 | 0.981354 | 00:00 | . 27 | 0.016159 | 0.022910 | 0.981845 | 00:00 | . 28 | 0.015959 | 0.022641 | 0.981845 | 00:00 | . 29 | 0.015768 | 0.022389 | 0.981845 | 00:00 | . 30 | 0.015587 | 0.022154 | 0.981845 | 00:00 | . 31 | 0.015414 | 0.021932 | 0.981845 | 00:00 | . 32 | 0.015249 | 0.021725 | 0.981845 | 00:00 | . 33 | 0.015092 | 0.021529 | 0.982336 | 00:00 | . 34 | 0.014941 | 0.021345 | 0.982336 | 00:00 | . 35 | 0.014796 | 0.021171 | 0.982826 | 00:00 | . 36 | 0.014658 | 0.021007 | 0.982826 | 00:00 | . 37 | 0.014524 | 0.020852 | 0.982826 | 00:00 | . 38 | 0.014396 | 0.020704 | 0.983317 | 00:00 | . 39 | 0.014272 | 0.020564 | 0.983317 | 00:00 | . We&#39;re not showing the 40 lines of output here to save room; the training process is recorded in learn.recorder, with the table of output stored in the values attribute, so we can plot the accuracy over training as: . plt.plot(L(learn.recorder.values).itemgot(2)); . ...and we can view the final accuracy: . learn.recorder.values[-1][2] . 0.983316957950592 . At this point we have something that is rather magical: . A function that can solve any problem to any level of accuracy (the neural network) given the correct set of parameters | A way to find the best set of parameters for any function (stochastic gradient descent) | This is why deep learning can do things which seem rather magical. Believing that this combination of simple techniques can really solve any problem here is one of the biggest steps that we find many students have to take. It seems too good to be true. It seems like things should be more difficult and complicated than this. Our recommendation: try it out! We will take our own recommendation and try this model on the MNIST dataset. Since we are doing everything from scratch ourselves (except for calculating the gradients) you know that there is no special magic hiding behind the scenes… . There is no need to stop at just two linear layers. We can add as many as we want, as long as we add a nonlinearity between each pair of linear layers. As we will learn, however, the deeper the model gets, the harder it is to optimise the parameters in practice. Later in this book we will learn about some simple but brilliantly effective techniques for training deeper models. . We already know that a single nonlinearity with two linear layers is enough to approximate any function. So why would we use deeper models? The reason is performance. With a deeper model (that is, one with more layers) we do not need to use as many parameters; it turns out that we can use smaller matrices, with more layers, and get better results than we would get with larger matrices, and few layers. . That means that we can train them more quickly, and our model will take up less memory. In the 1990s researchers were so focused on the universal approximation theorem that very few were experimenting with more than one nonlinearity. This theoretical but not practical foundation held back the field for years. Some researchers, however, did experiment with deep models, and eventually were able to show that these models could perform much better in practice. Eventually, theoretical results were developed which showed why this happens. Today, it is extremely unusual to find anybody using a neural network with just one nonlinearity. . Here what happens when we train 18 layer model using the same approach we saw in &lt;&gt;:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; dls = ImageDataLoaders.from_folder(path) learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(1, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.125685 | 0.026256 | 0.992640 | 00:11 | . Nearly 100% accuracy! That&#39;s a big difference compared to our simple neural net. But as you&#39;ll learn in the remainder of this book, there are just a few little tricks you need to use to get such great results from scratch yourself. You already know the key foundational pieces. (Of course, even once you know all the tricks, you&#39;ll nearly always want to work with the pre-built classes provided by PyTorch and fastai, because they save you having to think about all the little details yourself.) . Jargon recap . Congratulations: you now know how to create and train a deep neural network from scratch! There has been quite a few steps to get to this point, but you might be surprised at how simple it really has ended up. . Now that we are at this point, it is a good opportunity to define, and review, some jargon and concepts. . The neural network contains a lot of numbers. But those numbers only have one of two types: numbers that are calculated, and the parameters that these are calculated from. This gives us the two most important pieces of jargon to learn: . activations: numbers that are calculated (both by linear and non-linear layers) | parameters: numbers that are randomly initialised, and optimised (that is, the numbers that define the model) | . We will often talk in this book about activations and parameters. Remember that they have very specific meanings. They are numbers. They are not abstract concepts, but they are actual specific numbers that are in your model. Part of becoming a good deep learning practitioner is getting used to the idea of actually looking at your activations and parameters, and plotting them and testing whether they are behaving correctly. . Our activations and parameters are all contained in tensors. These are simply regularly shaped arrays. For example, a matrix. Matrices have rows and columns; we call these the axes or dimensions. The number of dimensions of a tensor is its rank. There are some special tensors: . rank zero: scalar | rank one: vector | rank two: matrix | . A neural network contains a number of layers. Each layer is either linear or nonlinear. We generally alternate between these two kinds of layers in a neural network. Sometimes people refer to both a linear layer and its subsequent nonlinearity together as a single layer. Yes, this is confusing. Sometimes a nonlinearity is referred to as an activation function. . &lt;&gt; contains the concepts related to SGD.&lt;/p&gt; asciidoc [[dljargon1]] .Deep learning vocabulary [options=&quot;header&quot;] |===== | Term | Meaning |**ReLU** | Funxtion that returns 0 for negatives numbers and doesn&#39;t change positive numbers |**mini-batch** | A few inputs and labels gathered together in two big arrays |**forward pass** | Applying the model to some input and computing the predictions |**loss** | A value that represents how well (or badly) our model is doing |**gradient** | The derivative of the loss with respect to some parameter of the model |**backard pass** | Computing the gradients of the loss with respect to all model parameters |**gradient descent** | Taking a step in the directions opposite to the gradients to make the model parameters a little bit better |**learning rate** | The size of the step we take when applying SGD to update the paramters of the model |===== . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Choose Your Own Adventure reminder . Did you choose to skip over chapters 2 &amp; 3, in your excitement to peak under the hood? Well, here&#39;s your reminder to head back to chapter 2 now, because you&#39;ll be needing to know that stuff very soon! . Questionnaire . How is a greyscale image represented on a computer? How about a color image? | How are the files and folders in the MNIST_SAMPLE dataset structured? Why? | Explain how the &quot;pixel similarity&quot; approach to classifying digits works. | What is a list comprehension? Create one now that selects odd numbers from a list and doubles them. | What is a &quot;rank 3 tensor&quot;? | What is the difference between tensor rank and shape? How do you get the rank from the shape? | What are RMSE and L1 norm? | How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop? | Create a 3x3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom right 4 numbers. | What is broadcasting? | Are metrics generally calculated using the training set, or the validation set? Why? | What is SGD? | Why does SGD use mini batches? | What are the 7 steps in SGD for machine learning? | How do we initialize the weights in a model? | What is &quot;loss&quot;? | Why can&#39;t we always use a high learning rate? | What is a &quot;gradient&quot;? | Do you need to know how to calculate gradients yourself? | Why can&#39;t we use accuracy as a loss function? | Draw the sigmoid function. What is special about its shape? | What is the difference between loss and metric? | What is the function to calculate new weights using a learning rate? | What does the DataLoader class do? | Write pseudo-code showing the basic steps taken each epoch for SGD. | Create a function which, if passed two arguments [1,2,3,4] and &#39;abcd&#39;, returns [(1, &#39;a&#39;), (2, &#39;b&#39;), (3, &#39;c&#39;), (4, &#39;d&#39;)]. What is special about that output data structure? | What does view do in PyTorch? | What are the &quot;bias&quot; parameters in a neural network? Why do we need them? | What does the @ operator do in python? | What does the backward method do? | Why do we have to zero the gradients? | What information do we have to pass to Learner? | Show python or pseudo-code for the basic steps of a training loop. | What is &quot;ReLU&quot;? Draw a plot of it for values from -2 to +2. | What is an &quot;activation function&quot;? | What&#39;s the difference between F.relu and nn.ReLU? | The universal approximation theorem shows that any function can be approximately as closely as needed using just one nonlinearity. So why do we normally use more? | Further research . Create your own implementation of Learner from scratch, based on the training loop shown in this chapter. | Complete all the steps in this chapter using the full MNIST datasets (that is, for all digits, not just threes and sevens). This is a significant project and will take you quite a bit of time to complete! You&#39;ll need to do some of your own research to figure out how to overcome some obstacles you&#39;ll meet on the way. | &lt;/div&gt; . .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_mnist_basics.html",
            "relUrl": "/2020/03/19/_mnist_basics.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Chapter 11",
            "content": "[[chapter_midlevel_data]] Data munging with fastai&#39;s mid-level API . We have seen what Tokenizer or a Numericalize do to a collection of texts, and how they&#39;re used inside the data block API, which handles those transforms for us directly using the TextBlock. But what if we want to only apply one of those transforms, either to see intermediate results or because we have already tokenized texts. More generally, what can we do when the data block API is not flexible enough to accommodate our particular use case? For this, we need to use fastai&#39;s mid-level API for processing data. The data block API is built on top of that layer, so it will allow you to do everything the data block API does, and much much more. . Going deeper into fastai&#39;s layered API . The fastai library is built on a layered API. At the very top layer, there are applications that allow us to train a model in five lines of codes, as we saw in &lt;&gt;. In the case of creating DataLoaders for a text classifier, for instance, we used the line:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from fastai2.text.all import * dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid=&#39;test&#39;) . The factory method TextDataLoaders.from_folder is very convenient when your data is arranged the exact same way as the IMDb dataset, but in practice, that often won&#39;t be the case. The data block API offers more flexibility. As we saw in the last chapter, we can ge the same result with: . path = untar_data(URLs.IMDB) dls = DataBlock( blocks=(TextBlock.from_folder(path),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path) . But it&#39;s sometimes not flexible enough. For debugging purposes for instance, we might need to apply just parts of the transforms that come with this data block. Or, we might want to create DataLoaders for some application that isn&#39;t directly supported by fastai. In this section, we&#39;ll dig into the pieces that are used inside fastai to implement the data block API. By understanding these pieces, you&#39;ll be able to leverage the power and flexibility of this mid-tier API. . . Note: The mid-level API in general does not only contain functionality for creating DataLoaders. It also has the callback system , which allows us to customize the training loop any way we like, and the general optimizer. Both will be covered in &lt;&gt;. &lt;/div&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Transforms . When we studied tokenization and numericalization in the last chapter, we started by grabbing a bunch of texts: . files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;]) txts = L(o.open().read() for o in files[:2000]) . We then showed how to tokenize them with a Tokenizer: . tok = Tokenizer.from_folder(path) tok.setup(txts) toks = txts.map(tok) toks[0] . (#228) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;this&#39;,&#39;movie&#39;,&#39;,&#39;,&#39;which&#39;,&#39;i&#39;,&#39;just&#39;,&#39;discovered&#39;,&#39;at&#39;...] . num = Numericalize() num.setup(toks) nums = toks.map(num) nums[0][:10] . tensor([ 2, 8, 20, 27, 11, 88, 18, 53, 3286, 45]) . And how to numericalize, including automatically creating the vocab for our corpus: . num = Numericalize() num.setup(toks) nums = toks.map(num) nums[0][:10] . tensor([ 2, 8, 20, 27, 11, 88, 18, 53, 3286, 45]) . The classes also have a decode method. For instance, Numericalize.decode gives us back the string tokens: . nums_dec = num.decode(nums[0][:10]); nums_dec . (#10) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;this&#39;,&#39;movie&#39;,&#39;,&#39;,&#39;which&#39;,&#39;i&#39;,&#39;just&#39;,&#39;discovered&#39;,&#39;at&#39;] . ...and Tokenizer.decode turns this back into a single string (it may not, however, be exactly the same as the original string; this depends on whether the tokenizer is reversible, which the default word tokenizer is not at the time we&#39;re writing this book): . tok.decode(nums_dec) . &#39;xxbos xxmaj this movie , which i just discovered at&#39; . decode is used by fastai&#39;s show_batch and show_results, as well as some other inference methods, to convert predictions and mini-batches into a human-understandable representation. . For each of tok or num above, we created an object, called the setup method (which trains the tokenizer if needed for tok and creates the vocab for num), applied it to our raw texts (by calling the object as a function), and then finally decoded it back to an understandable representation. These steps are needed for most data preprocessing tasks, so fastai provides a class that encapsulates them. This is the Transform class. Both Tokenize and Numericalize are Transforms. . In general, a Transform is an object that behaves like a function, has an optional setup that will initialize some inner state (like the vocab inside num for instance), and has an optional decode that will reverse the function (this reversal may not be perfect, as we saw above for tok). . A good example of decode is found in the Normalize transform that we saw in &lt;&gt;: to be able to plot the images its decode method undoes the normalization (i.e. it multiplies by the std and adds back the mean). On the other hand, data augmentation transforms do not have a decode method, since we want to show the effects on images, to make sure the data augmentation is working as we want.&lt;/p&gt; A special behavior of Transforms is that they always get applied over tuples: in general, our data is always a tuple (input,target) (sometimes with more than one input or more than one target). When applying a transform on an item like this, such as Resize, we don&#39;t want to resize the tuple, but resize the input (if applicable) and the target (if applicable). It&#39;s the same for the batch transforms that do data augmentation: when the input is an image and the target is a segmentation mask, the transform needs to be applied (the same way) to the input and the target. . We can see this behavior if we pass a tuple of texts to tok: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; tok((txts[0], txts[1])) . ((#228) [&#39;xxbos&#39;,&#39;xxmaj&#39;,&#39;this&#39;,&#39;movie&#39;,&#39;,&#39;,&#39;which&#39;,&#39;i&#39;,&#39;just&#39;,&#39;discovered&#39;,&#39;at&#39;...], (#238) [&#39;xxbos&#39;,&#39;i&#39;,&#39;stopped&#39;,&#39;watching&#39;,&#39;this&#39;,&#39;film&#39;,&#39;half&#39;,&#39;way&#39;,&#39;through&#39;,&#39;.&#39;...]) . Writing your own Transform . If you want to write a custom transform to apply to your data, the easiest way is to write a function. As you can see in this example, a Transform will only be applied to a matching type, if a type is provided (otherwise it will always be applied). In the following code, the :int in the function signature means that the f only gets applied to ints. That&#39;s why tfm(2.0) returns 2.0, but tfm(2) returns 3 here: . def f(x:int): return x+1 tfm = Transform(f) tfm(2),tfm(2.0) . (3, 2.0) . Here f is converted to a Transform with no setup and no decode method. . Python has a special syntax for passing a function (like f) to another function (or something that behaves like a function, known as a callable in Python), which is a decorator. A decorator is used by prepending a callable with @, and placing it before a function definition (there&#39;s lots of good online tutorials for Python decorators, so take a look if this is a new concept for you). The following is identical to the previous code: . @Transform def f(x:int): return x+1 f(2),f(2.0) . (3, 2.0) . If you need either setup or decode, you will need to subclass Transform. When writing this subclass, you need to implement the actual function in encodes, then (optionally), the setup behavior in setups and the decoding behavior in decodes: . class NormalizeMean(Transform): def setups(self, items): self.mean = sum(items)/len(items) def encodes(self, x): return x-self.mean def decodes(self, x): return x+self.mean . Here NormalizeMean will initialize some state during the setup (the mean of all elements passed), then the transformation is to subtract that mean. For decoding purposes, we implement the reverse of that transformation by adding the mean. Here is an example of NormalizeMean in action: . tfm = NormalizeMean() tfm.setup([1,2,3,4,5]) start = 2 y = tfm(start) z = tfm.decode(y) tfm.mean,y,z . (3.0, 5.0, 2.0) . Note that the method called and the method implemented are different, for each of these methods: . asciidoc [options=&quot;header&quot;] |====== | Class | To call | To implement | `nn.Module` (PyTorch) | `()` (i.e. call as function) | `forward` | `Transform` | `()` | `encodes` | `Transform` | `decode()` | `decodes` | `Transform` | `setup()` | `setups` |====== . So, for instance, you would never call setups directly, but instead would call setups. The reason for this is that setup does some work before and after calling setups for you. To learn more about Transforms and how you can use them to have different behavior depending on the type of the input, be sure to check the tutorials in the fastai docs. . Pipeline . To compose several transforms together, fastai provides Pipeline. We define a Pipeline by passing it a list of Transforms; it will then compose the transforms inside it. When you call a Pipeline on an object, it will automatically call the transforms inside, in order: . tfms = Pipeline([tok, num]) t = tfms(txts[0]); t[:20] . tensor([ 2, 8, 76, 10, 23, 3112, 23, 34, 3113, 33, 10, 8, 4477, 22, 88, 32, 10, 27, 42, 14]) . And you can call decode on the result of your encoding, to get back something you can display and analyze: . tfms.decode(t)[:100] . &#39;xxbos xxmaj well , &#34; cube &#34; ( 1997 ) , xxmaj vincenzo &#39;s first movie , was one of the most interesti&#39; . The only part that doesn&#39;t work the same way as in Transform is the setup. To properly setup a Pipeline of Transforms on some data, you need to use a TfmdLists. . TfmdLists and Datasets: Transformed collections . Your data is usually a set of raw items (like filenames, or rows in a dataframe) to which you want to apply a succession of transformations. We just saw that the succession of transformations was represented by a Pipeline in fastai. The class that groups together this pipeline with your raw items is called TfmdLists. . TfmdLists . Here is the short way of doing the transformation we saw in the previous section: . tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize]) . At initialization, the TfmdLists will automatically call the setup method of each transform in order, providing them not with the raw items but the items transformed by all the previous Transforms in order. We can get the result of our pipeline on any raw element just by indexing into the TfmdLists: . t = tls[0]; t[:20] . tensor([ 2, 8, 91, 11, 22, 5793, 22, 37, 4910, 34, 11, 8, 13042, 23, 107, 30, 11, 25, 44, 14]) . And the TfmdLists knows how to decode for showing purposing: . tls.decode(t)[:100] . &#39;xxbos xxmaj well , &#34; cube &#34; ( 1997 ) , xxmaj vincenzo &#39;s first movie , was one of the most interesti&#39; . In fact, it even has a show method: . tls.show(t) . xxbos xxmaj well , &#34; cube &#34; ( 1997 ) , xxmaj vincenzo &#39;s first movie , was one of the most interesting and tricky ideas that xxmaj i &#39;ve ever seen when talking about movies . xxmaj they had just one scenery , a bunch of actors and a plot . xxmaj so , what made it so special were all the effective direction , great dialogs and a bizarre condition that characters had to deal like rats in a labyrinth . xxmaj his second movie , &#34; cypher &#34; ( 2002 ) , was all about its story , but it was n&#39;t so good as &#34; cube &#34; but here are the characters being tested like rats again . &#34; nothing &#34; is something very interesting and gets xxmaj vincenzo coming back to his &#39; cube days &#39; , locking the characters once again in a very different space with no time once more playing with the characters like playing with rats in an experience room . xxmaj but instead of a thriller sci - fi ( even some of the promotional teasers and trailers erroneous seemed like that ) , &#34; nothing &#34; is a loose and light comedy that for sure can be called a modern satire about our society and also about the intolerant world we &#39;re living . xxmaj once again xxmaj xxunk amaze us with a great idea into a so small kind of thing . 2 actors and a blinding white scenario , that &#39;s all you got most part of time and you do n&#39;t need more than that . xxmaj while &#34; cube &#34; is a claustrophobic experience and &#34; cypher &#34; confusing , &#34; nothing &#34; is completely the opposite but at the same time also desperate . xxmaj this movie proves once again that a smart idea means much more than just a millionaire budget . xxmaj of course that the movie fails sometimes , but its prime idea means a lot and offsets any flaws . xxmaj there &#39;s nothing more to be said about this movie because everything is a brilliant surprise and a totally different experience that i had in movies since &#34; cube &#34; . . The TfmdLists is named with an &quot;s&quot; because it can handle a training and validation set with a splits argument. You just need to pass the indices of which elemets are in the training set, and which are in the validation set: . cut = int(len(files)*0.8) splits = [list(range(cut)), list(range(cut,len(files)))] tls = TfmdLists(files, [Tokenizer.from_folder(path), Numericalize], splits=splits) . You can then access them through the train and valid attribute: . tls.valid[0][:20] . tensor([ 2, 8, 20, 30, 87, 510, 1570, 12, 408, 379, 4196, 10, 8, 20, 30, 16, 13, 12216, 202, 509]) . If you have manually written a Transform that returns your whole data (input and target) from the raw items you had, then TfmdLists is the class you need. You can directly convert it to a DataLoaders object with the dataloaders method. This is what we will do in our Siamese example further in this chapter. . In general though, you have two (or more) parallel pipelines of transforms: one for processing your raw items into inputs and one to process your raw items into targets. For instance, here, the pipeline we defined only processes the input. If we want to do text classification, we have to process the labels as well. . Here we need to do two things: first take the label name from the parent folder. There is a function parent_label for this: . lbls = files.map(parent_label) lbls . (#50000) [&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;,&#39;pos&#39;...] . Then we need a Transform that will grab the unique items and build a vocab with it during setup, then will transform the string labels into integers when called. fastai provides this transform, it&#39;s called Categorize: . cat = Categorize() cat.setup(lbls) cat.vocab, cat(lbls[0]) . ((#2) [&#39;neg&#39;,&#39;pos&#39;], TensorCategory(1)) . To do the whole setup automatically on our list of files, we can create a TfmdLists as before: . tls_y = TfmdLists(files, [parent_label, Categorize()]) tls_y[0] . TensorCategory(1) . But then we end up with two separate objects for our inputs and targets, which is not what we want. This is where Datasets comes to the rescue. . Datasets . Datasets will apply two (or more) pipelines in parallel to the same raw object and build a tuple with the result. Like TfmdLists, it will automatically do the setup for us, and when we index into a Datasets, it will return us a tuple with the results of each pipeline: . x_tfms = [Tokenizer.from_folder(path), Numericalize] y_tfms = [parent_label, Categorize()] dsets = Datasets(files, [x_tfms, y_tfms]) x,y = dsets[0] x[:20],y . Like a TfmdLists, we can pass along splits to a Datasets to split our data between training and validation: . x_tfms = [Tokenizer.from_folder(path), Numericalize] y_tfms = [parent_label, Categorize()] dsets = Datasets(files, [x_tfms, y_tfms], splits=splits) x,y = dsets.valid[0] x[:20],y . (tensor([ 2, 8, 20, 30, 87, 510, 1570, 12, 408, 379, 4196, 10, 8, 20, 30, 16, 13, 12216, 202, 509]), TensorCategory(0)) . It can also decode any processed tuple or show it directly: . t = dsets.valid[0] dsets.decode(t) . (&#39;xxbos xxmaj this movie had horrible lighting and terrible camera movements . xxmaj this movie is a jumpy horror flick with no meaning at all . xxmaj the slashes are totally fake looking . xxmaj it looks like some 17 year - old idiot wrote this movie and a 10 year old kid shot it . xxmaj with the worst acting you can ever find . xxmaj people are tired of knives . xxmaj at least move on to guns or fire . xxmaj it has almost exact lines from &#34; when a xxmaj stranger xxmaj calls &#34; . xxmaj with gruesome killings , only crazy people would enjoy this movie . xxmaj it is obvious the writer does n &#39;t have kids or even care for them . i mean at show some mercy . xxmaj just to sum it up , this movie is a &#34; b &#34; movie and it sucked . xxmaj just for your own sake , do n &#39;t even think about wasting your time watching this crappy movie .&#39;, &#39;neg&#39;) . The last step is to convert your Datasets object to a DataLoaders, which can be done with the dataloaders method. Here we need to pass along special arguments to take care of the padding problem (as we saw in the last chapter). This needs to happen just before we batch the elements, so we pass it to before_batch: . dls = dsets.dataloaders(bs=64, before_batch=pad_input) . dataloaders directly calls DataLoader on each subset of our Datasets. fastai&#39;s DataLoader expands the PyTorch class of the same name and is responsible for collating the items from our datasets into batches. It has a lot of points of customization but the most important you should know are: . after_item: applied on each item after grabbing it inside the dataset. This is the equivalent of the item_tfms in DataBlock. | before_batch: applied on the list of items before they are collated. This is the ideal place to pad items to the same size. | after_batch: applied on the batch as a whole after its construction. This is the equivalent of the batch_tfms in DataBlock. | . As a conclusion, here is the full code necessary to prepare the data for text classification: . tfms = [[Tokenizer.from_folder(path), Numericalize], [parent_label, Categorize]] files = get_text_files(path, folders = [&#39;train&#39;, &#39;test&#39;]) splits = GrandparentSplitter(valid_name=&#39;test&#39;)(files) dsets = Datasets(files, tfms, splits=splits) dls = dsets.dataloaders(dl_type=SortedDL, before_batch=pad_input) . The two differences with what we had above is the use of GrandParentSplitter to split our training and validation data, and the dl_type argument. This is to tell dataloaders to use the SortedDL class of DataLoader, and not the usual one. This is the class that will handle the construction of batches by putting samples of roughly the same lengths into batches. . This does the exact same thing as our DataBlock from above: . path = untar_data(URLs.IMDB) dls = DataBlock( blocks=(TextBlock.from_folder(path),CategoryBlock), get_y = parent_label, get_items=partial(get_text_files, folders=[&#39;train&#39;, &#39;test&#39;]), splitter=GrandparentSplitter(valid_name=&#39;test&#39;) ).dataloaders(path) . ...except that now, you know how to customize every single piece of it! . Let&#39;s practice what we just learned on this mid-level API for data preprocessing on a computer vision example now, with a Siamese Model input pipeline. . Applying the mid-tier data API: SiamesePair . A Siamese model takes two images and has to determine if they are of the same class or not. For this example, we will use the pets dataset again, and prepare the data for a model that will have to predict if two images of pets are of the same breed or not. We will explain here how to prepare the data for such a model, then we will train that model in &lt;&gt;.&lt;/p&gt; Firs things first, let&#39;s get the images in our dataset. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from fastai2.vision.all import * path = untar_data(URLs.PETS) files = get_image_files(path/&quot;images&quot;) . If we didn&#39;t care about showing our objects at all, we could directly create one transform to completely preprocess that list of files. We will want to look at those images though, so we need to create a custom type. When you call the show method on a TfmdLists or a Datasets object, it will decode items until it reaches a type that contains a show method and use it to show the object. That show method gets passed a ctx, which could be a matplotlib axes for images, or the row of a dataframe for texts. . Here we create a SiameseImage object that subclasses Tuple and is intended to contain three things: two images, and a boolean that&#39;s True if they are the breed. We also implement the special show method, such that it concatenates the two images, with a black line in the middle. Don&#39;t worry too much about the part that is in the if test (which is to show the SiameseImage when the images are Pillow images, and not tensors), the important part is in the last three lines. . class SiameseImage(Tuple): def show(self, ctx=None, **kwargs): img1,img2,same_breed = self if not isinstance(img1, Tensor): if img2.size != img1.size: img2 = img2.resize(img1.size) t1,t2 = tensor(img1),tensor(img2) t1,t2 = t1.permute(2,0,1),t2.permute(2,0,1) else: t1,t2 = img1,img2 line = t1.new_zeros(t1.shape[0], t1.shape[1], 10) return show_image(torch.cat([t1,line,t2], dim=2), title=same_breed, ctx=ctx) . Let&#39;s then create a first SiameseImage and check our show method works: . img = PILImage.create(files[0]) s = SiameseImage(img, img, True) s.show(); . We can also try with a second image that&#39;s not from the same class: . img1 = PILImage.create(files[1]) s1 = SiameseImage(img, img1, False) s1.show(); . The important thing with Transforms we saw before is that they dispatch over tuples or their subclasses. That&#39;s precisely why we chose to subclass tuple in this instance: this way we can apply any transform that work on images to our SiameseImage and it will be applied on each image in the tuple: . s2 = Resize(224)(s1) s2.show(); . Here the resize transform is applied to each of the two images, but not the boolean flag. Even if we have a custom type, we can thus benefit form all the data augmentation transforms inside the library. . We are now ready to build the Transform that we will use to get our data ready for a Siamese model. First, we will need a function to determine the class of all our images: . def label_func(fname): return re.match(r&#39;^(.*)_ d+.jpg$&#39;, fname.name).groups()[0] . Then here is our main transform. For each image, il will, with a probability of 0.5, draw an image from the same class and return a SiameseImage with a true label, or draw an image from another class and a return a SiameseImage with a false label. This is all done in the private _draw function. There is one difference between the training and validation set, which is why the transform needs to be initialized with the splits: on the training set, we will make that random pick each time we read an image, whereas on the validation set, we make this random pick once and for all at initialization. This way, we get more varied samples during training, but always the same validation set. . class SiameseTransform(Transform): def __init__(self, files, label_func, splits): self.labels = files.map(label_func).unique() self.lbl2files = {l: L(f for f in files if label_func(f) == l) for l in self.labels} self.label_func = label_func self.valid = {f: self._draw(f) for f in files[splits[1]]} def encodes(self, f): f2,t = self.valid.get(f, self._draw(f)) img1,img2 = PILImage.create(f),PILImage.create(f2) return SiameseImage(img1, img2, t) def _draw(self, f): same = random.random() &lt; 0.5 cls = self.label_func(f) if not same: cls = random.choice(L(l for l in self.labels if l != cls)) return random.choice(self.lbl2files[cls]),same . We can then create our main transform: . splits = RandomSplitter()(files) tfm = SiameseTransform(files, label_func, splits) tfm(files[0]).show(); . In the middle level API for data collection, we have two objects that can help us apply transforms on a set of items, TfmdLists and Datasets. If you remember what we have just seen, one applies a Pipeline of transforms and the other applies several Pipeline of transforms in parallel, to build tuples. Here, our main transform already builds the tuples, so we use TfmdLists: . tls = TfmdLists(files, tfm, splits=splits) show_at(tls.valid, 0); . And we can finally get our data in DataLoaders by calling the dataloaders method. One thing to be careful here is that this method does not take item_tfms and batch_tfms like a DataBlock. The fastai DataLoader has several hooks that are named after events: here what we apply on the items after they are grabbed is called after_item, and what we apply on the batch once it&#39;s buils is called after_batch. . dls = tls.dataloaders(after_item=[Resize(224), ToTensor], after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)]) . Note that we need to pass more transforms than usual: that&#39;s because the data block API usually adds them automatically: . ToTensor is the one that converts images to tensors (again, it&#39;s applied on every part of the tuple) | IntToFloatTensor convert the tensor of images that have integers from 0 to 255 to a tensor of floats, and divides by 255 to make the values between 0 and 1. | . And we have can now train a model using those DataLoaders. It needs a bit more customization than the usual model provided by cnn_learner since it has to take two images instead of one. We will see how to create such a model and train it in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; TK conclusion and questionnaire . Becoming a deep learning practitioner . Congratulations — you&#39;ve completed all of the chapters in this book which cover the key practical parts of training and using deep learning! You know how to use all of fastai&#39;s built in applications, and how to customise them using the data blocks API and loss functions. You even know how to create a neural network from scratch, and train it! (And hopefully you now know some of the questions to ask to help make sure your creations help improve society too.) . The knowledge you already have is enough to create full working prototypes of many types of neural network application. More importantly, it will help you understand the capabilities and limitations of deep learning models, and how to design a system which best handles these capabilities and limitations. . In the rest of this book we will be pulling apart these applications, piece by piece, to understand all of the foundations they are built on. This is important knowledge for a deep learning practitioner, because it is the knowledge which allows you to inspect and debug models that you build, and to create new applications which are customised for your particular projects. . &lt;/div&gt; . .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_midlevel_data.html",
            "relUrl": "/2020/03/19/_midlevel_data.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Title",
            "content": "fastai Learner from scratch . This final chapter (other than the conclusion, and the online chapters) is going to look a bit different. We will have far more code, and far less prose than previous chapters. We will introduce new Python keywords and libraries without discussing them. This chapter is meant to be the start of a significant research project for you. You see, we are going to implement many of the key pieces of the fastai and PyTorch APIs from scratch, building on nothing other than the components that we developed in &lt;&gt;! The key goal here is to end up with our own Learner class, and some callbacks--enough to be able to train a model on Imagenette, including examples of each of the key techniques we&#39;ve studied. On the way to building Learner, we will be creating Module, Parameter, and even our own parallel DataLoader… and much more.&lt;/p&gt; The end of chapter questionnaire is particularly important for this chapter. This is where we will be getting you started on the many interesting directions that you could take, using this chapter as your starting out point. What we are really saying is: follow through with this chapter on your computer, not on paper, and do lots of experiments, web searches, and whatever else you need to understand what&#39;s going on. You&#39;ve built up the skills and expertise to do this in the rest of this book, so we think you are going to go great! . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; First let&#39;s start with gathering (manually) the data. . Data . Have a look at the source to untar_data to see how it works. We&#39;ll use it here to access the 160-pixel version of Imagenette for use in this chapter: . path = untar_data(URLs.IMAGENETTE_160) . To access the image files, we can use get_image_files: . t = get_image_files(path) t[0] . Path(&#39;/home/jhoward/.fastai/data/imagenette2-160/val/n03417042/n03417042_3752.JPEG&#39;) . ...but you could do the same thing using just Python&#39;s standard library, with glob: . from glob import glob files = L(glob(f&#39;{path}/**/*.JPEG&#39;, recursive=True)).map(Path) files[0] . Path(&#39;/home/jhoward/.fastai/data/imagenette2-160/val/n03417042/n03417042_3752.JPEG&#39;) . If you look at the source for get_image_files, you&#39;ll see it uses Python&#39;s os.walk; this is a faster and more flexible function than glob, so be sure to try it out. . We can open an image with the Python Imaging Library&#39;s Image class: . im = Image.open(files[0]) im . im_t = tensor(im) im_t.shape . torch.Size([160, 213, 3]) . That&#39;s going to be the basis of our independent variable. For our dependent variable, we can use Path.parent from pathlib. First we&#39;ll need our vocab: . lbls = files.map(Self.parent.name()).unique(); lbls . (#10) [&#39;n03417042&#39;,&#39;n03445777&#39;,&#39;n03888257&#39;,&#39;n03394916&#39;,&#39;n02979186&#39;,&#39;n03000684&#39;,&#39;n03425413&#39;,&#39;n01440764&#39;,&#39;n03028079&#39;,&#39;n02102040&#39;] . ...and the reverse mapping, thanks to L.val2idx: . v2i = lbls.val2idx(); v2i . {&#39;n03417042&#39;: 0, &#39;n03445777&#39;: 1, &#39;n03888257&#39;: 2, &#39;n03394916&#39;: 3, &#39;n02979186&#39;: 4, &#39;n03000684&#39;: 5, &#39;n03425413&#39;: 6, &#39;n01440764&#39;: 7, &#39;n03028079&#39;: 8, &#39;n02102040&#39;: 9} . That&#39;s all the pieces we need to put together our Dataset. . Dataset . A Dataset in PyTorch can be anything which supports indexing (__getitem__) and len: . class Dataset: def __init__(self, fns): self.fns=fns def __len__(self): return len(self.fns) def __getitem__(self, i): im = Image.open(self.fns[i]).resize((64,64)).convert(&#39;RGB&#39;) y = v2i[self.fns[i].parent.name] return tensor(im).float()/255, tensor(y) . We need a list of training and validation filenames to pass to Dataset.__init__: . train_filt = L(o.parent.parent.name==&#39;train&#39; for o in files) train,valid = files[train_filt],files[~train_filt] len(train),len(valid) . (9469, 3925) . Now we can try it out: . train_ds,valid_ds = Dataset(train),Dataset(valid) x,y = train_ds[0] x.shape,y . (torch.Size([64, 64, 3]), tensor(0)) . show_image(x, title=lbls[y]); . As you see, our dataset is returning the independent and dependent variable as a tuple, which is just what we need. We&#39;ll need to be able to collate these into a mini-batch. Generally this is done with torch.stack, which is what we&#39;ll use here: . def collate(idxs, ds): xb,yb = zip(*[ds[i] for i in idxs]) return torch.stack(xb),torch.stack(yb) . Here&#39;s a mini-batch with two items, for testing our collate: . x,y = collate([1,2], train_ds) x.shape,y . (torch.Size([2, 64, 64, 3]), tensor([0, 0])) . Now that we have a dataset and a collation function, we&#39;re ready to create DataLoader. We&#39;ll add two more things here: optional shuffle for the training set, and ProcessPoolExecutor to do our preprocessing in parallel. A parallel data loader is very important, because opening and decoding a jpeg image is a slow process. One CPU core is not enough to decode images fast enough to keep a modern GPU busy. . class DataLoader: def __init__(self, ds, bs=128, shuffle=False, n_workers=1): self.ds,self.bs,self.shuffle,self.n_workers = ds,bs,shuffle,n_workers def __len__(self): return (len(self.ds)-1)//self.bs+1 def __iter__(self): idxs = L.range(self.ds) if self.shuffle: idxs = idxs.shuffle() chunks = [idxs[n:n+self.bs] for n in range(0, len(self.ds), self.bs)] with ProcessPoolExecutor(self.n_workers) as ex: yield from ex.map(collate, chunks, ds=self.ds) . Let&#39;s try it out with our training and validation datasets. . n_workers = min(16, defaults.cpus) train_dl = DataLoader(train_ds, bs=128, shuffle=True, n_workers=n_workers) valid_dl = DataLoader(valid_ds, bs=256, shuffle=False, n_workers=n_workers) xb,yb = first(train_dl) xb.shape,yb.shape,len(train_dl) . (torch.Size([128, 64, 64, 3]), torch.Size([128]), 74) . The speed of this data loader is not much slower than PyTorch&#39;s, but it&#39;s far simpler. So if you&#39;re debugging a complex data loading process, don&#39;t be afraid to try doing things manually to help see exactly what&#39;s going on. . For normalization, we&#39;ll need image statistics. Generally it&#39;s fine to calculate these on a single training mini-batch, since precision isn&#39;t needed here: . stats = [xb.mean((0,1,2)),xb.std((0,1,2))] stats . [tensor([0.4544, 0.4453, 0.4141]), tensor([0.2812, 0.2766, 0.2981])] . Our Normalize class just needs to store these stats and apply them. To see why the to_device is needed, try commenting it out, and see what happens later in this notebook. . class Normalize: def __init__(self, stats): self.stats=stats def __call__(self, x): if x.device != self.stats[0].device: self.stats = to_device(self.stats, x.device) return (x-self.stats[0])/self.stats[1] . We always like to test everything we build in a notebook, as soon as we build it: . norm = Normalize(stats) def tfm_x(x): return norm(x).permute((0,3,1,2)) . t = tfm_x(x) t.mean((0,2,3)),t.std((0,2,3)) . (tensor([0.3732, 0.4907, 0.5633]), tensor([1.0212, 1.0311, 1.0131])) . Here tfm_x isn&#39;t just applying Normalize, but is also permuting the axis order from NHWC to NCHW (see &lt;&gt; if you need a reminder what these acronyms refer to). PIL uses HWC axis order, which we can&#39;t use with PyTorch, hence the need for this permute.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; That&#39;s all we need for the data for our model. So now we need the model itself! . Module and Parameter . To create a model, we&#39;ll need Module. To create Module, we&#39;ll need Parameter, so let&#39;s start there. Recall that in &lt;&gt; we said that Parameter &quot;this class doesn&#39;t actually add any functionality (other than automatically calling requires_grad_() for us). It&#39;s only used as a &#39;marker&#39; to show what to include in parameters()&quot;. Here&#39;s a definition which does exactly that:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; class Parameter(Tensor): def __new__(self, x): return Tensor._make_subclass(Parameter, x, True) def __init__(self, *args, **kwargs): self.requires_grad_() . The implementation here is a bit awkward: we have to define the special __new__ Python method, and use the internal PyTorch method _make_subclass because, as at the time of writing, PyTorch doesn&#39;t otherwise work correctly with this kind of subclassing, or provide an officially supported API to do this. This may be fixed by the time you read this book, so look on the book website to see if there are updated details. . Our Parameter now behaves just like a tensor, as we wanted: . Parameter(tensor(3.)) . tensor(3., requires_grad=True) . Now that we have this, we can define Module: . class Module: def __init__(self): self.hook,self.params,self.children,self._training = None,[],[],False def register_parameters(self, *ps): self.params += ps def register_modules (self, *ms): self.children += ms @property def training(self): return self._training @training.setter def training(self,v): self._training = v for m in self.children: m.training=v def parameters(self): return self.params + sum([m.parameters() for m in self.children], []) def __setattr__(self,k,v): super().__setattr__(k,v) if isinstance(v,Parameter): self.register_parameters(v) if isinstance(v,Module): self.register_modules(v) def __call__(self, *args, **kwargs): res = self.forward(*args, **kwargs) if self.hook is not None: self.hook(res, args) return res def cuda(self): for p in self.parameters(): p.data = p.data.cuda() . The key functionality is in the definition of parameters(): . self.params + sum([m.parameters() for m in self.children], []) . This means that we can ask any Module for its parameters, and it will return them, including all children modules (recursively). But how does it know what its parameters are? It&#39;s thanks to implementing Python&#39;s special __setattr__ method, which is called for us any time Python sets an attribute on a class. Our implementation includes this line: . if isinstance(v,Parameter): self.register_parameters(v) . As you see, this is where we use our new Parameter class as a &quot;marker&quot;--anything of this class is added to our params. . Python&#39;s __call__ allows us to define what happens when our object is treated as a function; we just call forward (which doesn&#39;t exist here, so it&#39;ll need to be added by subclasses). Before we do, we&#39;ll call a hook, if it&#39;s defined. Now you can see that PyTorch hooks aren&#39;t doing anything fancy at all--they&#39;re just calling any hooks have been registered. . Other than these pieces of functionality, our Module also provides cuda and training attributes, which we&#39;ll use shortly. . Now we can create our first Module, which is ConvLayer: . class ConvLayer(Module): def __init__(self, ni, nf, stride=1, bias=True, act=True): super().__init__() self.w = Parameter(torch.zeros(nf,ni,3,3)) self.b = Parameter(torch.zeros(nf)) if bias else None self.act,self.stride = act,stride init = nn.init.kaiming_normal_ if act else nn.init.xavier_normal_ init(self.w) def forward(self, x): x = F.conv2d(x, self.w, self.b, stride=self.stride, padding=1) if self.act: x = F.relu(x) return x . We&#39;re not implementing F.conv2d from scratch, since you should have already done that (using unfold) in the questionnaire in &lt;&gt;, but instead we&#39;re just creating a small class that wraps it up, along with bias, and weight initialization. Let&#39;s check that it works correctly with Module.parameters():&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; l = ConvLayer(3, 4) len(l.parameters()) . 2 . ...and that we can call it (which will result in forward being called): . xbt = tfm_x(xb) r = l(xbt) r.shape . torch.Size([128, 4, 64, 64]) . In the same way, we can implement Linear: . class Linear(Module): def __init__(self, ni, nf): super().__init__() self.w = Parameter(torch.zeros(nf,ni)) self.b = Parameter(torch.zeros(nf)) nn.init.xavier_normal_(self.w) def forward(self, x): return x@self.w.t() + self.b . ...and test it works: . l = Linear(4,2) r = l(torch.ones(3,4)) r.shape . torch.Size([3, 2]) . Let&#39;s also create a testing module to check that if we include multiple parameters as attributes, then they are all correctly registered: . class T(Module): def __init__(self): super().__init__() self.c,self.l = ConvLayer(3,4),Linear(4,2) . Since we have a conv layer and a linear layer, each of which has weights and biases, we&#39;d expect four parameters in total: . t = T() len(t.parameters()) . 4 . We should also find that calling cuda() on this class puts all these parameters on the GPU: . t.cuda() t.l.w.device . device(type=&#39;cuda&#39;, index=5) . We can now use those pieces to create a CNN. . Simple CNN . As we&#39;ve seen, a Sequential class makes many architectures easier to implement, so let&#39;s make one: . class Sequential(Module): def __init__(self, *layers): super().__init__() self.layers = layers self.register_modules(*layers) def forward(self, x): for l in self.layers: x = l(x) return x . The forward method here just calls each layer in turn. Note that we have to use the register_modules method we defined in Module, since otherwise the contents of layers won&#39;t appear in parameters(). . . Important: Remember that we&#8217;re not using any PyTorch functionality for modules here; we&#8217;re defining everything ourselves. So if you&#8217;re not sure what register_modules does, or why it&#8217;s needed, have another look at our code for Module above to see what we wrote! . We can create a simplified AdaptivePool which only handles pooling to a 1x1 output, and flattens it as well, by just using mean: . class AdaptivePool(Module): def forward(self, x): return x.mean((2,3)) . That&#39;s enough for us to create a CNN! . def simple_cnn(): return Sequential( ConvLayer(3 ,16 ,stride=2), #32 ConvLayer(16,32 ,stride=2), #16 ConvLayer(32,64 ,stride=2), # 8 ConvLayer(64,128,stride=2), # 4 AdaptivePool(), Linear(128, 10) ) . Let&#39;s see if our parameters are all being registered correctly: . m = simple_cnn() len(m.parameters()) . 10 . Now we can try adding a hook. Note that we&#39;ve only left room for one hook in Module; you could make it a list, or else use something like Pipeline to run a few as a single function. . def print_stats(outp, inp): print (outp.mean().item(),outp.std().item()) for i in range(4): m.layers[i].hook = print_stats r = m(xbt) r.shape . 0.5239089727401733 0.8776043057441711 0.43470510840415955 0.8347987532615662 0.4357188045978546 0.7621666193008423 0.46562111377716064 0.7416611313819885 . torch.Size([128, 10]) . We have data and model. Now we need a loss function. . Loss . We&#39;ve already seen how to define &quot;negative log likelihood&quot;: . def nll(input, target): return -input[range(target.shape[0]), target].mean() . Well actually, there&#39;s no log here, since we&#39;re using the same definition as PyTorch. That means we need to put the log together with softmax: . def log_softmax(x): return (x.exp()/(x.exp().sum(-1,keepdim=True))).log() sm = log_softmax(r); sm[0][0] . tensor(-1.2790, grad_fn=&lt;SelectBackward&gt;) . Combining these together give us our cross entropy loss: . loss = nll(sm, yb) loss . tensor(2.5666, grad_fn=&lt;NegBackward&gt;) . Note that the formula . $$ log left ( frac{a}{b} right ) = log(a) - log(b)$$ . gives a simplification when we compute the log softmax, which was previously defined as (x.exp()/(x.exp().sum(-1))).log(): . def log_softmax(x): return x - x.exp().sum(-1,keepdim=True).log() sm = log_softmax(r); sm[0][0] . tensor(-1.2790, grad_fn=&lt;SelectBackward&gt;) . Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the LogSumExp trick. The idea is to use the following formula: . $$ log left ( sum_{j=1}^{n} e^{x_{j}} right ) = log left ( e^{a} sum_{j=1}^{n} e^{x_{j}-a} right ) = a + log left ( sum_{j=1}^{n} e^{x_{j}-a} right )$$where a is the maximum of the $x_{j}$. . Here&#39;s the same thing in code: . x = torch.rand(5) a = x.max() x.exp().sum().log() == a + (x-a).exp().sum().log() . tensor(True) . We&#39;ll put that into a function: . def logsumexp(x): m = x.max(-1)[0] return m + (x-m[:,None]).exp().sum(-1).log() logsumexp(r)[0] . tensor(3.9784, grad_fn=&lt;SelectBackward&gt;) . ...so we can use it for our log_softmax function: . def log_softmax(x): return x - x.logsumexp(-1,keepdim=True) . ...which gives the same result as before: . sm = log_softmax(r); sm[0][0] . tensor(-1.2790, grad_fn=&lt;SelectBackward&gt;) . We&#39;ll use these to create cross_entropy: . def cross_entropy(preds, yb): return nll(log_softmax(preds), yb).mean() . Let&#39;s now combine all those pieces together to create a Learner. . Learner . We have data, model, and loss function; only one thing left before we can fit a model, and that&#39;s an optimizer! Here&#39;s SGD: . class SGD: def __init__(self, params, lr, wd=0.): store_attr(self, &#39;params,lr,wd&#39;) def step(self): for p in self.params: p.data -= (p.grad.data + p.data*self.wd) * self.lr p.grad.data.zero_() . As we&#39;ve seen in this book, life is easier with a Learner, which needs to know our training and validation sets, which means we need DataLoaders to store them. We don&#39;t need any other functionality--just a place to store them and access them: . class DataLoaders: def __init__(self, *dls): self.train,self.valid = dls dls = DataLoaders(train_dl,valid_dl) . Now we&#39;re ready to create our Learner class, which you can see in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; class Learner: def __init__(self, model, dls, loss_func, lr, cbs, opt_func=SGD): store_attr(self, &#39;model,dls,loss_func,lr,cbs,opt_func&#39;) for cb in cbs: cb.learner = self def one_batch(self): self(&#39;before_batch&#39;) xb,yb = self.batch self.preds = self.model(xb) self.loss = self.loss_func(self.preds, yb) if self.model.training: self.loss.backward() self.opt.step() self(&#39;after_batch&#39;) def one_epoch(self, train): self.model.training = train self(&#39;before_epoch&#39;) dl = self.dls.train if train else self.dls.valid for self.num,self.batch in enumerate(progress_bar(dl, leave=False)): self.one_batch() self(&#39;after_epoch&#39;) def fit(self, n_epochs): self(&#39;before_fit&#39;) self.opt = self.opt_func(self.model.parameters(), self.lr) self.n_epochs = n_epochs try: for self.epoch in range(n_epochs): self.one_epoch(True) self.one_epoch(False) except CancelFitException: pass self(&#39;after_fit&#39;) def __call__(self,name): for cb in self.cbs: getattr(cb,name,noop)() . This is our largest class we&#39;ve created in the book, but each method is quite small, so by looking at each in turn we&#39;ll be able to following what&#39;s going on. . The main method we&#39;ll be calling is fit. This loops with: . for self.epoch in range(n_epochs) . ...and at each epoch calls self.one_epoch for each of train=True and then train=False. Then self.one_epoch calls self.one_batch for each batch in dls.train or dls.valid as appropriate (after wrapping the DataLoader in fastprogress.progress_bar. Finally, self.one_batch follows the usual set of steps to fit one mini-batch that we&#39;ve seen through this book. . Before and after each step, Learner calls self(...), which calls __call__ (which is standard Python functionality). __call__ uses getattr(cb,name) on each callback in self.cbs, which is a Python builtin function which returns the attribute (a method, in this case) with the requested name. So, for instance, self(&#39;before_fit&#39;) will call cb.before_fit() for each callback where that method is defined. . So we can see that Learner is really just using our standard training loop, except that it&#39;s also calling callbacks at appropriate times. So let&#39;s define some callbacks! . Callbacks . In Learner.__init__ we have: . for cb in cbs: cb.learner = self . In other words, every callback knows what learner it is used in. This is critical, since otherwise a callback can&#39;t get information from the learner, or change things in the learner. Since getting information from the learner is so common, we make that easier by definined Callback as a subclass of GetAttr, with a default attribute of learner. GetAttr is a class in fastai which implements Python&#39;s standard __getattr__ and __dir__ methods for you, such such any time you try to access an attribute that doesn&#39;t exist, it passes the request along to whatever you have defined as _default. . class Callback(GetAttr): _default=&#39;learner&#39; . For instance, we want to move all model parameters to the GPU automatically at the start of fit. We could do this by defining before_fit as self.learner.model.cuda(); however, because learner is the default attribute, and we have SetupLearnerCB inherit from Callback (which inherits from GetAttr), we can remove the .learner and just call self.model.cuda(): . class SetupLearnerCB(Callback): def before_batch(self): xb,yb = to_device(self.batch) self.learner.batch = tfm_x(xb),yb def before_fit(self): self.model.cuda() . In SetupLearnerCB we also move each mini-batch to the GPU, by calling to_device(self.batch) (we could also have used the longer to_device(self.learner.batch). Note however that in the line self.learner.batch = tfm_x(xb),yb we can&#39;t remove .learner, because here we&#39;re setting the attribute, not getting it. . Before we try our Learner out, let&#39;s create a callback to track and print progress, otherwise we won&#39;t really know if it&#39;s working properly: . class TrackResults(Callback): def before_epoch(self): self.accs,self.losses,self.ns = [],[],[] def after_epoch(self): n = sum(self.ns) print(self.epoch, self.model.training, sum(self.losses).item()/n, sum(self.accs).item()/n) def after_batch(self): xb,yb = self.batch acc = (self.preds.argmax(dim=1)==yb).float().sum() self.accs.append(acc) n = len(xb) self.losses.append(self.loss*n) self.ns.append(n) . Now we&#39;re ready to use our Learner for the first time! . cbs = [SetupLearnerCB(),TrackResults()] learn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs) learn.fit(1) . 0 True 2.1275552130636814 0.2314922378287042 . 0 False 1.9942575636942674 0.2991082802547771 . It&#39;s quite amazing to realize that we can implement all the key ideas from fastai&#39;s Learner in so little code! Let&#39;s now add some learning rate scheduling. . Scheduling the learning rate . If we&#39;re going to get good results, we&#39;ll want an LR Finder and one-cycle training. These are both annealing callbacks--that is, they are gradually changing hyperparameters as we train. Here&#39;s LRFinder: . class LRFinder(Callback): def before_fit(self): self.losses,self.lrs = [],[] self.learner.lr = 1e-6 def before_batch(self): if not self.model.training: return self.opt.lr *= 1.2 def after_batch(self): if not self.model.training: return if self.opt.lr&gt;10 or torch.isnan(self.loss): raise CancelFitException self.losses.append(self.loss.item()) self.lrs.append(self.opt.lr) . This shows how we&#39;re using CancelFitException, which is itself an empty class, only used to signify the type of exception. You can see in Learner that this exception is caught. (You should add and test CancelBatchException, CancelEpochException, etc yourself.) Let&#39;s try it out, by adding it to our list of callbacks: . lrfind = LRFinder() learn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[lrfind]) learn.fit(2) . 0 True 2.6336045582954903 0.11014890695955222 . 0 False 2.230653363853503 0.18318471337579617 . &lt;progress value=&#39;12&#39; class=&#39;&#39; max=&#39;74&#39;, style=&#39;width:300px; height:20px; vertical-align: middle;&#39;&gt;&lt;/progress&gt; 16.22% [12/74 00:02&lt;00:12] ...and have a look at the results: . plt.plot(lrfind.lrs[:-2],lrfind.losses[:-2]) plt.xscale(&#39;log&#39;) . Now we can define our OneCycle training callback: . class OneCycle(Callback): def __init__(self, base_lr): self.base_lr = base_lr def before_fit(self): self.lrs = [] def before_batch(self): if not self.model.training: return n = len(self.dls.train) bn = self.epoch*n + self.num mn = self.n_epochs*n pct = bn/mn pct_start,div_start = 0.25,10 if pct&lt;pct_start: pct /= pct_start lr = (1-pct)*self.base_lr/div_start + pct*self.base_lr else: pct = (pct-pct_start)/(1-pct_start) lr = (1-pct)*self.base_lr self.opt.lr = lr self.lrs.append(lr) . We&#39;ll try an LR of 0.1: . onecyc = OneCycle(0.1) learn = Learner(simple_cnn(), dls, cross_entropy, lr=0.1, cbs=cbs+[onecyc]) . Let&#39;s fit for a while and see how it looks (we won&#39;t show all the output in the book--try it in the notebook to see the results): . #hide_output learn.fit(8) . Finally, we&#39;ll check that the learning rate followed the schedule we defined (as you see, we&#39;re not using cosine annealing here). . plt.plot(onecyc.lrs); . TK conclusion . Questionnaire . For the questions here that ask you to explain what some function or class is, you should also complete your own code experiments. . What is glob? | How do you open an image with the Python imaging library? | What does L.map do? | What does Self do? | What is L.val2idx? | What methods do you need to implement to create your own Dataset? | Why do we call convert when we open an image from Imagenette? | What does ~ do? How is it useful for splitting training and validation sets? | Which of these classes does ~ work with: L, Tensor, numpy array, Python list, pandas DataFrame? | What is ProcessPoolExecutor? | How does L.range(self.ds) work? | What is __iter__? | What is first? | What is permute? Why is it needed? | What is a recursive function? How does it help us define the parameters method? | Write a recursive function which returns the first 20 items of the Fibonacci sequence. | What is super? | Why do subclasses of Module need to override forward instead of defining __call__? | In ConvLayer why does init depend on act? | Why does Sequential need to call register_modules? | Write a hook that prints the shape of every layers activations. | What is LogSumExp? | Why is log_softmax useful? | What is GetAttr? How is it helpful for callbacks? | Reimplement one of the callbacks in this chapter without inheriting from Callback or GetAttr. | What does Learner.__call__ do? | What is getattr? (Note the case difference to GetAttr!) | Why is there a try block in fit? | Why do we check for model.training in one_batch? | What is store_attr? | What is the purpose of TrackResults.before_epoch? | What does model.cuda() do? How does it work? | Why do we need to check model.training in LRFinder and OneCycle? | Use cosine annealing in OneCycle. | Further research . Write resnet18 from scratch (refer to &lt;&gt; as needed), and train it with the Learner in this chapter.&lt;/li&gt; Implement a batchnorm layer from scratch and use it in your resnet18. | Write a mixup callback for use in this chapter. | Add momentum to SGD. | Pick a few features that you&#39;re interested in from fastai (or any other library) and implement them in this chapter. | Pick a research paper that&#39;s not yet implemented in fastai or PyTorch and implement it in this chapter. Port it over to fastai. | Submit a PR to fastai, or create your own extension module and release it. | Hint: you may find it helpful to use nbdev to create and deploy your package. | . | &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; | . .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_learner.html",
            "relUrl": "/2020/03/19/_learner.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Title",
            "content": "[[chapter_foundations]] A neural net from the foundations . This chapter begins a journey where we will go from the very basics and dig inside what was hidden in the models we used in the previous chapters. We will be covering many of the same things we&#39;ve seen before, but this time around we&#39;ll be looking much more closely at the implementation details, and much less closely at the practical issues of how and why things are as they are. . We will build everything from scratch, only using basic indexing into a tensor. We write a neural net from the foundations, then we will implement our own backpropagation from scratch, so we&#39;ll know what is happening in PyTorch when we do loss.backward(). We&#39;ll also see how to extend PyTorch with custom autograd functions that allow you to specify your own forward and backward computations. . A neural net layer from scratch . Let&#39;s start by refreshing our understanding of how matrix multiplication is used in a basic neural network. Since we&#39;re building everything up from scratch, we&#39;ll use nothing but plain Python initially (except for indexing into PyTorch tensors), and then replace the plain Python with PyTorch functionality once we&#39;ve seen how to create it. . Modeling a neuron . A neuron receives a given number of inputs and has an internal weight for each of them. It then sums those weighted inputs to produce an output and add an inner bias. In math, this can be written: . $$ out = sum_{i=1}^{n} x_{i} w_{i} + b$$if we name our inputs $(x_{1}, dots,x_{n})$, our weights $(w_{1}, dots,w_{n})$ and our bias $b$. In code this translates into: . output = sum([x*w for x,w in zip(inputs,weights)]) + bias . This output is then fed into a non-linear function before being sent to another neuron called an activation function, and the most common function used in Deep Learning for this the Rectified Linear Unit or ReLU, which, as we&#39;ve seen, is a fancy way of saying . def relu(x): return x if x &gt;= 0 else 0 . A Deep Learning model is then built by stacking a lot of those neurons in successive layers. We create a first layer with a certain number of neurons (usually called hidden size) and link all the inputs to each of those neurons. Such a layer is often called fully connected layer or a dense layer (for densely connected) or a linear layer. . If you have done a little bit of linear algebra, you may remember than when you have a lot of: . sum([x*w for x,w in zip(input,weight)]) . ...for each input in our batch and the weight of each neuron, it&#39;s the equivalent of one matrix multiplication. More precisely, if our inputs are in a matrix x which is batch_size by n_inputs, and if we have grouped the weights of our neurons in a matrix w which is n_neurons by n_inputs (each neuron must have the same number of weights as they have inputs) and all the biases in a vector b of size n_neurons, then the output of this fully connected layer is . y = x @ w.t() + b . where @ represents the matrix product and w.t() is the transpose matrix of w. The output y is then of size batch_size by n_neurons and in position (i,j), we have (for the mathy folks out there): . $$y_{i,j} = sum_{k=1}^{n} x_{i,k} w_{k,j} + b_{j}$$or in code: . y[i,j] = sum([a * b for a,b in zip(x[i,:],w[j,:])]) + b[j] . The transpose is necessary because in the mathematical definition of the matrix product m @ n, the coefficient (i,j) is: . sum([a * b for a,b in zip(m[i,:],n[:,j])]) . So the very basic operation we need is a matrix multiplication, as it&#39;s what is hidden in the core of a neural net. . Matrix multiplication from scratch . Let&#39;s write a function that computes the matrix product of two tensors, before we allow ourselves to use the PyTorch version of it. We will only use the indexing in PyTorch tensors. . import torch from torch import tensor . We&#39;ll need three nested for loops: one for the row indices, one for the column indices and one for the inner sum. ac, ar stand for number of columns of a, number of rows of a respectively (same convention for b) and we make sure the matrix product is possible by checking that a has as many columns as b has rows. . def matmul(a,b): ar,ac = a.shape # n_rows * n_cols br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): for k in range(ac): c[i,j] += a[i,k] * b[k,j] return c . To test this out, we&#39;ll pretend (using random matrices) that we&#39;re working with a small batch of 5 MNIST images, flattened into 28*28 vectors, and a linear model to turn them into 10 activations: . m1 = torch.randn(5,28*28) m2 = torch.randn(784,10) . Let&#39;s time our function, using the Jupyter &quot;magic&quot; %time: . %time t1=matmul(m1, m2) . CPU times: user 1.15 s, sys: 4.09 ms, total: 1.15 s Wall time: 1.15 s . ...and how does that compare to PyTorch&#39;s builtin? . %timeit -n 20 t2=m1@m2 . 14 µs ± 8.95 µs per loop (mean ± std. dev. of 7 runs, 20 loops each) . As we can see, in Python three nested loops is a very bad idea! Python is a slow language, and this isn&#39;t going to be very efficient. We see here that PyTorch is around 100,000 times faster than Python--and that&#39;s before we even start using the GPU! . Where does this difference come from? That&#39;s because PyTorch didn&#39;t write its matrix multiplication in Python but in C++ to make it fast. In general, whenever we do some computations on tensors, we will need to vectorize them so that we can take advantage of the speed of PyTorch, usually by using two techniques: elementwise arithmetic and broadcasting. . We will show how to do this on our example of matrix multiplication. . Elementwise arithmetic . All the basic operators (+,-,*,/,&gt;,&lt;,==) can be applied element-wise. That means if we write a+b for two tensors a and b that have the same shape, we will get a tensor with the sums of one element of a with one element of `b. . a = tensor([10., 6, -4]) b = tensor([2., 8, 7]) a + b . tensor([12., 14., 3.]) . The booleans operators will return an array of booleans: . a &lt; b . tensor([False, True, True]) . If we want to know if every element of a is less than the corresponding element in b, or if two tensors are equals, we need to combine those elementwise operations with torch.all. . (a &lt; b).all(), (a==b).all() . (tensor(False), tensor(False)) . Note that reduction operations (that returns only one element) like all(), sum() or mean() return tensors with only one element calles rank-0 tensors. If you want to convert it to a plain Python boolean or number, you need to call .item(). . (a + b).mean().item() . 9.666666984558105 . The elementwise operations work on tensors of any ranks, as long as they have the same shape. . m = tensor([[1., 2, 3], [4,5,6], [7,8,9]]) m*m . tensor([[ 1., 4., 9.], [16., 25., 36.], [49., 64., 81.]]) . However you can&#39;t have element-wise operations of tensors that don&#39;t have the same shape (unless they are broadcastable, see below). . n = tensor([[1., 2, 3], [4,5,6]]) m*n . RuntimeError Traceback (most recent call last) &lt;ipython-input-12-add73c4f74e0&gt; in &lt;module&gt; 1 n = tensor([[1., 2, 3], [4,5,6]]) -&gt; 2 m*n RuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0 . With element-wise arithmetic, we can remove one of our three nested loops: we can multiply the tensors that correspond to the i-th row of a and the j-th column of b before summing all the elements, which will speed up things because the inner loop will now be executed by PyTorch at C speed. . To access one row/column, we can simply write a[i,:] or b[:,j]. The column means take everything in that dimension. We could restrict and only take a slice on this particular dimension by passing a range like 1:5 instead of just :. In that case, we would take the elements in column 1 to 4 (the last part is always excluded). . One simplification is that we can always omit trailing columns, so a[i,:] can be abbreviated to a[i]. With all of that, we can write a new version of our matrix multiplication: . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): for j in range(bc): c[i,j] = (a[i] * b[:,j]).sum() return c . %timeit -n 20 t3 = matmul(m1,m2) . 1.7 ms ± 88.1 µs per loop (mean ± std. dev. of 7 runs, 20 loops each) . We are already ~700 times faster, just by removing that inner for loop! And that is just the beginning. By combining this with broadcasting, we can remove another loop and get an even more important speed-up. . Broadcasting . As we discussed in &lt;&gt;, broadcasting is a term introduced by the numpy library that describes how tensor of different ranks are treated during arithmetic operations. For instance, it&#39;s obvious there is no way to add a 3 by 3 matrix with a 4 by 5 matrix, but what if we want to add one scalar (which can be represented as a 1 by 1 tensor) with a matrix? Or a vector of size 3 with a 3 by 4 matrix? In both cases, we can find a way to make sense of what the operation could be.&lt;/p&gt; Broadcasting gives specific rules to codify when shapes are compatible when trying to do an element-wise operation, and how the tensor of the smaller shape is expanded to match the tensor of the bigger shape. It&#39;s essential to master those rules if you want to be able to write code that executes quickly. In this section, we&#39;ll expand our previous treatment of broadcasting to understand these rules. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Broadcasting with a scalar . Broadcasting with a scalar is the easiest broadcating: when we have a tensor a and a scalar, we just imagine a tensor of the same shape as a filled with that scalar and perform the operation. . a = tensor([10., 6, -4]) a &gt; 0 . tensor([ True, True, False]) . How are we able to do this comparison? 0 is being broadcast to have the same dimensions as a. Note that this is done without creating a tensor full of zeros in memory (that would be very inefficient). . This is very useful if you want to normalize your dataset by subtracting the mean (a scalar) from the entire data set (a matrix) and dividing by the standard deviation (another scalar): . m = tensor([[1., 2, 3], [4,5,6], [7,8,9]]) (m - 5) / 2.73 . tensor([[-1.4652, -1.0989, -0.7326], [-0.3663, 0.0000, 0.3663], [ 0.7326, 1.0989, 1.4652]]) . Now you could have different means for each row of the matrix, in which case you would need to broadcast a vector to a matrix. . Broadcasting a vector to a matrix . We can also broadcast a vector to a matrix: . c = tensor([10.,20,30]) m = tensor([[1., 2, 3], [4,5,6], [7,8,9]]) m.shape,c.shape . (torch.Size([3, 3]), torch.Size([3])) . m + c . tensor([[11., 22., 33.], [14., 25., 36.], [17., 28., 39.]]) . Here the elements of c are expanded to make three rows that match, and this way the operation is possible. Again, behind the scenes PyTorch doesn&#39;t create three copies of c in memory. This is done by the expand_as method behind the scenes: . c.expand_as(m) . tensor([[10., 20., 30.], [10., 20., 30.], [10., 20., 30.]]) . If we look at the corresponding tensor, we can ask for its storage property (which shows the actual contents of the memory used for the tensor) to check there is no useless data stored: . t = c.expand_as(m) t.storage() . 10.0 20.0 30.0 [torch.FloatStorage of size 3] . Even if it has officially 9 elements, the memory used is only 3 scalars. It&#39;s possible with a clever trick by giving a stride of 0 on that dimension (which means that when it looks for the next row by adding the stride, it doesn&#39;t move). . t.stride(), t.shape . ((0, 1), torch.Size([3, 3])) . Since m is of size 3 by 3, there were two ways to do broadcasting. The fact it was done on the last dimension is a convention that comes from the rules of broadcasting and has nothing to do with the way we ordered our tensors: . c + m . tensor([[11., 22., 33.], [14., 25., 36.], [17., 28., 39.]]) . We get the same result. In fact it&#39;s only possible to broadcast a vector of size n with a matrix of size m by n: . c = tensor([10.,20,30]) m = tensor([[1., 2, 3], [4,5,6]]) c+m . tensor([[11., 22., 33.], [14., 25., 36.]]) . This won&#39;t work: . c = tensor([10.,20]) m = tensor([[1., 2, 3], [4,5,6]]) c+m . RuntimeError Traceback (most recent call last) &lt;ipython-input-25-64bbbad4d99c&gt; in &lt;module&gt; 1 c = tensor([10.,20]) 2 m = tensor([[1., 2, 3], [4,5,6]]) -&gt; 3 c+m RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 1 . If we want to broadcast in the other dimension, we have to change the shape of our vector to make it a 3 by 1 matrix. This is done with the unsqueeze method in PyTorch. . c = tensor([10.,20,30]) m = tensor([[1., 2, 3], [4,5,6], [7,8,9]]) c = c.unsqueeze(1) m.shape,c.shape . (torch.Size([3, 3]), torch.Size([3, 1])) . And this time, c is expanded on the columns side. . c+m . tensor([[11., 12., 13.], [24., 25., 26.], [37., 38., 39.]]) . Like before the corresponding storage contains only three scalars. . t = c.expand_as(m) t.storage() . 10.0 20.0 30.0 [torch.FloatStorage of size 3] . And the expanded tensor has the right shape by giving it a stride of 0 on the column dimension. . t.stride(), t.shape . ((1, 0), torch.Size([3, 3])) . The way broadcasting works is that if we need to add dimensions, the default is to add them at the beginning. When we were broadcasting before, it was doing c.unsqueeze(0) behind the scenes. . c = tensor([10.,20,30]) c.shape, c.unsqueeze(0).shape,c.unsqueeze(1).shape . (torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1])) . The unsqueeze command can be replaced by None indexing. . c.shape, c[None,:].shape,c[:,None].shape . (torch.Size([3]), torch.Size([1, 3]), torch.Size([3, 1])) . You can always omit traiiling columns, and ... means all preceding dimensions: . c[None].shape,c[...,None].shape . (torch.Size([1, 3]), torch.Size([3, 1])) . With this, we can remove another for loop in our matrix multiplication function: instead of multiplying a[i] with b[:,j], we can multiply a[i] with the whole matrix b using broadcasting, then sum all the results. . def matmul(a,b): ar,ac = a.shape br,bc = b.shape assert ac==br c = torch.zeros(ar, bc) for i in range(ar): # c[i,j] = (a[i,:] * b[:,j]).sum() # previous c[i] = (a[i ].unsqueeze(-1) * b).sum(dim=0) return c . %timeit -n 20 t4 = matmul(m1,m2) . 357 µs ± 7.2 µs per loop (mean ± std. dev. of 7 runs, 20 loops each) . We&#39;re now 3,700 times faster than our first implementation! Now let&#39;s discuss the exact rules of broadcasting. . Broadcasting Rules . When operating on two tensors, PyTorch compares their shapes element-wise. It starts with the trailing dimensions, and works its way backward, adding 1 when it meets empty dimensions. Two dimensions are compatible when . they are equal, or | one of them is 1, in which case that dimension is broadcasted to make it the same size | . Arrays do not need to have the same number of dimensions. For example, if you have a 256*256*3 array of RGB values, and you want to scale each color in the image by a different value, you can multiply the image by a one-dimensional array with 3 values. Lining up the sizes of the trailing axes of these arrays according to the broadcast rules, shows that they are compatible: . Image (3d tensor): 256 x 256 x 3 Scale (1d tensor): (1) (1) 3 Result (3d tensor): 256 x 256 x 3 . However, a 2d tensor of size 256 x 256 isn&#39;t compatible with our image. . Image (3d tensor): 256 x 256 x 3 Scale (1d tensor): (1) 256 x 256 Error . In the first examples we had with a 3x3 matrix and vector of size 3, broadcast is done on the rows: . Matrix (2d tensor): 3 x 3 Vector (1d tensor): (1) 3 Result (2d tensor): 3 x 3 . As a little exercise around those rules, try to determine what dimensions to add (and where) when you need to normalize a batch of images of size 64 x 3 x 256 x 256 with vectors of three elements (one for the mean and one for the standard deviation). . Another useful thing for tensor manipulations is the use of Einstein summations. . Einstein summation . Before using the PyTorch operation @ or torch.matmul, there is a last way we can implement this matrix multiplication: einstein summation (einsum). This is a compact representation for combining products and sums in a general way. We write an equation like this . ik,kj -&gt; ij . The left hand side represents the operands dimensions, separated by commas. Here we have two tensors taht each have two dimensions (i,k and k,j). The right hand side represents the result dimensions, so here we have a tensor with two dimensions i,j. . There are essentially three rules of Einstein summation notation, namely: . Repeated indices are implicitly summed over. | Each index can appear at most twice in any term. | Each term must contain identical non-repeated indices. | So in the example above, since k is repeated, we sum over that index. In the end the above formula represents the matrix obtained when we put in (i,j) the sum of all the coefficients (i,k) in the first tensor multiplied by the coefficients (k,j) in the second tensor... which is the matrix product! . def matmul(a,b): return torch.einsum(&#39;ik,kj-&gt;ij&#39;, a, b) . Einstein summation is a very practical way of expressing operations involving indexing and sum of products. Note that you can have only one member in the left hand side. For instance . torch.einsum(&#39;ij-&gt;ji&#39;, a) . returns the transpose of the matrix a. You can also have three or more members: . torch.einsum(&#39;bi,ij,bj-&gt;b&#39;, a, b, c) . will return a vector of size b where the k-th coordinate is the sum of the a[k,i] b[i,j] c[k,j]. This notation is getting really convenient when you have more dimensions because of batches, for instance if you have two batches of matrices and want compute the matrix product per batch, you would go: . torch.einsum(&#39;bik,bkj-&gt;bij&#39;, a, b) . %timeit -n 20 t5 = matmul(m1,m2) . 68.7 µs ± 4.06 µs per loop (mean ± std. dev. of 7 runs, 20 loops each) . As we see, not only is it practical, but it&#39;s very fast. einsum is often the fastest way to do custom operations in PyTorch, without diving into C++ and CUDA. (But it&#39;s generally not as fast as carefully optimized CUDA code, as you see in the matrix multiplication example). . Now that we know how to implement a matrix multiplication from scratch, we are ready to build our neural net, specifically its forward and backward passes, just using matrix multiplications. . The forward and backward passes . As we saw in &lt;&gt;, to train it, we will need to compute all the gradients of a given a loss with respect to its parameters, which is known as the backward pass. The forward pass is computing the output of the model on a given input, which is just based on the matrix products we saw. As we define our first neural net, we will also delve in the problem of properly initializing the weights, which is crucial to make training start properly.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Defining and initializing a layer . We will take the example of a two-layer neural net first. As we saw, one layer can be expressed as y = x @ w + b with x out inputs, y our outputs, w the weights of the layer (which is of size number of inputs by neuron of neurons if we don&#39;t transpose like before) and b is the bias vector. . def lin(x, w, b): return x @ w + b . We can stack two layers on top of the other, but since mathematically, the composition of two linear operations is another linear operation, this only makes sense if we put something non-linear in the middle called an activation function. The activation function most popularly used is a ReLU, which, as we saw, is just the maximum of x and 0. . We won&#39;t actually train our model in this chapter so we use random tensors for our inputs and targets. Let&#39;s say our inputs are 200 vectors of size 100, which we group in one batch, and our targets are 200 random floats. . x = torch.randn(200, 100) y = torch.randn(200) . For our two-layers model we will need two weight matrices and two bias vectors. Let&#39;s say we have a hidden size of 50 and the output size is 1 (for one of our input, the corresponding output is one float in this toy example). We initialize the weights randomly and the bias at zero. . w1 = torch.randn(100,50) b1 = torch.zeros(50) w2 = torch.randn(50,1) b2 = torch.zeros(1) . Then the result of our first layer is simply: . l1 = lin(x, w1, b1) l1.shape . torch.Size([200, 50]) . Note that this formula works with our batch of inputs, and returns a batch of hidden state: l1 is a matrix of 200 (our batch size) by 50 (our hidden size). . There is a problem with the way our model was initialized however. To understand it, we need to look at the mean and standard deviation (std) of l1. . l1.mean(), l1.std() . (tensor(0.0019), tensor(10.1058)) . The mean is close to zero, which is understandable since both our input and weight matrix have a mean close to zero. However the standard deviation, which represents how far away our activation go from the mean, went from 1 to 10. This is a really big problem because that&#39;s with just one layer. Modern neural nets can have hundred of layers, so if each of them multiply the scale of our activations by 10, by the end of the last layer we won&#39;t have numbers representable by a computer. . Indeed, if we make just 50 multiplications between x and random matrices of size 100 x 100: . x = torch.randn(200, 100) for i in range(50): x = x @ torch.randn(100,100) x[0:5,0:5] . tensor([[nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan], [nan, nan, nan, nan, nan]]) . The result is nans everywhere. So maybe the scale of our matrix was too big, and we need to have smaller weights. But if we use too small weights we will have the opposite problem: the scale of our activations will get from 1 to 0.1 and after 100 layers, we&#39;ll be left with zeros everywhere. . x = torch.randn(200, 100) for i in range(50): x = x @ (torch.randn(100,100) * 0.01) x[0:5,0:5] . tensor([[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]) . So we have to scale our weights matrices exactly right so that the standard deviation of our activations stays at 1. We can compute the exact value mathematically, and this has been done by Xavier Glorot and Yoshua Bengio in Understanding the difficulty of training deep feedforward neural networks. The right scale for a given layer is $1/ sqrt{n_{in}}$, where $n_{in}$ represents the number of inputs. . In our case, if we have 100 inputs, we should scale our weight matrices by 0.1: . x = torch.randn(200, 100) for i in range(50): x = x @ (torch.randn(100,100) * 0.1) x[0:5,0:5] . tensor([[ 0.7554, 0.6167, -0.1757, -1.5662, 0.5644], [-0.1987, 0.6292, 0.3283, -1.1538, 0.5416], [ 0.6106, 0.2556, -0.0618, -0.9463, 0.4445], [ 0.4484, 0.7144, 0.1164, -0.8626, 0.4413], [ 0.3463, 0.5930, 0.3375, -0.9486, 0.5643]]) . Finally some numbers that are neither zeros nor infinity! Notice how stable the scale of our activations is, even after those 50 fake layers: . x.std() . tensor(0.7042) . You can play a little bit with the values of the scale and notice that even a slight variation from 0.1 will get you either to very small or very large numbers, so initializing the weights properly is extremely important. Let&#39;s go back to our neural net. Since we messed a bit with our inputs we need to redefine them: . x = torch.randn(200, 100) y = torch.randn(200) . and for our weights, we use the right scale, which is known as Xavier initialization (or Glorot initialization). . from math import sqrt w1 = torch.randn(100,50) / sqrt(100) b1 = torch.zeros(50) w2 = torch.randn(50,1) / sqrt(50) b2 = torch.zeros(1) . Now if compute the result of the first layer, we can check the mean and standard deviation are under control: . l1 = lin(x, w1, b1) l1.mean(),l1.std() . (tensor(-0.0050), tensor(1.0000)) . Very good, now we need to go through a relu, so let&#39;s define one. A relu removes the negatives and replace them by 0, which is another way of saying it clamps our tensor at 0. . def relu(x): return x.clamp_min(0.) . Now let&#39;s make our activations go through a relu . l2 = relu(l1) l2.mean(),l2.std() . (tensor(0.3961), tensor(0.5783)) . And we&#39;re back to square one: the mean of our activation has gone to 0.4 (which is understandable since we removed the negatives) and the std went down to 0.5-0.6. So like before, after a few layers we will probably get to 0: . x = torch.randn(200, 100) for i in range(50): x = relu(x @ (torch.randn(100,100) * 0.1)) x[0:5,0:5] . tensor([[0.0000e+00, 1.9689e-08, 4.2820e-08, 0.0000e+00, 0.0000e+00], [0.0000e+00, 1.6701e-08, 4.3501e-08, 0.0000e+00, 0.0000e+00], [0.0000e+00, 1.0976e-08, 3.0411e-08, 0.0000e+00, 0.0000e+00], [0.0000e+00, 1.8457e-08, 4.9469e-08, 0.0000e+00, 0.0000e+00], [0.0000e+00, 1.9949e-08, 4.1643e-08, 0.0000e+00, 0.0000e+00]]) . So our initialization wasn&#39;t right. This is because at the same the previous article was written, the popular activation in a neural net was the hyperbolic tangent (which is the one they use) and that initialization doesn&#39;t account for our ReLU. Fortunately someone else has done the math for us and computed the right scale we should use. Kaiming He et al. in Delving Deep into Rectifiers: Surpassing Human-Level Performance (which we&#39;ve seen before--it&#39;s the article that introduced the ResNet) show we should use the following scale instead: $ sqrt{2 / n_{in}}$ where $n_{in}$ is the number of inputs of our model. . x = torch.randn(200, 100) for i in range(50): x = relu(x @ (torch.randn(100,100) * sqrt(2/100))) x[0:5,0:5] . tensor([[0.2871, 0.0000, 0.0000, 0.0000, 0.0026], [0.4546, 0.0000, 0.0000, 0.0000, 0.0015], [0.6178, 0.0000, 0.0000, 0.0180, 0.0079], [0.3333, 0.0000, 0.0000, 0.0545, 0.0000], [0.1940, 0.0000, 0.0000, 0.0000, 0.0096]]) . And indeed if we use it we can check our numbers aren&#39;t all zeroed this time. So let&#39;s go back to the definition of our neural net and use this initialization (which is named Kaiming initialization or He initialization). . x = torch.randn(200, 100) y = torch.randn(200) . w1 = torch.randn(100,50) * sqrt(2 / 100) b1 = torch.zeros(50) w2 = torch.randn(50,1) * sqrt(2 / 50) b2 = torch.zeros(1) . Now after going through the first linear layer and relu, let&#39;s look at the scale of our activations: . l1 = lin(x, w1, b1) l2 = relu(l1) l2.mean(), l2.std() . (tensor(0.5661), tensor(0.8339)) . Now that our weights are properly initialized, we can define our whole model: . def model(x): l1 = lin(x, w1, b1) l2 = relu(l1) l3 = lin(l2, w2, b2) return l3 . This is the forward pass, now all there is left to do is to compare our output to the labels we have (random numbers, in this example) with a loss function. In this case, we will use the mean squared error. (It&#39;s a toy problem in any case and this is the easiest loss function to use for what is next, computing the gradients.) . The only subtlety is that our output and target don&#39;t have exactly the same shape: after going though the model, we get an output like this. . out = model(x) out.shape . torch.Size([200, 1]) . To get rid of this trailing 1 dimension, we use the squeeze function. . def mse(output, targ): return (output.squeeze(-1) - targ).pow(2).mean() . And now we are ready to compute our loss. . loss = mse(out, y) . That is all for the forward pass, let now look at the gradients. . Gradients and backward pass . We&#39;ve seen that PyTorch computes all the gradient we need with a magic call to loss.backward() but how is it done behind the scenes? . Now comes the part where we need to compute the gradients of the loss with respect to all the weights of our model, so all the floats in w1, b1, w2 and b2. For this, we will need a bit of math, specifically the chain rule. If you don&#39;t remember it from high school, this is the rule of calculus that guides how we can compute the derivative of a composed function: . $$(g circ f)&#39;(x) = g&#39;(f(x)) f&#39;(x)$$ j: I find this notation very hard to wrap my head around, so instead I like to think of it as: if y = g(u) and u=f(x); then dy/dx = dy/du * du/dx. The two notations mean the same thing, so use whatever works for you. . Our loss is a big composition of different functions: mean-squared error (which is in turn the composition of a mean and a power of two), the second linear layer, a relu and the first linear layer. For instance, we want the gradients of the loss with respect to b2 and our loss is defined by . loss = mse(out,y) = mse(lin(l2, w2, b2), y) . The chain rule tells us that we have $$ frac{ text{d} loss}{ text{d} b_{2}} = frac{ text{d} loss}{ text{d} out} times frac{ text{d} out}{ text{d} b_{2}} = frac{ text{d}}{ text{d} out} mse(out, y) times frac{ text{d}}{ text{d} b_{2}} lin(l_{2}, w_{2}, b_{2})$$ . To compute the gradients of the loss with respect to $b_{2}$, we first need the gradients of the loss with respect to our output $out$. It&#39;s the same if we want the gradients of the loss with respect to $w_{2}$. Then, to get the gradients of the loss with respect to $b_{1}$ or $w_{1}$, we will need the gradients of the loss with respect to $l_{1}$, which in turn requires the gradients of the loss with respect to $l_{2}$, which will need the gradients of the loss with respect to $out$. . So to compute all the gradients we need for the update, we need to begin from the output of the model and work our way backward, one layer after the other, which is why this step is known as backpropagation. We can automate it by having each function we implemented (relu, mse, lin) provide its backward step, that is how to derive the gradients of the loss with respect to the input(s) from the gradient of the loss with respect to the output. . Here we populate those gradients in an attribute of each tensor, a bit like PyTorch does with .grad. . The first are the gradients of the loss with respect to the output of our model (which is the input of the loss function). We have to undo the squeeze we did in mse then we use the formula that gives us the derivative of $x^{2}$: $2x$. The derivative of the mean is just 1/n where n is the number of elements in our input. . def mse_grad(inp, targ): # grad of loss with respect to output of previous layer inp.g = 2. * (inp.squeeze() - targ).unsqueeze(-1) / inp.shape[0] . For the gradients of the relu and our linear layer, we use the gradients of the loss with respect to the output (in out.g) and apply the chain rule to compute the gradients of the loss with respect to the output (in inp.g). The chain rule tells us that inp.g = relu&#39;(inp) * out.g. The derivative of relu is either 0 (when inputs are negative) or 1 (when inputs are positive), so this gives us: . def relu_grad(inp, out): # grad of relu with respect to input activations inp.g = (inp&gt;0).float() * out.g . The scheme is the same to compute the gradients of the loss with respect to the inputs, weights and bias in the linear layer. We won&#39;t linger on the mathematical formulas that define them since they&#39;re not important for our purposes--but do check out Khan Academy&#39;s excellent calculus lessons if you&#39;re interested in this topic. . def lin_grad(inp, out, w, b): # grad of matmul with respect to input inp.g = out.g @ w.t() w.g = inp.t() @ out.g b.g = out.g.sum(0) . Sidebar: SymPy . An extremely useful library for working with calculus is SymPy. SymPy is a library for symbolic computation, which is defined in the SymPy documentation: . : Symbolic computation deals with the computation of mathematical objects symbolically. This means that the mathematical objects are represented exactly, not approximately, and mathematical expressions with unevaluated variables are left in symbolic form. . To do symbolic computation, first define a symbol, and then do a computation, like so: . from sympy import symbols,diff sx,sy = symbols(&#39;sx sy&#39;) diff(sx**2, sx) . $ displaystyle 2 sx$ Here, SymPy has taken the derivative of x**2 for us! SymPy can take the derivative of complicated compound expressions, and can also simplify and factor equations, and much more. There&#39;s really not much reason for anyone to do calculus manually nowadays--for calculating gradients, PyTorch does it for us, and for showing the equation, SymPy does it for us! . End sidebar . Once we have have defined those functions we can use them to write the backward pass. Since each gradient is automatically populated in the right tensor, we don&#39;t need to store the results of those _grad functions anywhere, we just need to execute them in the reverse order as the forward pass, to make sure that in each function, out.g exists. . def forward_and_backward(inp, targ): # forward pass: l1 = inp @ w1 + b1 l2 = relu(l1) out = l2 @ w2 + b2 # we don&#39;t actually need the loss in backward! loss = mse(out, targ) # backward pass: mse_grad(out, targ) lin_grad(l2, out, w2, b2) relu_grad(l1, l2) lin_grad(inp, l1, w1, b1) . And now we can access to the gradients of our model parameters in w1.g, b1.g, w2.g, b2.g. . We have sucessfuly defined our model, now let&#39;s make it a bit more like a PyTorch module. . Refactor the model . The three functions we used have two associated functions: a forward pass and a backward pass. Instead of writing them separately, we can create a class to wrap them together. That class can also store the inputs and outputs for the backward pass, this way we will just have to call backward(). . class Relu(): def __call__(self, inp): self.inp = inp self.out = inp.clamp_min(0.) return self.out def backward(self): self.inp.g = (self.inp&gt;0).float() * self.out.g . The __call__ name is a magic name in PyThon that will make our class callable. This what will be executed when we type y = Relu()(x). We can do the same for our linear layer and the MSE loss. . class Lin(): def __init__(self, w, b): self.w,self.b = w,b def __call__(self, inp): self.inp = inp self.out = inp@self.w + self.b return self.out def backward(self): self.inp.g = self.out.g @ self.w.t() self.w.g = self.inp.t() @ self.out.g self.b.g = self.out.g.sum(0) . class Mse(): def __call__(self, inp, targ): self.inp = inp self.targ = targ self.out = (inp.squeeze() - targ).pow(2).mean() return self.out def backward(self): x = (self.inp.squeeze()-self.targ).unsqueeze(-1) self.inp.g = 2.*x/self.targ.shape[0] . Then we can put everything in a model that we initiate with our tensors w1, b1, w2, b2. . class Model(): def __init__(self, w1, b1, w2, b2): self.layers = [Lin(w1,b1), Relu(), Lin(w2,b2)] self.loss = Mse() def __call__(self, x, targ): for l in self.layers: x = l(x) return self.loss(x, targ) def backward(self): self.loss.backward() for l in reversed(self.layers): l.backward() . What is really nice about this refactoring and registering things as layers of our model is that the forward and backward pass are now really easy to write. If we want to instantiate our model, we just need to write: . model = Model(w1, b1, w2, b2) . The forward pass would then be executed with: . loss = model(x, y) . And the backward pass with: . model.backward() . Going to PyTorch . The three classes we wrote for Lin, Mse and Relu have a lot in common, so we could make them all inherit from the same basic class. . class LayerFunction(): def __call__(self, *args): self.args = args self.out = self.forward(*args) return self.out def forward(self): raise Exception(&#39;not implemented&#39;) def bwd(self): raise Exception(&#39;not implemented&#39;) def backward(self): self.bwd(self.out, *self.args) . Then we just need to implement forward and bwd in each of our subclass. . class Relu(LayerFunction): def forward(self, inp): return inp.clamp_min(0.) def bwd(self, out, inp): inp.g = (inp&gt;0).float() * out.g . class Lin(LayerFunction): def __init__(self, w, b): self.w,self.b = w,b def forward(self, inp): return inp@self.w + self.b def bwd(self, out, inp): inp.g = out.g @ self.w.t() self.w.g = self.inp.t() @ self.out.g self.b.g = out.g.sum(0) . class Mse(LayerFunction): def forward (self, inp, targ): return (inp.squeeze() - targ).pow(2).mean() def bwd(self, out, inp, targ): inp.g = 2*(inp.squeeze()-targ).unsqueeze(-1) / targ.shape[0] . Then our model can be the same as before. This is getting closer and closer to what PyTorch does. Each basic function we need to differentiate is written as a torch.autograd.Function object that has a forward and a backward method. PyTorch will then keep trace of any computation we do to be able to properly run the backward pass unless we set the requires_grad attribute of our tensors to False. . Writing one is (almost) as easy as we had before. The difference is that we choose what to save and what to put in a context variable (so that we make sure we don&#39;t save anything we don&#39;t need) and that we return the gradients in the backward pass. It&#39;s very rare to have to write your own Function but if you ever need something exotic or want to mess with the gradients of a regular function, here is how we write one: . from torch.autograd import Function class MyRelu(Function): @staticmethod def forward(ctx, i): result = i.clamp_min(0.) ctx.save_for_backward(i) return result @staticmethod def backward(ctx, grad_output): i, = ctx.saved_tensors return grad_output * (i&gt;0).float() . Then the structure used to build a more complex model that takes advantage of those functions is a torch.nn.Module. This is the base structure for all models and all the neural nets you have seen up until now where from that class. It mostly helps to register all the trainable parameters, which as we&#39;ve seen can be used in the training loop. . To implement a nn.Module you just need to . Make sure the superclass __init__ is called first when you initiliaze it, | Define any parameter of the model as attributes with nn.Parameter, | Define a forward function that returns the output of your model. | . As an example, here is the linear layer from scratch: . import torch.nn as nn class LinearLayer(nn.Module): def __init__(self, n_in, n_out): super().__init__() self.weight = nn.Parameter(torch.randn(n_out, n_in) * sqrt(2/n_in)) self.bias = nn.Parameter(torch.zeros(n_out)) def forward(self, x): return x @ self.weight.t() + self.bias . As you see, this class automatically keeps track of what parameters have been defined: . lin = LinearLayer(10,2) p1,p2 = lin.parameters() p1.shape,p2.shape . (torch.Size([2, 10]), torch.Size([2])) . It is thanks to this feature of nn.Module that we can just say opt.step() and have an optimizer loop through the parameters and update each one. . Note that in PyTorch, the weights are stored as an n_out x n_in matrix, which is why we have the transpose in the forward pass. . By using the linear layer from PyTorch (which uses the Kaiming initialization as well), the model we have seen during this chapter can be written like this: . class Model(nn.Module): def __init__(self, n_in, nh, n_out): super().__init__() self.layers = nn.Sequential( nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)) self.loss = mse def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ) . fastai provides its own variant of Module which is identical to nn.Module, but doesn&#39;t require you to call super().__init__() (it does that for you automatically): . class Model(Module): def __init__(self, n_in, nh, n_out): self.layers = nn.Sequential( nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)) self.loss = mse def forward(self, x, targ): return self.loss(self.layers(x).squeeze(), targ) . In the next chapter, we will start from such a model and see how we build a training loop from scratch and refactor it to what we&#39;ve been using in previous chapters. . Things to remember . TK tweak this and make it a real conclusion . A neural net is basically a bunch of matrix multiplications with non-linearities in-between. | Python is slow so to write fast code we have to vectorize it and take advantage of element-wise arithmetic or broadcasting. | Two tensors are broadcastable if the dimensions starting from the end and going backward match (they are the same or one of them is 1). To make tensors broadcastable, we may need to add dimensions of size 1 with unsqueeze or a None index. | Properly initializing a neural net is crucial to get training started. Kaiming initialization should be used when we have ReLU non-linearities. | The backward pass is the chain rule applied multiple times, computing the gradients from the output of our model and going back, one layer at a time. | When subclassing nn.Module (if not using fastai&#39;s Module) we have to call the superclass __init__ method in our __init__ method and we have to define a forward function that takes an input and returns the desired result. | . Questionnaire . Write the Python code to implement a single neuron. | Write the Python code to implement ReLU. | Write the Python code for a dense layer in terms of matrix multiplication. | Write the Python code for a dense layer in plain Python (that is with list comprehensions and functionality built into Python). | What is the hidden size of a layer? | What does the t method to in PyTorch? | Why is matrix multiplication written in plain Python very slow? | In matmul, why is ac==br? | In Jupyter notebook, how do you measure the time taken for a single cell to execute? | What is elementwise arithmetic? | Write the PyTorch code to test whether every element of a is greater than the corresponding element of b. | What is a rank-0 tensor? How do you convert it to a plain Python data type? | What does this return, and why?: tensor([1,2]) + tensor([1]) | What does this return, and why?: tensor([1,2]) + tensor([1,2,3]) | How does elementwise arithmetic help us speed up matmul? | What are the broadcasting rules? | What is expand_as? Show an example of how it can be used to match the results of broadcasting. | How does unsqueeze help us to solve certain broadcasting problems? | How can you use indexing to do the same operation as unsqueeze? | How do we show the actual contents of the memory used for a tensor? | When adding a vector of size 3 to a matrix of size 3 x 3, are the elements of the vector added to each row, or each column of the matrix? (Be sure to check your answer by running this code in a notebook.) | Do broadcasting and expand_as result in increased memory use? Why or why not? | Implement matmul using Einstein summation. | What does a repeated index letter represent on the left-hand side of einsum? | What are the three rules of Einstein summation notation? Why? | What is the forward pass, and the backward pass, of a neural network? | Why do we need to store some of the activations calculated for intermediate layers in the forward pass? | What is the downside of having activations with a standard deviation too far away from one? | How can weight initialisation help avoid this problem? | What is the formula to initialise weights such that we get a standard deviation of one, for a plain linear layer; for a linear layer followed by ReLU? | Why do we sometimes have to use the squeeze method in loss functions? | What does the argument to the squeeze method do? Why might it be important to include this argument, even though PyTorch does not require it? | What is the chain rule? Show the equation in either of the two forms shown in this chapter. | Show how to calculate the gradients of mse(lin(l2, w2, b2), y) using the chain rule. | What is the gradient of relu? Show in math or code. (You shouldn&#39;t need to commit this to memory—try to figure it using your knowledge of the shape of the function.) | In what order do we need to call the *_grad functions in the backward pass? Why? | What is __call__? | What methods do we need to implement when writing a torch.autograd.Function? | Write nn.Linear from scratch, and test it works. | What is the difference between nn.Module and fastai&#39;s Module? | Further research . Implement relu as a torch.autograd.Function and train a model with it. | If you are mathematically inclined, find out what the gradients of a linear layer are in maths notation. Map that to the implementation we saw in this chapter. | Learn about the unfold method in PyTorch, and use it along with matrix multiplication to implement your own 2d convolution function, and train a CNN that uses it. | Implement all what is in this chapter using numpy instead of PyTorch. | &lt;/div&gt; .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_foundations.html",
            "relUrl": "/2020/03/19/_foundations.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Chapter 13",
            "content": "[[chapter_convolutions]] Convolutional neural networks . In &lt;&gt; we learned how to create a neural network recognising images. We were able to achieve a bit over 98% accuracy at recognising threes from sevens. But we also saw that fastai&#39;s built in classes were able to get close to 100%. Let&#39;s start trying to close the gap.&lt;/p&gt; In this chapter, we will start by digging into what convolutions are and build a CNN from scratch. We will then study a range of techniques to improve training stability and learn all the tweaks the library usually applies for us to get great results. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The magic of convolutions . One of the most powerful tools that machine learning practitioners have at their disposal is feature engineering. A feature is a transformation of the data which is designed to make it easier to model. For instance, the add_datepart function that we used for our tabular dataset preprocessing added date features to the Bulldozers dataset. What kind of features might we be able to create from images? . jargon: Feature engineering: creating new transformations of the input data in order to make it easier to model. . In the context of an image, a feature will be a visually distinctive attribute of an image. Here&#39;s an idea: the number seven is characterised by a horizontal edge near the top of the digit, and a bottom left to top right diagonal edge underneath that. On the other hand, the number three is characterised by a diagonal edge in one direction in the top left and bottom right of the digit, the opposite diagonal on the bottom left and top right, a horizontal edge in the middle of the top and the bottom, and so forth. So what if we could extract information about where the edges occur in each image, and then use that as our features, instead of raw pixels? . It turns out that finding the edges in an image is a very common task in computer vision, and is surprisingly straightforward. To do it, we use something called a convolution. A convolution requires nothing more than multiplication, and addition — two operations which are responsible for the vast majority of work that we will see in every single deep learning model in this book! . A convolution applies a kernel across an image. A kernel is a little matrix, such as the 3x3 matrix in the top right of &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Applying a kernel to one location . The 7x7 grid to the left is our image we&#39;re going to apply the kernel to. The convolution operation multiplies each element of the kernel, to each element of a 3x3 block of the image. The results of these multiplications are then added together. The diagram above shows an example of applying a kernel to a single location in the image, the 3x3 block around cell 18. . Let&#39;s do this with code. First, we create a little 3x3 matrix like so: . top_edge = tensor([[-1,-1,-1], [ 0, 0, 0], [ 1, 1, 1]]).float() . We&#39;re going to call this our kernel (because that&#39;s what fancy computer vision researchers call these). And we&#39;ll need an image, of course: . path = untar_data(URLs.MNIST_SAMPLE) . im3 = Image.open(path/&#39;train&#39;/&#39;3&#39;/&#39;12.png&#39;) show_image(im3); . Now we&#39;re going to take the top 3x3 pixel square of our image, and we&#39;ll multiply each of those by each item in our kernel. Then we&#39;ll add them up. Like so: . im3_t = tensor(im3) im3_t[0:3,0:3] * top_edge . tensor([[-0., -0., -0.], [0., 0., 0.], [0., 0., 0.]]) . (im3_t[0:3,0:3] * top_edge).sum() . tensor(0.) . Not very interesting so far - they are all white pixels in the top left corner. But let&#39;s pick a couple of more interesting spots: . #hide_output df = pd.DataFrame(im3_t[:10,:20]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 0 | 0 | 0 | 12 | 99 | 91 | 142 | 155 | 246 | 182 | 155 | 155 | 155 | 155 | 131 | 52 | 0 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 138 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 254 | 252 | 210 | 122 | 33 | 0 | . 7 0 | 0 | 0 | 220 | 254 | 254 | 254 | 235 | 189 | 189 | 189 | 189 | 150 | 189 | 205 | 254 | 254 | 254 | 75 | 0 | . 8 0 | 0 | 0 | 35 | 74 | 35 | 35 | 25 | 0 | 0 | 0 | 0 | 0 | 0 | 13 | 224 | 254 | 254 | 153 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 90 | 254 | 254 | 247 | 53 | 0 | . . There&#39;s a top edge at cell 5,7. Let&#39;s repeat our calculation there: . (im3_t[4:7,6:9] * top_edge).sum() . tensor(762.) . There&#39;s a right edge at cell 8,18. What does that give us?: . (im3_t[7:10,17:20] * top_edge).sum() . tensor(-29.) . As you can see, this little calculation is returning a high number where the 3x3 pixel square represents a top edge (i.e. where there are low values at the top of the square, and high values immediately underneath). That&#39;s because the -1 values in our kernel have little impact in that case, but the 1 values have a lot. . Let&#39;s look a tiny bit at the math. The filter will take any window of size 3 by 3 in our images, and if we name the pixel values like this: . $$ begin{matrix} a1 &amp; a2 &amp; a3 a4 &amp; a5 &amp; a6 a7 &amp; a8 &amp; a9 end{matrix}$$it will return $a1+a2+a3-a7-a8-a9$. Now if we are in a part of the image where there $a1$, $a2$ and $a3$ are kind of the same as $a7$, $a8$ and $a9$, then the terms will cancel each other and we will get 0. However if $a1$ is greater than $a7$, $a2$ is greater than $a8$ and $a3$ is greater than $a9$, we will get a bigger number as a result. So this filter detects horizontal edges, more precisely edges where we go from bright parts of the image at the top to darker parts at the bottom. . Changing our filter to have the row of ones at the top and the -1 at the bottom would detect horizonal edges that go from dark to light. Putting the ones and -1 in columns versus rows would give us a filter that detect vertical edges. Each set of weights will produce a different kind of outcome. . Let&#39;s create a function to do this for one location, and check it matches our result from before: . def apply_kernel(row, col, kernel): return (im3_t[row-1:row+2,col-1:col+2] * kernel).sum() . apply_kernel(5,7,top_edge) . tensor(762.) . But note that we can&#39;t apply it to the corner (such as location 0,0), since there isn&#39;t a complete 3x3 square there. . Mapping a convolution kernel . We can map apply_kernel() across the coordinate grid. That is, we&#39;ll be taking our 3x3 kernel, and applying it to each 3x3 section of our image. For instance, here are the positions a 3x3 kernel can be applied to in the first row of a 5x5 image: . Applying a kernel across a grid . To get a grid of coordinates we can use a nested list comprehension, like so: . [[(i,j) for j in range(1,5)] for i in range(1,5)] . [[(1, 1), (1, 2), (1, 3), (1, 4)], [(2, 1), (2, 2), (2, 3), (2, 4)], [(3, 1), (3, 2), (3, 3), (3, 4)], [(4, 1), (4, 2), (4, 3), (4, 4)]] . . Note: Nested list comprehensions are used a lot in Python, so if you haven&#8217;t seen them before, take a few minutes to make sure you understand what&#8217;s happening here, and experiment with writing your own nested list comprehensions. . Here&#39;s the result of applying our kernel over a coordinate grid. . rng = range(1,27) top_edge3 = tensor([[apply_kernel(i,j,top_edge) for j in rng] for i in rng]) show_image(top_edge3); . Looking good! Our top edges are black, and bottom edges are white (since they are the opposite of top edges). Now that our image contains negative numbers too, matplotlib has automatically changed our colors, so that white is the smallest number in the image, black the highest, and zeros appear as grey. . We can try the same thing for left edges: . left_edge = tensor([[-1,1,0], [-1,1,0], [-1,1,0]]).float() left_edge3 = tensor([[apply_kernel(i,j,left_edge) for j in rng] for i in rng]) show_image(left_edge3); . This operation of applying a kernel over a grid in this way is called convolution. In the paper A guide to convolution arithmetic for deep learning there are many great diagrams showing how image kernels can be applied. Here&#39;s an example from the paper showing (at bottom) a light blue 4x4 image, with a dark blue 3x3 kernel being applied, creating a 2x2 green output activation map at the top. (We&#39;ll be using quite a few images from this paper in this book--when you see images in this style, you&#39;ll know they&#39;re from this great paper.) . Result of applying a 3x3 kernel to a 4x4 image . Look at the shape of the result. If the original image has a height of h and a width of w, how many 3 by 3 windows can we find? As you see from the example, there are h-2 by w-2 windows, so the image we get as a result as a height of h-2 and a witdh of w-2. . We won&#39;t implement this convolution function from scratch, but use PyTorch&#39;s implementation instead (it is way faster than anything we could do in Python). . Convolutions in PyTorch . Convolution is such an important and widely-used operation that PyTorch has it builtin. It&#39;s called F.conv2d (Recall F is a fastai import from torch.nn.functional as recommended by PyTorch). The PyTorch docs tell us that it includes these parameters: . input: input tensor of shape (minibatch, in_channels, iH, iW) | weight: filters of shape (out_channels, in_channels, kH, kW) | . Here iH,iW is the height and width of the image (i.e. 28,28), and kH,kW is the height and width of our kernel (3,3). But apparently PyTorch is expecting rank 4 tensors for both these arguments, but currently we only have rank 2 tensors (i.e. matrices, arrays with two axes). . The reason for these extra axes is that PyTorch has a few tricks up its sleeve. The first trick is that PyTorch can apply a convolution to multiple images at the same time. That means we can call it on every item in a batch at once! . The second trick is that PyTorch can apply multiple kernels at the same time. So let&#39;s create the diagonal edge kernels too, and then stack all 4 of our edge kernels into a single tensor: . diag1_edge = tensor([[ 0,-1, 1], [-1, 1, 0], [ 1, 0, 0]]).float() diag2_edge = tensor([[ 1,-1, 0], [ 0, 1,-1], [ 0, 0, 1]]).float() edge_kernels = torch.stack([left_edge, top_edge, diag1_edge, diag2_edge]) edge_kernels.shape . torch.Size([4, 3, 3]) . In order to test on a mini-batch, we&#39;ll need a DataLoader, and a sample mini-batch. Let&#39;s use the data block API: . mnist = DataBlock((ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(), get_y=parent_label) dls = mnist.dataloaders(path) xb,yb = first(dls.valid) xb.shape . torch.Size([64, 1, 28, 28]) . By default, fastai puts data on the GPU when using data blocks. Let&#39;s move it to the CPU for our examples: . xb,yb = to_cpu(xb),to_cpu(yb) . One batch contains 64 images, each of 1 channel, with 28x28 pixels. F.conv2d can handle multi-channel (e.g. colour) images. A channel is a single basic color in an image--for regular full color images there are 3 channels, red, green, and blue. PyTorch represents an image as a rank-3 tensor, with dimensions channels x rows x columns. . We&#39;ll see how to handle more than one channel later in this chapter. Kernels passed to F.conv2d need to be rank-4 tensors: channels_in x features_out x rows x columns. edge_kernels is currently missing one of these: the 1 for features_out. We need to tell PyTorch that the number of input channels in the kernel is one, by inserting an axis of size one (this is known as a unit axis) in the first location, since the PyTorch docs show that&#39;s where in_channels is expected. To insert a unit axis into a tensor, use the unsqueeze method: . edge_kernels.shape,edge_kernels.unsqueeze(1).shape . (torch.Size([4, 3, 3]), torch.Size([4, 1, 3, 3])) . This is now the correct shape for edge_kernels. Let&#39;s pass this all to conv2d: . edge_kernels = edge_kernels.unsqueeze(1) . batch_features = F.conv2d(xb, edge_kernels) batch_features.shape . torch.Size([64, 4, 26, 26]) . The output shape shows our 64 images in the mini-batch, 4 kernels, and 26x26 edge maps (we started with 28x28 images, but lost one pixel from each side as discussed earlier). We can see we get the same results as when we did this manually: . show_image(batch_features[0,0]); . The most important trick that PyTorch has up its sleeve is that it can use the GPU to do all this work in parallel. That is, applying multiple kernels, to multiple images, across multiple channels. Doing lots of work in parallel is critical to getting GPUs to work efficiently; if we did each of these one at a time, we&#39;ll often run hundreds of times slower (and if we used our manual convolution loop from the previous section, we&#39;d be millions of times slower!) Therefore, to become a strong deep learning practitioner, one skill to practice is giving your GPU plenty of work to do at a time. . It would be nice to not lose those two pixels on each axis. The way we do that is to add padding, which is simply additional pixels added around the outside of our image. Most commonly, pixels of zeros are added. . Strides and padding . With appropriate padding, we can ensure that the output activation map is the same size as the original image, which can make things a lot simpler when we construct our architectures. . A convolution with padding . With a 5x5 input, and 4x4 kernel, and 2 pixels of padding, we end up with a 6x6 activation map, as we can see in &lt;&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; 4x4 kernel with 5x5 input and 2 pixels of padding . If we add a kernel of size ks by ks (with ks an odd number), the necessary padding on each side to keep the same shape is ks//2. An even number for ks would require a different amount of padding on the top/bottom, left/right, but in practice we almost never use an even filter size. . So far, when we have applied the kernel to the grid, we have moved it one pixel over at a time. But we can jump further; for instance, we could move over two pixels after each kernel application as in &lt;&gt;. This is known as a stride 2 convolution. The most common kernel size in practice is 3x3, and the most common padding is 1. As you&#39;ll see, stride 2 convolutions are useful for decreasing the size of our outputs, and stride 1 convolutions are useful for adding layers without changing the output size.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; 3x3 kernel with 5x5 input, stride 2 convolution, and 1 pixel of padding . In an image of size h by w like before, using a padding of 1 and a stride of 2 will give us a result of size (h+1)//2 by (w+1)//2. The general formula for each dimension is (n + 2*pad - ks)//stride + 1 where pad is the padding, ks the size of our kernel and stride the stride. . Let&#39;s now have a look at how the pixel values of the result of our convolutions are computed. . Understanding the convolution equations . To explain the math behing convolutions, fast.ai student Matt Kleinsmith came up with the very clever idea of showing CNNs from different viewpoints. In fact, it&#39;s so clever, and so helpful, we&#39;re going to show it here too! . Here&#39;s our 3x3 pixel image, with each pixel labeled with a letter: . . ...and our kernel, with each weight labeled with a greek letter: . . Since the filter fits in the image four times, we have four results: . . Here’s how we applied the kernel to each section of the image to yield each result: . Applying the kernel . The equation view: . The equation . Notice that the bias term, b, is the same for each section of the image. You can consider the bias as part of the filter, just like the weights (α, β, γ, δ) are part of the filter. . The compact equation view: . . Here&#39;s an interesting insight -- a convolution can be represented as a special kind of matrix multiplication. The weight matrix is just like the ones from traditional neural networks. However, this weight matrix has two special properties: . The zeros shown in gray are untrainable. This means that they’ll stay zero throughout the optimization process. | Some of the weights are equal, and while they are trainable (i.e. changeable), they must remain equal. These are called shared weights. | The zeros correspond to the pixels that the filter can&#39;t touch. Each row of the weight matrix corresponds to one application of the filter. . Convolution as matrix multiplication . Now that we understand what a convolution is, let&#39;s use them to build a neural net. . Our first convolutional neural network . There is no reason to believe that some particular edge filters are the most useful kernels for image recognition. Furthermore, we&#39;ve seen that in later layers convolutional kernels become complex transformations of features from lower levels — we do not have a good idea of how to manually construct these. . Instead, it would be best to learn the values of the kernels. We already know how to do this — SGD! In effect, the model will learn the features that are useful for classification. . When we use convolutions instead of (or in addition to) regular linear layers we create a convolutional neural network, or CNN. . Creating the CNN . Let&#39;s go back to the basic neural network we had in &lt;&gt;. It was defined like this:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; simple_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1) ) . We can view a model&#39;s definition: . simple_net . Sequential( (0): Linear(in_features=784, out_features=30, bias=True) (1): ReLU() (2): Linear(in_features=30, out_features=1, bias=True) ) . We now want to create a similar architecture to this linear model, but using convolutional layers instead of linear. nn.Conv2d is the module equivalent of F.conv2d. It&#39;s more convenient than F.conv2d when creating an architecture, because it creates the weight matrix for us automatically when we instantiate it. . Here&#39;s a possible architecture: . broken_cnn = sequential( nn.Conv2d(1,30, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(30,1, kernel_size=3, padding=1) ) . One thing to note here is that we didn&#39;t need to specify &quot;28x28&quot; as the input size. That&#39;s because a linear layer needs a weight in the weight matrix for every pixel. So it needs to know how many pixels there are. But a convolution is applied over each pixel automatically. The weights only depend on the number of input and output channels, and the kernel size, as we saw in the previous section. . Have a think about what the output shape is going to be. . Let&#39;s try it and see: . broken_cnn(xb).shape . torch.Size([64, 1, 28, 28]) . This is not something we can use to do classification, since we need a single output activation per image, not a 28x28 map of activations. One way to deal with this is to use enough stride-2 convolutions such that the final layer is size 1. That is, after one stride-2 convolution, the size will be 14x14, after 2 it will be 7x7, then 4x4, 2x2, and finally size 1. . Let&#39;s try that now. First, we&#39;ll define a function with the basic parameters we&#39;ll use in each convolution: . def conv(ni, nf, ks=3, act=True): res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2) if act: res = nn.Sequential(res, nn.ReLU()) return res . . Important: Refactoring parts of your neural networks like this makes it much less likely you&#8217;ll get errors due to inconsistencies in your architectures, and makes it more obvious to the reader which parts of your layers are actually changing. . When we use a stride-2 convolution, we often increase the number of features at the same time. This is because we&#39;re decreasing the number of activations in the activation map by a factor of 4; we don&#39;t want to decrease the capacity of a layer by too much at a time. . jargon: channels and features: These two terms are largely used interchangably, and refer to the size of the second axis of a weight matrix, which is, therefore, the number of activations per grid cell after a convolution. Features is never used to refer to the input data, but channels can refer to either the input data (generally channels are colors) or activations inside the network. . simple_cnn = sequential( conv(1 ,4), #14x14 conv(4 ,8), #7x7 conv(8 ,16), #4x4 conv(16,32), #2x2 conv(32,2, act=False), #1x1 Flatten(), ) . j: I like to add comments like the above after each convolution to show how large the activation map will be after each layer. The above comments assume that the input size is 28x28 . Now the network outputs two activations, which maps to the two possible levels in our labels: . simple_cnn(xb).shape . torch.Size([64, 2]) . We can now create our Learner: . learn = Learner(dls, simple_cnn, loss_func=F.cross_entropy, metrics=accuracy) . To see exactly what&#39;s going on in your model, use summary: . learn.summary() . Sequential (Input shape: [&#39;64 x 1 x 28 x 28&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ Conv2d 64 x 4 x 14 x 14 40 True ________________________________________________________________ ReLU 64 x 4 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 8 x 7 x 7 296 True ________________________________________________________________ ReLU 64 x 8 x 7 x 7 0 False ________________________________________________________________ Conv2d 64 x 16 x 4 x 4 1,168 True ________________________________________________________________ ReLU 64 x 16 x 4 x 4 0 False ________________________________________________________________ Conv2d 64 x 32 x 2 x 2 4,640 True ________________________________________________________________ ReLU 64 x 32 x 2 x 2 0 False ________________________________________________________________ Conv2d 64 x 2 x 1 x 1 578 True ________________________________________________________________ Flatten 64 x 2 0 False ________________________________________________________________ Total params: 6,722 Total trainable params: 6,722 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fbc9c258cb0&gt; Loss function: &lt;function cross_entropy at 0x7fbca9ba0170&gt; Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . Note that the output of the final Conv2d layer is 64x2x1x1. We need to remove those extra 1x1 axes; that&#39;s what Flatten does. It&#39;s basically the same as PyTorch&#39;s squeeze method, but as a module. . Let&#39;s see if this trains! Since this is a deeper network than we&#39;ve built from scratch before, we&#39;ll use a lower learning rate and more epochs: . learn.fit_one_cycle(2, 0.01) . epoch train_loss valid_loss accuracy time . 0 | 0.072684 | 0.045110 | 0.990186 | 00:05 | . 1 | 0.022580 | 0.030775 | 0.990186 | 00:05 | . Success! It&#39;s getting closer to the resnet-18 result we had, although it&#39;s not quite there yet, and it&#39;s taking more epochs, and we&#39;re needing to use a lower learning rate. So we&#39;ve got a few more tricks still to learn--but we&#39;re getting closer and closer to being able to create a modern CNN from scratch. . Understanding convolution arithmetic . We can see from the summary that we have an input of size 64x1x28x28. The axes are: batch,channel,height,width. This is often represented as NCHW (where N refers to batch size). Tensorflow, on the other hand, uses NHWC axis order. The first layer is: . m = learn.model[0] m . Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)) (1): ReLU() ) . So we have 1 channel input, 4 channel output, and a 3x3 kernel. Let&#39;s check the weights of the first convolution: . m[0].weight.shape . torch.Size([4, 1, 3, 3]) . The summary shows we have 40 parameters, and 4*1*3*3 is 36. What are the other 4 parameters? Let&#39;s see what the bias contains: . m[0].bias.shape . torch.Size([4]) . We can now use this information to better understand our earlier statement in this section: &quot;because we&#39;re decreasing the number of activations in the activation map by a factor of 4; we don&#39;t want to decrease the capacity of a layer by too much at a time&quot;. . There is one bias for each channel. (Sometimes channels are called features or filters when they are not input channels.) The output shape is 64x4x14x14, and this will therefore become the input shape to the next layer. The next layer, according to summary, has 296 parameters. Let&#39;s ignore the batch axis to keep things simple. So for each of 14*14=196 locations we are multiplying 296-8=288 weights (ignoring the bias for simplicity), so that&#39;s 196*288=56_448 multiplications at this layer. The next layer will have 7*7*(1168-16)=56_448 multiplications. . So what happened here is that our stride 2 conv halved the grid size from 14x14 to 7x7, and we doubled the number of filters from 8 to 16, resulting in no overall change in the amount of computation. If we left the number of channels the same in each stride 2 layer, the amount of computation being done in the net would get less and less as it gets deeper. But we know that the deeper layers have to compute semantically rich features (such as eyes, or fur), so we wouldn&#39;t expect that doing less compute would make sense. . Another way to think of this is based on receptive fields. . Receptive fields . The &quot;receptive field&quot; is the area of an image that is involved in the calculation of a layer. On the book website, you&#39;ll find an Excel spreadsheet called conv-example.xlsx that shows the calculation of two stride 2 convolutional layers using an MNIST digit. Each layer has a single kernel. If we click on one of the cells in the conv2 section, which shows the output of the second convolutional layer, and click trace precendents, we see this: . Immediate precedents of conv2 layer . Here, the green cell is the cell we clicked on, and the blue highlighted cells are its precedents--that is, the cells used to calculate its value. These cells are the corresponding 3x3 area of cells from the input layer (on the left), and the cells from the filter (on the right). Let&#39;s now click show precedents again, to show what cells are used to calculate these inputs, and see what happens: . Secondary precedents of conv2 layer . In this example, we just have two convolutional layers, each of stride 2, so this is now tracing right back to the input image. We can see that a 7x7 area of cells in the input layer is used to calculate the single green cell in the Conv2 layer. This 7x7 area is the receptive field in the Input of the green activation in Conv2. We can also see that a second filter kernel is needed now, since we have two layers. . As you see from this example, the deeper we are in the network (specifically, the more stride 2 convs we have before a layer), the larger the receptive field for an activation in that layer. A large receptive field means that a large amount of the input image is used to calculate each activation in that layer. So we know now that in the deeper layers of the network, we have semantically rich features, corresponding to larger receptive fields. Therefore, we&#39;d expect that we&#39;d need more weights for each of our features to handle this increasing complexity. This is another way of seeing the same thing we saw in the previous section: when we introduce a stride 2 conv in our network, we should also increase the number of channels. . When writing this particular chapter, we had a lot of questions we needed answers for, to be able to explain to you those CNNs as best we could. Believe it or not, we found most of the answers on Twitter. . A note about Twitter . We are not, to say the least, big users of social networks in general. But our goal of this book is to help you become the best deep learning practitioner you can, and we would be remiss not to mention how important Twitter has been in our own deep learning journeys. . You see, there&#39;s another part of Twitter, far away from Donald Trump and the Kardashians, which is the part of Twitter where deep learning researchers and practitioners talk shop every day. As we were writing the section above, Jeremy wanted to double-check to ensure that what we were saying about stride 2 convolutions was accurate, so he asked on Twitter: . . A few minutes later, this answer popped up: . . Christian Szegedy is the first author of Inception, the 2014 Imagenet winner and source of many key insights used in modern neural networks. Two hours later, this appeared: . . Do you recognize that name? We saw a picture of him back in &lt;&gt;, when we were talking about the Turing Award winners who set the foundation of deep learning today!&lt;/p&gt; Jeremy also asked on Twitter for help checking our description of label smoothing in &lt;&gt; was accurate, and got a response from again from directly from Christian Szegedy (label smoothing was originally introduced in the Inception paper):&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . Many of the top people in deep learning today are Twitter regulars, and are very open about interacting with the wider community. One good way to get started is to look at a list of Jeremy&#39;s recent Twitter likes, or Sylvain&#39;s. That way, you can see a list of Twitter users that we thought had interesting and useful things to say. . Twitter is the main way we both stay up to date with interesting papers, software releases, and other deep learning news. For making connections with the deep learning community, we recommend getting involved both in the fast.ai forums and Twitter. . Up until now, we have only shown you examples of pictures in black and white, with only one value per pixel. In practice, most colored images have three values per pixel to define is color. . Colour images . A colour picture is a rank-3 tensor. . im = image2tensor(Image.open(&#39;images/grizzly.jpg&#39;)) im.shape . torch.Size([3, 1000, 846]) . show_image(im); . The first axis contains the channels: red, green, and blue: . _,axs = subplots(1,3) for bear,ax,color in zip(im,axs,(&#39;Reds&#39;,&#39;Greens&#39;,&#39;Blues&#39;)): show_image(255-bear, ax=ax, cmap=color) . We saw what the convolution operation was for one filter on one channel of the image (our examples were done on a square). A convolution layer will take an image with a certain number of channels (3 for the first layer for regular RGB color images) and output an image with a different number of channels. Like our hidden size that represented the numbers of neurons in a linear layer, we can decide to have has many filters as we want, and each of them will be able to specialize, some to detect horizontal edges, other to detect vertical edges and so forth, to give something like we studied in &lt;&gt;.&lt;/p&gt; On one sliding window, we have a certain number of channels and we need as many filters (we don&#39;t use the same kernel for all the channels). So our kernel doesn&#39;t have a size of 3 by 3, but ch_in (for channel in) by 3 by 3. On each channel, we multiply the elements of our window by the elements of the coresponding filter then sum the results (as we saw before) and sum over all the filters. In the example given by &lt;&gt;, the result of our conv layer on that window is red + green + blue.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Convolution over an RGB image . So, in order to apply a convolution to a colour picture we require a kernel tensor with a matching size as the first axis. At each location, the corresponding parts of the kernel and the image patch are multiplied together. . These are then all added together, to produce a single number, for each grid location, for each output feature, as shown in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Adding the RGB filters . Then we have ch_out filters like this, so in the end, the result of our convolutional layer will be a batch of images with ch_out channels and a height and width given by the formula above. This give us ch_out tensors of size ch_in x ks x ks that we represent in one big tensor of 4 dimensions. In PyTorch, the order of the dimensions for those weights is ch_out x ch_in x ks x ks. . Additionally, we may want to have a bias for each filter. In the example above, the final result for our convolutional layer would be $y_{R} + y_{G} + y_{B} + b$ in that case. Like in a linear layer, there are as many bias as we have kernels, so the bias is a vector of size ch_out. . There are no special mechanisms required when setting up a CNN for training with color images. Just make sure your first layer has 3 inputs. . There are lots of ways of processing color images. For instance, you can change them to black and white, or change from RGB to HSV (Hue, Saturation, and Value) color space, and so forth. In general, it turns out experimentally that changing the encoding of colors won&#39;t make any difference to your model results, as long as you don&#39;t lose information in the transformation. So transforming to black and white is a bad idea, since it removes the color information entirely (and this can be critical; for instance a pet breed may have a distinctive color); but converting to HSV generally won&#39;t make any difference. . Now you know what those pictures in &lt;&gt; of &quot;what a neural net learns&quot; from the Zeiler and Fergus paper mean! This is their picture of some of the layer 1 weights which we showed:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . This is taking the 3 slices of the convolutional kernel, for each output feature, and displaying them as images. We can see that even although the creators of the neural net never explicitly created kernels to find edges, for instance, the neural net automatically discovered these features using SGD. . Now let&#39;s see how we can train those CNNs, and show you all the techniques fastai uses behind the hood for efficient training. . Improving training stability . Since we are so good at recognizing threes from sevens, let&#39;s move onto something harder—recognizing all 10 digits. That means we&#39;ll need to use MNIST instead of MNIST_SAMPLE: . path = untar_data(URLs.MNIST) . path.ls() . (#2) [Path(&#39;testing&#39;),Path(&#39;training&#39;)] . The data is in two folders named training and testing, so we have to tell GrandparentSplitter about that (it defaults to train and valid). We define a function get_dls to make it easy to change our batch size later: . def get_dls(bs=64): return DataBlock( blocks=(ImageBlock(cls=PILImageBW), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(&#39;training&#39;,&#39;testing&#39;), get_y=parent_label, batch_tfms=Normalize() ).dataloaders(path, bs=bs) dls = get_dls() . It is always a good idea to look at your data before you use it: . dls.show_batch(max_n=9, figsize=(4,4)) . Now that we have our data ready, we can train a simple model on it. . A simple baseline . In the previous section, we built a model based on a conv function like this: . def conv(ni, nf, ks=3, act=True): res = nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2) if act: res = nn.Sequential(res, nn.ReLU()) return res . Let&#39;s start with a basic CNN as a baseline. We&#39;ll use the same as we had in the last Section, but with one tweak: we&#39;ll use more activations. Since we have more numbers to differentiate, it&#39;s likely we will need to learn more filters. . As we discussed, we generally want to double the number of filters each time we have a stride 2 layer. So, one way to increase the number of filters throughout our network is to double the number of activations in the first layer – then every layer after that will end up twice as big as the previous version as well. . But there is a subtle problem with this. Consider the kernel which is being applied to each pixel. By default, we use a 3x3 pixel kernel. That means that there are a total of 3×3 = 9 pixels that the kernel is being applied to at each location. Previously, our first layer had four filters output. That meant that there were four values being computed from nine pixels at each location. Think about what happens if we double this output to 8 filters. Then when we apply our kernel we would be using nine pixels to calculate eight numbers. That means that it isn&#39;t really learning much at all — the output size is almost the same as the input size. Neural networks will only create useful features if they&#39;re forced to do so—that is, that the number of outputs from an operation is smaller than the number of inputs. . To fix this, we can use a larger kernel in the first layer. If we use a kernel of 5x5 pixels then there are 25 pixels being used at each kernel application — creating eight filters from this will mean the neural net will have to find some useful features. . def simple_cnn(): return sequential( conv(1 ,8, ks=5), #14x14 conv(8 ,16), #7x7 conv(16,32), #4x4 conv(32,64), #2x2 conv(64,10, act=False), #1x1 Flatten(), ) . As you&#39;ll see in a moment, we&#39;re going to look inside our models while they&#39;re training in order to try to find ways to make them train better. To do this, we use the ActivationStats callback, which records the mean, standard deviation, and histogram of activations of every trainable layer (as we&#39;ve seen, callbacks are used to add behavior to the training loop; we&#39;ll see how they work in &lt;&gt;).&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; from fastai2.callback.hook import * . We want to train quickly, so that means training at a high learning rate. Let&#39;s see how we go at 0.06: . def fit(epochs=1): learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True)) learn.fit(epochs, 0.06) return learn . learn = fit() . epoch train_loss valid_loss accuracy time . 0 | 2.307071 | 2.305865 | 0.113500 | 00:16 | . This didn&#39;t train at all well! Let&#39;s find out why. . One handy feature of the callbacks passed to Learner is that they are made available automatically, with the same name as the callback class, except in camel_case. So our ActivationStats callback can be accessed through activation_stats. In fact--I&#39;m sure you remember learn.recorder... can you guess how that is implemented? That&#39;s right, it&#39;s a callback called Recorder! . ActivationStats includes some handy utilities for plotting the activations during training. plot_layer_stats(idx) plots the mean and standard deviation of the activations of layer number idx, along with the percent of activations near zero. Here&#39;s the first layer&#39;s plot: . learn.activation_stats.plot_layer_stats(0) . Generally our model should have a consistent, or at least smooth, mean and standard deviation of layer activations during training. Activations near zero are particularly problematic, because it means we have computation in the model that&#39;s doing nothing at all (since multiplying by zero gives zero). When you have some zeros in one layer, they will therefore generally carry over to the next layer... which will then create more zeros. Here&#39;s the penultimate layer of our network: . learn.activation_stats.plot_layer_stats(-2) . As expected, the problems get worse towards the end of the network, as the instability and zero activations compound over layers. The first thing we can do to make training more stable is to increase the batch size. . Increase batch size . One way to make training more stable is to increase the batch size. Larger batches have gradients that are more accurate, since they&#39;re calculated from more data. On the downside though, a larger batch size means fewer batches per epoch, which means less opportunities for your model to update weights. Let&#39;s see if a batch size of 512 helps: . dls = get_dls(512) . learn = fit() . epoch train_loss valid_loss accuracy time . 0 | 2.309385 | 2.302744 | 0.113500 | 00:08 | . Let&#39;s see what the penultimate layer looks like: . learn.activation_stats.plot_layer_stats(-2) . Again, we&#39;ve got most of our activations near zero. Let&#39;s see what else we can do to improve training stability. . 1cycle training . Our initial weights are not well suited to the task we&#39;re trying to solve. Therefore, it is dangerous to begin training with a high learning rate: we may very well make the training diverge instantly, as we&#39;ve seen above. We probably don&#39;t want to end training with a high learning rate either, so that we don&#39;t skip over a minimum. But we want to train at a high learning rate for the rest of training, because we&#39;ll be able to train more quickly. Therefore, we should change the learning rate during training, from low, to high, and then back to low again. . Leslie Smith (yes, the same guy that invented the learning rate finder!) developed this idea in his article Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates by designing a schedule for learning rate separated in two phases: one were the learning rate grows from the minimum value to the maximum value (warm-up), and then one where it decreases back to the minimum value (annealing). Smith called this combination of approaches 1cycle training. . 1cycle training allows us to use a much higher maximum learning rate than other types of training, which gives two benefits: . By training with higher learning rates, we train faster, a phenomenon Leslie N. Smith named super-convergence | By training with higher learning rates, we overfit less because we skip over the sharp local minimas to end-up in a smoother (and therefore more generalizable) part of the loss. | . The second point is an interesting and subtle idea; it is based on the observation that a model that generalises well is one whose loss would not change very much if you change the input by a small amount. If a model trains at a large learning rate for quite a while, and can find a good loss when doing so, it must have found an area that also generalises well, because it is jumping around a lot from batch to batch (that is basically the definition of a high learning rate). The problem is that, as we have discussed, just jumping to a high learning rate is more likely to result in diverging losses, rather than seeing your losses improve. So we don&#39;t just jump to a high learning rate. Instead, we start at a low learning rate, where our losses do not diverge, and we allow the optimiser to gradually find smoother and smoother areas of our parameters, by gradually going to higher and higher learning rates. . Then, once we have found a nice smooth area for our parameters, we then want to find the very best part of that area, which means we have to bring out learning rates down again. This is why 1cycle training has a gradual learning rate warmup, and a gradual learning rate cooldown. Many researchers have found that in practice this approach leads to more accurate models, and trains more quickly. That is why it is the approach that is used by default for fine_tune in fastai. . In &lt;&gt; we&#39;ll learn all about momentum in SGD. Briefly, momentum is a technique where the optimizer takes a step not only in the direction of the gradients, but also continues in the direction of previous steps. Leslie Smith introduced cyclical momentums in A disciplined approach to neural network hyper-parameters: Part 1. It suggests that the momentum varies in the opposite direction of the learning rate: when we are at high learning rates, we use less momentum, and we use more again in the annealing phase.&lt;/p&gt; We can use 1cycle training in fastai by calling fit_one_cycle: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; def fit(epochs=1, lr=0.06): learn = Learner(dls, simple_cnn(), loss_func=F.cross_entropy, metrics=accuracy, cbs=ActivationStats(with_hist=True)) learn.fit_one_cycle(epochs, lr) return learn . learn = fit() . epoch train_loss valid_loss accuracy time . 0 | 0.210838 | 0.084827 | 0.974300 | 00:08 | . We&#39;re finally making some progress! It&#39;s giving us a reasonable accuracy now. . We can view the learning rate and momentum throughout training by calling plot_sched on learn.recorder. learn.recorder (as the name suggests) records everything that happens during training, including losses, metrics, and hyperparameters such as learning rate and momentum: . learn.recorder.plot_sched() . Smith&#39;s original 1cycle paper used a linear warm-up and linear annealing. As you see above, we adapted the approach in fastai by combining it with another popular approach: cosine annealing. fit_one_cycle provides the following parameters you can adjust: . lr_max:: The highest learning rate that will be used (this can also be a list of learning rates for each layer group, or a python slice object containing the first and last layer group learning rates) | div:: How much to divide lr_max by to get the starting learning rate | div_final:: How much to divide lr_max by to get the ending learning rate | pct_start:: What % of the batches to use for the warmup | moms:: A tuple (mom1,mom2,mom3) where mom1 is the initial momentum, mom2 is the minimum momentum, and mom3 is the final momentum. | . Let&#39;s take a look at our layer stats again: . learn.activation_stats.plot_layer_stats(-2) . The % of non-zero weights is getting much better, although it&#39;s still quite high. . We can see even more about what&#39;s going on in our training using color_dim, passing it a layer index: . learn.activation_stats.color_dim(-2) . color_dim was developed by fast.ai in conjunction with a student, Stefano Giomo. Stefano, who refers to the idea as the colorful dimension, has a detailed explanation of the history and details behind the method. The basic idea is to create a histogram of the activations of a layer, which we would hope would follow a smooth pattern such as the normal distribution shown by Stefano here: . Histogram in &#39;colorful dimension&#39; . To create color_dim, we take the histogram shown on the left here, and convert it into just the colored representation shown at the bottom. Then we flip it on its side, as shown on the right. We found that the distribution is clearer if we take the log of the histogram values. Then, Stefano describes: . : The final plot for each layer is made by stacking the histogram of the activations from each batch along the horizontal axis. So each vertical slice in the visualisation represents the histogram of activations for a single batch. The color intensity corresponds to the height of the histogram, in other words the number of activations in each histogram bin. This is Stefano&#39;s picture of how this all fits together: . Summary of &#39;colorful dimension&#39; . TK Add an explanation of the picture . So with that in mind, let&#39;s take another look at the result for the penultimate layer: . learn.activation_stats.color_dim(-2) . This shows a classic picture of &quot;bad training&quot;. We start with nearly all activations at zero--that&#39;s what we see at the far left, with nearly all the left hand side dark blue; the bright yellow at the bottom are the near-zero activations. Then over the first few batches we see the number of non-zero activations exponentially increasing. But it goes too far, and collapses! We see the dark blue return, and the bottom becomes bright yellow again. It almost looks like training restarts from scratch. Then we see the activations increase again, and then it collapses again. After repeating a few times, eventually we see a spread of activations throughout the range. . It&#39;s much better if training can be smooth from the start. The cycles of exponential increase and then collapse that we see above tend to result in a lot of near-zero activations, resulting in slow training, and poor final results. One way to solve this problem is to use Batch normalization. . Batch normalization . To fix the slow training and poor final results we ended up with in the previous section, we need to both fix the initial large percentage of near-zero activations, and then try to maintain a good distribution of activations throughout training. . Sergey Ioffe and Christian Szegedy showed a solution to this problem in the 2015 paper Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In the abstract, they describe just the problem that we&#39;ve seen: . : &quot;Training Deep Neural Networks is complicated by the fact that the distribution of each layer&#39;s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization... We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs.&quot; Their solution, they say is: : &quot;...making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization.&quot; The paper caused great excitement as soon as it was released, because they showed the chart in &lt;&gt;, which clearly demonstrated that batch normalization could train a model that was even more accurate than the current state of the art (the inception architecture), around 5x faster:&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Impact of batch normalization . The way batch normalization (often just called batchnorm) works is that it takes an average of the mean and standard deviations of the activations of a layer, and uses those to normalize the activations. However, this can cause problems because the network might really want some activations to be really high in order to make accurate predictions, they also add two learnable parameters (meaning they will be updated in our SGD step), usually called gamma and beta; after normalizing the activations to get some new activation vector y, a batchnorm layer returns gamma*y + beta. . That why our activations can have any mean or variance, which is independent from the mean and std of the results of the previous layer. Those statistics are learned separately, making training easier on our model. The behavior is different during training and validation: during training, we use the mean and standard deviation of the batch to normalize the data. During validation, we instead use a running mean of the statistics calculated during training. . Let&#39;s add a batchnorm layer to conv: . def conv(ni, nf, ks=3, act=True): layers = [nn.Conv2d(ni, nf, stride=2, kernel_size=ks, padding=ks//2)] layers.append(nn.BatchNorm2d(nf)) if act: layers.append(nn.ReLU()) return nn.Sequential(*layers) . ...and fit our model: . learn = fit() . epoch train_loss valid_loss accuracy time . 0 | 0.130036 | 0.055021 | 0.986400 | 00:10 | . That&#39;s a great result! Let&#39;s take a look at color_dim: . learn.activation_stats.color_dim(-4) . This is just what we hope to see: a smooth development of activations, with no &quot;crashes&quot;. Batchnorm has really delivered on its promise here! In fact, batchnorm has been so successful that we see it (or something very similar) today in nearly all modern neural networks. . An interesting observation about models containing batch normalisation layers is that they tend to generalise better than models that don&#39;t contain them. Although we haven&#39;t as yet seen a rigourous analysis of what&#39;s going on here, most researchers believe that the reason for this is that batch normalisation add some extra randomness to the training process. Each mini batch will have a somewhat different mean and standard deviation to other mini batches. Therefore, the activations will be normalised by different values each time. In order for the model to make accurate predictions, it will have to learn to become robust with these variations. In general, adding additional randomisation to the training process often helps. . Since things are going so well, let&#39;s train for a few more epochs and see how it goes. In fact, let&#39;s even increase the learning rate, since the abstract of the batchnorm paper claimed we should be able to &quot;train at much higher learning rates&quot;: . learn = fit(5, lr=0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.191731 | 0.121738 | 0.960900 | 00:11 | . 1 | 0.083739 | 0.055808 | 0.981800 | 00:10 | . 2 | 0.053161 | 0.044485 | 0.987100 | 00:10 | . 3 | 0.034433 | 0.030233 | 0.990200 | 00:10 | . 4 | 0.017646 | 0.025407 | 0.991200 | 00:10 | . learn = fit(5, lr=0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.183244 | 0.084025 | 0.975800 | 00:13 | . 1 | 0.080774 | 0.067060 | 0.978800 | 00:12 | . 2 | 0.050215 | 0.062595 | 0.981300 | 00:12 | . 3 | 0.030020 | 0.030315 | 0.990700 | 00:12 | . 4 | 0.015131 | 0.025148 | 0.992100 | 00:12 | . At this point, I think it&#39;s fair to say we know how to recognize digits! It&#39;s time to move on to something harder... . Conclusions . We&#39;ve seen that convolutions are just a type of matrix multiplication, with two constraints on the weight matrix: some elements are always zero, and some elements are tied (forced to always have the same value). In &lt;&gt; we saw the eight requirements from the 1986 book Parallel Distributed Processing; one of them was &quot;A pattern of connectivity among units&quot;. That&#39;s exactly what these constraints do: they enforce a certain pattern of connectivity.&lt;/p&gt; These constraints allow us to use far less parameters in our model, without sacrificing the ability to represent complex visual features. That means we can train deeper models faster, with less over-fitting. Although the universal approximation theorem shows that it should be possible to represent anything in a fully connected network in one hidden layer, we&#39;ve seen now that in practice we can train much better models by being thoughtful about network architecture. . Convolutions are by far the most common pattern of connectivity we see in neural nets (along with regular linear layers, which we refer to as fully connected), but it&#39;s likely that many more will be discovered. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; TK add some takeways from the last section to the conclusion . Questionnaire . What is a &quot;feature&quot;? | Write out the convolutional kernel matrix for a top edge detector. | Write out the mathematical operation applied by a 3 x 3 kernel to a single pixel in an image. | What is the value of a convolutional kernel apply to a 3 x 3 matrix of zeros? | What is padding? | What is stride? | Create a nested list comprehension to complete any task that you choose. | What are the shapes of the input and weight parameters to PyTorch&#39;s 2D convolution? | What is a channel? | What is the relationship between a convolution and a matrix multiplication? | What is a convolutional neural network? | What is the benefit of refactoring parts of your neural network definition? | What is Flatten? Where does it need to be included in the MNIST CNN? Why? | What does &quot;NCHW&quot; mean? | Why does the third layer of the MNIST CNN have 7*7*(1168-16) multiplications? | What is a receptive field? | What is the size of the receptive field of an activation after two stride 2 convolutions? Why? | Run conv-example.xlsx yourself and experiment with &quot;trace precedents&quot;. | Have a look at Jeremy or Sylvain&#39;s list of recent Twitter &quot;like&quot;s, and see if you find any interesting resources or ideas there. | How is a color image represented as a tensor? | How does a convolution work with a color input? | What method can we use to see that data in DataLoaders? | Why do we double the number of filters after each stride 2 conv? | Why do we use a larger kernel in the first conv with MNIST (with simple_cnn)? | What information does ActivationStats save for each layer? | How can we access a learner&#39;s callback after training? | What are the three statistics plotted by plot_layer_stats? What does the x-axis represent? | Why are activations near zero problematic? | What are the upsides and downsides of training with a larger batch size? | Why should we avoid using a high learning rate at the start of training? | What is 1cycle training? | What are the benefits of training with a high learning rate? | Why do we want to use a low learning rate at the end of training? | What is cyclical momentum? | What callback tracks hyperparameter values during training (along with other information)? | What does one column of pixels in the color_dim plot represent? | What does &quot;bad training&quot; look like in color_dim? Why? | What trainable parameters does a batch normalization layer contain? | What statistics are used to normalize in batch normalization during training? How about during validation? | Why do models with batch normalization layers generalize better? | Further research . What features other than edge detectors have been used in computer vision (especially before deep learning became popular)? | There are other normalization layers available in PyTorch. Try them out and see what works best. Learn about why other normalization layers have been developed, and how they differ from batch normalization. | Try moving the activation function after the batch normalization layer in conv. Does it make a difference? See what you can find out about what order is recommended, and why. | Batch normalization isn&#39;t defined for a batch size of one, since the standard deviation isn&#39;t defined for a single item. | &lt;/div&gt; . . .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_convolutions.html",
            "relUrl": "/2020/03/19/_convolutions.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Title",
            "content": "[[chapter_conclusion]] Concluding thoughts . Congratulations! You&#39;ve made it! If you have worked through all of the notebooks to this point, then you have joined a small, but growing group of people that are able to harness the power of deep learning to solve real problems. You may not feel that way; in fact you probably do not feel that way. We have seen again and again that students that complete the fast.ai courses dramatically underestimate how effective they are as deep learning practitioners. We&#39;ve also seen that these people are often underestimated by those that have come out of a classic academic background. So for you to rise above your own expectations and the expectations of others what you do next, after closing this book, is even more important than what you&#39;ve done to get to this point. . The most important thing is to keep the momentum going. In fact, as you know from your study of optimisers, momentum is something which can build upon itself! So think about what it is you can do now to maintain and accelerate your deep learning journey. &lt;&gt; can give you a few ideas.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; What to do next . We&#39;ve talked a lot in this book about the value of writing, whether it be code or prose. But perhaps you haven&#39;t quite written as much as you had hoped so far. That&#39;s okay! Now is a great chance to turn that around. You have a lot to say, at this point. Perhaps you have tried some experiments on a dataset which other people don&#39;t seem to have looked at in quite the same way — so tell the world about it! Or perhaps you are just curious to try out some ideas that you had been thinking about while you were reading; now is a great chance to turn those ideas into code. . One fairly low-key place for your writing is the fast.ai forums at forums.fast.ai. You will find that the community there is very supportive and helpful, so please do drop by and let us know what you&#39;ve been up to. Or see if you can answer a few questions for those folks who are earlier in their journey than you. . And if you do have some success, big or small, in your deep learning journey, be sure to let us know! It&#39;s especially helpful if you post about it on the forums, because for others to learn about the successes of other students can be extremely motivating. . Perhaps the most important approach for many people to stay connected with their learning journey is to build a community around it. For instance, you could try to set up a small deep learning Meetup in your local neighbourhood, or a study group, or even offer to do a talk at a local meet up about what you&#39;ve learned so far, or some particular aspect that interested you. It is okay that you are not the world&#39;s leading expert just yet – the important thing to remember is that you now know about plenty of stuff that other people don&#39;t, so they are very likely to appreciate your perspective. . Another community event which many people find useful is a regular book club or paper reading club. You might find that there are some in your neighbourhood already, or otherwise you could try to get one started yourself. Even if there is just one other person doing it with you, it will help give you the support and encouragement to get going. . If you are not in a geography where it&#39;s easy to get together with like-minded folks in person, drop by the forums, because there are lots of people always starting up virtual study groups. These generally involve a bunch of people getting together over video chat once every week or so, and discussing some deep learning topic. . Hopefully, by this point, you have a few little projects that you put together, and experiments that you&#39;ve run. Our recommendation is generally to pick one of these and make it as good as you can. Really polish it up into the best piece of work that you can — something you are really proud of. This will force you to go much deeper into a topic, which will really test out your understanding, and give you the opportunity to see what you can do when you really put your mind to it. . Also, you may want to take a look at the fast.ai free online course which covers the same material as this book. Sometimes, seeing the same material in two different ways, can really help to crystallise the ideas. In fact, human learning researchers have found that this is one of the best ways to learn material — to see the same thing from different angles, described in different ways. . Your final mission, should you choose to accept it, is to take this book, and give it to somebody that you know — and let somebody else start their way down their own deep learning journey! . &lt;/div&gt; .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_conclusion.html",
            "relUrl": "/2020/03/19/_conclusion.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Chapter 8",
            "content": "[[chapter_collab]] Collaborative filtering deep dive . One very common problem to solve is when you have a number of users, and a number of products, you then want to recommend which products are most likely to be useful for which users. There are many variations of this, for example, recommending movies (such as on Netflix), figuring out what to highlight for a user on a homepage, deciding what stories to show in a social media feed, and so forth. There is a general solution to this problem, called collaborative filtering, which works like this: have a look at what products the current user has used or liked, find other users that have used or liked similar products, and then recommend the products that those other users have used or liked. . For example, on Netflix you may have watched lots of movies that are science-fiction, full of action, and were made in the 1970s. Netflix may not know these particular properties of the films you have watched, but it would be able to see that other people that have watched the same movies that you watched also tended to watch other movies that are science-fiction, full of action, and were made in the 1970s. In other words, to use this approach we don&#39;t necessarily need to know anything about the movies, except who like to watch them. . There is actually a more general class of problems that this approach can solve; not necessarily just things involving users and products. Indeed, for collaborative filtering we more commonly refer to items, rather than products. Items could be links that you click on, diagnoses that are selected for patients, and so forth. . The key foundational idea is that of latent factors. In the above Netflix example, we started with the assumption that you like old action sci-fi movies. But you never actually told Netflix that you like these kinds of movies. And Netflix never actually needed to add columns to their movies table saying which movies are of these types. But there must be some underlying concept of sci-fi, action, and movie age. And these concepts must be relevant for at least some people&#39;s movie watching decisions. . First, let&#39;s get some data suitable for a collaboratie filtering model. . A first look at the data . For this chapter we are going to work on this movie review problem. We do not have access to Netflix&#39;s entire dataset of movie watching history, but there is a great dataset that we can use, called MovieLens. This dataset contains tens of millions of movie rankings (that is a combination of a movie ID, a user ID, and a numeric rating), although we will just use a subset of 100,000 of them for our example. If you&#39;re interested, it would be a great learning project to try and replicate this approach on the full 25 million recommendation dataset you can get from their website. . The dataset is available through the usual fastai function: . from fastai2.collab import * from fastai2.tabular.all import * path = untar_data(URLs.ML_100k) . According to the README, the main table is in the file u.data. It is tab-separated and the columns are respectively user, movie, rating and timestamp. Since those names are not encoded, we need to indicate them when reading the file with pandas. Here is a way to open this table and take a look: . ratings = pd.read_csv(path/&#39;u.data&#39;, delimiter=&#39; t&#39;, header=None, names=[&#39;user&#39;,&#39;movie&#39;,&#39;rating&#39;,&#39;timestamp&#39;]) ratings.head() . user movie rating timestamp . 0 196 | 242 | 3 | 881250949 | . 1 186 | 302 | 3 | 891717742 | . 2 22 | 377 | 1 | 878887116 | . 3 244 | 51 | 2 | 880606923 | . 4 166 | 346 | 1 | 886397596 | . Although this has all the information we need, it is not a particularly helpful way for humans to look at this data. &lt;&gt; shows the same data cross tabulated into a human friendly table.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Crosstab of movies and users . We have selected just a few of the most popular movies, and users who watch the most movies, for this crosstab example. The empty cells in this table are the things that we would like our model to learn to fill in. Those other places where a user has not reviewed the movie yet, presumably because they have not watched. So for each user, we would like to figure out which of those movies they might be most likely to enjoy. . If we knew for each user to what degree they liked each important category that a movie might fall into, such as genre, age, preferred directors and actors, and so forth, and we knew the same information about each movie, then a simple way to fill in this table would be to multiply this information together for each movie and use a combination. For instance, assuming these factors range between -1 and positive one, and positive means high match and negative means low match, and the categories are science-fiction, action, and old movies, then we could represent the movie The Last Skywalker as: . last_skywalker = np.array([0.98,0.9,-0.9]) . Here, for instance, we are scoring very science-fiction as 0.98, and very not old as -0.9. We could represent a user who likes modern sci-fi action movies as: . user1 = np.array([0.9,0.8,-0.6]) . …and we can now calculate the match between this combination: . (user1*last_skywalker).sum() . 2.1420000000000003 . When we multiply two vectors together and add up the results, this is known as the dot product. It is used a lot in machine learning, and forms the basis of matrix modification. We will be looking a lot more at matrix modification and dot products in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; jargon: dot product: the mathematical operation of multiplying the elements of two vectors together, and then summing up the result. . On the other hand, we might represent the movie Casablanca as: . casablanca = np.array([-0.99,-0.3,0.8]) . …and the match between this combination is: . (user1*casablanca).sum() . -1.611 . Since we don&#39;t know what the latent factors actually are, and we don&#39;t know how to score them for each user and movie, we should learn them. . Learning the latent factors . There is surprisingly little distance from specifying the structure of a model, as we did in the last section, and learning one, since we can just use our general gradient descent approach. . Step one of this approach is to randomly initialise some parameters. These parameters will be a set of latent factors for each user and movie. We will have to decide how many to use. We will discuss how to select this shortly, but for illustrative purposes let&#39;s use 5 for now. Because each user will have a set of these factors, and each movie will have a set of these factors, we can show these randomly initialise values right next to the users and movies in our crosstab, and we can then fill in the dot products for each of these combinations in the middle. For example, &lt;&gt; shows what it looks like in Microsoft Excel, with the top-left cell formula displayed as an example.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Latent factors with crosstab . Step two of this approach is to calculate our predictions. As we&#39;ve discussed, we can do this by simply taking the dot product of each movie with each user. If for instance the first latent user factor represents how much they like action movies, and the first latent movie factor represents if the movie has a lot of action or not, when the product of those will be particularly high if either the user likes action movie and the movie has a lot of action in it or if the user doesn&#39;t like action movie and the movie doesn&#39;t have any action in it. On the other hand, if we have a mismatch (a user loves action movies but the movie isn&#39;t, or the user doesn&#39;t like action movies and it is one), the product will be very low. . Step three is to calculate our loss. We can use any loss function that we wish; that&#39;s pick mean squared error for now, since that is one reasonable way to represent the accuracy of a prediction. . That&#39;s all we need. With this in place, we can optimise our parameters (that is, the latent factors) using stochastic gradient descent, such as to minimise the loss. At each step, the stochastic gradient descent optimiser will calculate the match between each movie and each user using the dot product, and will compare it to the actual rating that each user gave to each movie, and it will then calculate the derivative of this value, and will step the weights by multiplying this by the learning rate. After doing this lots of times, the loss will get better and better, and the recommendations will also get better and better. . To use the usual Learner fit function, we will need to get our data into DataLoaders, so let&#39;s focus on that now. . Creating the DataLoaders . When showing the data we would rather see movie titles than their ids. The table u.item contains the coorespondance id to title: . movies = pd.read_csv(path/&#39;u.item&#39;, delimiter=&#39;|&#39;, encoding=&#39;latin-1&#39;, usecols=(0,1), names=(&#39;movie&#39;,&#39;title&#39;), header=None) movies.head() . movie title . 0 1 | Toy Story (1995) | . 1 2 | GoldenEye (1995) | . 2 3 | Four Rooms (1995) | . 3 4 | Get Shorty (1995) | . 4 5 | Copycat (1995) | . Next we merge it to our ratings to get the titles. . ratings = ratings.merge(movies) ratings.head() . user movie rating timestamp title . 0 196 | 242 | 3 | 881250949 | Kolya (1996) | . 1 63 | 242 | 3 | 875747190 | Kolya (1996) | . 2 226 | 242 | 5 | 883888671 | Kolya (1996) | . 3 154 | 242 | 3 | 879138235 | Kolya (1996) | . 4 306 | 242 | 5 | 876503793 | Kolya (1996) | . We can then build a DataLoaders object from this table. By default, it takes the first column for user, the second column for the item (here our movies) and the third column for the ratings. We need to change the value of item_name in our case, to use the titles instead of the ids: . dls = CollabDataLoaders.from_df(ratings, item_name=&#39;title&#39;, bs=64) dls.show_batch() . user title rating . 0 207 | Four Weddings and a Funeral (1994) | 3 | . 1 565 | Remains of the Day, The (1993) | 5 | . 2 506 | Kids (1995) | 1 | . 3 845 | Chasing Amy (1997) | 3 | . 4 798 | Being Human (1993) | 2 | . 5 500 | Down by Law (1986) | 4 | . 6 409 | Much Ado About Nothing (1993) | 3 | . 7 721 | Braveheart (1995) | 5 | . 8 316 | Psycho (1960) | 2 | . 9 883 | Judgment Night (1993) | 5 | . In order to represent collaborative filtering in PyTorch we can&#39;t just use the crosstab representation directly, especially if we want it to fit into our deep learning framework. We can represent our movie and user latent factor tables as simple matrices: . n_users = len(dls.classes[&#39;user&#39;]) n_movies = len(dls.classes[&#39;title&#39;]) n_factors = 5 user_factors = torch.randn(n_users, n_factors) movie_factors = torch.randn(n_movies, n_factors) . To calculate the result for a particular movie and use a combination we have two look up the index of the movie in our movie latent factors matrix, and the index of the user in our user latent factors matrix, and then we can do our dot product between the two latent factor vectors. But look up in an index is not an operation which our deep learning models know how to do. They know how to do matrix products, and activation functions. . It turns out that we can represent look up in an index as a matrix product! The trick is to replace our indices with one hot encoded vectors. He is an example of what happens if we multiply a vector by a one hot encoded vector representing the index three: . one_hot_3 = one_hot(3, n_users).float() user_factors.t() @ one_hot_3 . tensor([-0.4586, -0.9915, -0.4052, -0.3621, -0.5908]) . It gives us the same vector as the one at index 3 in the matrix: . user_factors[3] . tensor([-0.4586, -0.9915, -0.4052, -0.3621, -0.5908]) . If we do that for a few indices at once, we will have a matrix of one-hot encoded vectors and that operation will be a matrix multiplication! This would be a perfectly acceptable way to build models using this kind of architecture, except that it would use a lot more memory and time than necessary. We know that there is no real underlying reason to store the one hot encoded vector, or to search through it to find the occurrence of the number one — we should just be able to index into an array directly with an integer. Therefore, most deep learning libraries, including PyTorch, include a special layer which does just this; it indexes into a vector using an integer, but has its derivative calculated in such a way that it is identical to what it would have been if it had of done a matrix multiplication with a one hot encoded vector. This is called an embedding. . jargon: embedding layer: multiplying by a one hot encoded matrix, using the computational shortcut that it can be implemented by simply indexing directly. It is quite a fancy word for a very simple concept. The thing that you multiply the one hot encoded matrix by (or, using the computational shortcut, index into directly) is called the embedding matrix. . In computer vision, we had a very easy way to get all the information of a pixel through its RGB values: each pixel in a coloured imaged is represented by three numbers. Those three numbers gave us the red-ness, the green-ness and the blue-ness, which is enough to get our model to work afterward. . For the problem at hand, we don&#39;t have the same easy way to characterize a user or a movie. There is probably relations with genres: if a given user likes romance, he is likely to put higher scores to romance movie. Or wether the movie is more action-centered vs heavy on dialogue. Or the presence of a specific actor that one use might particularly like. . How do we determine numbers to characterize those? The answer is, we don&#39;t. We will let our model learn them. By analyzing the existing relations between users and movies, let our model figure out itself the features that seem important or not. . This is what embeddings are. We will attribute to each of our users and each of our movie a random vector of a certain length (here n_factors=5), and we will make those learnable parameters. That means that at each step, when we compute the loss by comparing our predictions to our targets, we will compute the gradients of the loss with respect to those embedding vectors and update them with the rule of SGD (or another optimizer). . At the beginning, those numbers don&#39;t mean anything since we have chosen them randomly, but by the end of training, they will. By learning on existing data between users and movies, without having any other information, we will see that they still get some important features, and can isolate blockbusters from independent cinema, action movies from romance... . We are now in a position that we can create our whole model from scratch. . Collaborative filtering from scratch . Before we can write a model in PyTorch, we first need to learn the basics of object-oriented programming and Python. If you haven&#39;t done any object oriented programming before, we will give you a quick introduction here, but we would recommend looking up a tutorial and doing some practice before moving on. . The key idea in object-oriented programming is the class. We have been using classes throughout this book, such as DataLoader, string, and Learner. Python makes it easy for us to create new classes. Here is an example of a simple class: . class Example: def __init__(self, a): self.a = a def say(self,x): return f&#39;Hello {self.a}, {x}.&#39; . The most important piece of this is the special method called __init__ (pronounced dunder init). In Python, any method surrounded in double underscores like this is considered special. It indicates that there is some extra behaviour associated with this method name. In the case of __init__, this is the method which Python will call when your new object is created. So, this is where you can set up any state which needs to be done upon object creation. Any parameters included when the user constructs an instance of your class will be passed to the __init__ method is parameters. Note that the first parameter to any methods defined inside a class is self, so you can use this to set and get any attributes that you will need. . ex = Example(&#39;Sylvain&#39;) ex.say(&#39;nice to meet you&#39;) . &#39;Hello Sylvain, nice to meet you.&#39; . Also note that creating a new PyTorch module requires inheriting from Module. Inheritance is an important object-oriented concept which we will not discuss in detail here — in short, it means that we can add additional behaviour to an existing class. PyTorch already provides a Module class, which provides some basic foundations that we want to build on. So, we add the name of this super class after the name of the class that we are defining, as you see above. . The final thing that you need to know to create a new PyTorch module, is that when your module is called, PyTorch will call a method in your class called forward, and will pass along to that any parameters that are included in the call. Here is our dot product model: . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return (users * movies).sum(dim=1) . If you haven&#39;t seen object-oriented programming before, then don&#39;t worry, you won&#39;t need to use it much in this book. We are just mentioning this approach here, because most online tutorials and documentation will use the object-oriented syntax. . Note that the input of the model is a tensor of shape batch_size x 2, where the first columns (x[:, 0]) contains the user ids and the second column (x[:, 1]) contains the movie ids. As explained before, we use the embedding layers to represent our matrices of user and movie latent factors. . x,y = dls.one_batch() x.shape . torch.Size([64, 2]) . Now that we have defined our architecture, and created our parameter matrices, we need to create a Learner to optimize our model. In the past we have used special functions, such as cnn_learner, which set up everything for us for a particular application. Since we are doing things from scratch here, we will use the plain Learner class: . model = DotProduct(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) . We are now ready to fit our model: . learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 1.326261 | 1.295701 | 00:12 | . 1 | 1.091352 | 1.091475 | 00:11 | . 2 | 0.961574 | 0.977690 | 00:11 | . 3 | 0.829995 | 0.893122 | 00:11 | . 4 | 0.781661 | 0.876511 | 00:12 | . The first thing we can do to make this model a little bit better is to force those predictions between 0 and 5. For this, we just need to use sigmoid_range, like in the previous chapter. One thing we discovered empirically is that it&#39;s better to have the range go a little bit over 5, so we use (0, 5.5). . class DotProduct(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.movie_factors = Embedding(n_movies, n_factors) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) return sigmoid_range((users * movies).sum(dim=1), *self.y_range) . model = DotProduct(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 0.976380 | 1.001455 | 00:12 | . 1 | 0.875964 | 0.919960 | 00:12 | . 2 | 0.685377 | 0.870664 | 00:12 | . 3 | 0.483701 | 0.874071 | 00:12 | . 4 | 0.385249 | 0.878055 | 00:12 | . This is a reasonable start, but we can do better. One obvious missing piece is that some users are just more positive or negative in their recommendations and others, and some movies are just plain better or worse than others. But in our dot product representation we do not have any way to encode either of these things. If all you can say, for instance, about the movie is that it is very sci-fi, very action oriented, and very not old, then you don&#39;t really have any way to say most people like it. . That&#39;s because at this point we only have weights; we do not have biases. If we have a single number for each user which we add to our scores, and ditto for each movie, then this will handle this missing piece very nicely. So first of all, let&#39;s adjust our model architecture: . class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = Embedding(n_users, n_factors) self.user_bias = Embedding(n_users, 1) self.movie_factors = Embedding(n_movies, n_factors) self.movie_bias = Embedding(n_movies, 1) self.y_range = y_range def forward(self, x): users = self.user_factors(x[:,0]) movies = self.movie_factors(x[:,1]) res = (users * movies).sum(dim=1, keepdim=True) res += self.user_bias(x[:,0]) + self.movie_bias(x[:,1]) return sigmoid_range(res, *self.y_range) . Let&#39;s try training this and see how it goes: . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3) . epoch train_loss valid_loss time . 0 | 0.929161 | 0.936303 | 00:13 | . 1 | 0.820444 | 0.861306 | 00:13 | . 2 | 0.621612 | 0.865306 | 00:14 | . 3 | 0.404648 | 0.886448 | 00:13 | . 4 | 0.292948 | 0.892580 | 00:13 | . Instead of being better, it ends up being worse (at least at the end of training). Why is that? If we look at both trainings carefully, we can see the validation loss stopped improving in the middle and started to get worse. As we&#39;ve seen, this is a clear indication of overfitting. In this case, there is no way to use data augmentation, so we will have to use another regularisation technique. One approach that can be helpful is weight decay. . Weight decay . Weight decay, or L2 regularization, consists in adding to your loss function the sum of all the weights squared. Why do that? Because when we compute the gradients, it will add a contribution to them that will encourage the weights to be as small as possible. . Why would it prevent overfitting? The idea is that the larger the coefficient are, the more sharp canyons we will have in the loss function. If we take the basic example of parabola, y = a * (x**2), the larger a is, the more narrow the parabola is. . So by letting our model learn high parameters, it might fit all the data points in the training set with an over-complex function that has very sharp changes, which will lead to overfitting. . Limiting our weights from growing to much is going to hinder the training of the model, but it will yield to a state where it generalizes better. Going back to the theory a little bit, weight decay (or just wd) is a parameter that controls that sum of squares we add to our loss (assuming parameters is a tensor of all parameters): . loss_with_wd = loss + wd * (parameters**2).sum() . In practice though, it would be very inefficient (and maybe numerically unstable) to compute that big sum and add it to the loss. If you remember a little bit of high schoool math, you might recall that the derivative of p**2 with respect to p is 2*p, so adding that big sum to our loss is exactly the same as doing: . weight.grad += wd * 2 * weight . In practice, since wd is a parameter that we choose, we can just make it twice as bit, so we don&#39;t even need the *2 in the above equation. To use weight decay in fastai, just pass wd in your call to fit: . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.972090 | 0.962366 | 00:13 | . 1 | 0.875591 | 0.885106 | 00:13 | . 2 | 0.723798 | 0.839880 | 00:13 | . 3 | 0.586002 | 0.823225 | 00:13 | . 4 | 0.490980 | 0.823060 | 00:13 | . Much better! . Creating our own Embedding module . So far, we&#39;ve used Embedding without thinking about how it really works. Let&#39;s recreate DotProductBias without using this class. We&#39;ll need a randomly initialized weight matrix for each of the embeddings. We have to be careful, however. Recall from &lt;&gt; that optimizers require that they can get all the parameters of a module from a module&#39;s parameters() method. However, this does not happen fully automatically. If we just add a tensor as an attribute to a Module, it will not be included in parameters:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; class T(Module): def __init__(self): self.a = torch.ones(3) L(T().parameters()) . (#0) [] . To tell Module that we want to treat a tensor as parameters, we have to wrap it in the nn.Parameter class. This class doesn&#39;t actually add any functionality (other than automatically calling requires_grad_() for us). It&#39;s only used as a &quot;marker&quot; to show what to include in parameters(): . class T(Module): def __init__(self): self.a = nn.Parameter(torch.ones(3)) L(T().parameters()) . (#1) [Parameter containing: tensor([1., 1., 1.], requires_grad=True)] . All PyTorch modules use nn.Parameter for any trainable parameters, which is why we haven&#39;t needed to explicitly use this wrapper up until now: . class T(Module): def __init__(self): self.a = nn.Linear(1, 3, bias=False) t = T() L(t.parameters()) . (#1) [Parameter containing: tensor([[-0.9595], [-0.8490], [ 0.8159]], requires_grad=True)] . type(t.a.weight) . torch.nn.parameter.Parameter . We can create a tensor as a parameter, with random initialization, like so: . def create_params(size): return nn.Parameter(torch.zeros(*size).normal_(0, 0.01)) . Let&#39;s use this to create DotProductBias again, but without Embedding: . class DotProductBias(Module): def __init__(self, n_users, n_movies, n_factors, y_range=(0,5.5)): self.user_factors = create_params([n_users, n_factors]) self.user_bias = create_params([n_users]) self.movie_factors = create_params([n_movies, n_factors]) self.movie_bias = create_params([n_movies]) self.y_range = y_range def forward(self, x): users = self.user_factors[x[:,0]] movies = self.movie_factors[x[:,1]] res = (users*movies).sum(dim=1) res += self.user_bias[x[:,0]] + self.movie_bias[x[:,1]] return sigmoid_range(res, *self.y_range) . Let&#39;s train it again to check it&#39;s around the same results we saw in the previous section: . model = DotProductBias(n_users, n_movies, 50) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.962146 | 0.936952 | 00:14 | . 1 | 0.858084 | 0.884951 | 00:14 | . 2 | 0.740883 | 0.838549 | 00:14 | . 3 | 0.592497 | 0.823599 | 00:14 | . 4 | 0.473570 | 0.824263 | 00:14 | . Now, let&#39;s have a look at what our model has learned. . Interpreting embeddings and biases . Our model is already useful, in that it can provide us with recommendations for movies for our users — but it is also interesting to see what parameters it has discovered. The easiest to interpret are the biases. Here are the movies with the lowest values in the bias vector: . movie_bias = learn.model.movie_bias.weight.squeeze() idxs = movie_bias.argsort()[:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . [&#39;Children of the Corn: The Gathering (1996)&#39;, &#39;Lawnmower Man 2: Beyond Cyberspace (1996)&#39;, &#39;Beautician and the Beast, The (1997)&#39;, &#39;Crow: City of Angels, The (1996)&#39;, &#39;Home Alone 3 (1997)&#39;] . Have a think about what this means. What this is saying is, that for these movies, even when a user is very well matched to its latent factors (which, as we will see in a moment, tend to represent things like level of action, age of movie, and so forth) they still generally don&#39;t like it. We could have simply sorted movies directly by the average rating, but looking at their learned bias tells us something much more interesting. It tells us not just whether a movie is of a kind that people tend not to enjoy watching, but that people type like watching it even if it is of a kind that they would otherwise enjoy! By the same token, here are the movies with the highest bias: . idxs = movie_bias.argsort(descending=True)[:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . [&#39;L.A. Confidential (1997)&#39;, &#39;Titanic (1997)&#39;, &#39;Silence of the Lambs, The (1991)&#39;, &#39;Shawshank Redemption, The (1994)&#39;, &#39;Star Wars (1977)&#39;] . So, for instance, even if you don&#39;t normally enjoy detective movies, you might enjoy LA Confidential! . It is not quite so easy to directly interpret the embedding matrices. There is just too many factors for a human to look at. But there is a technique which can pull out the most important underlying directions in such a matrix, called principal component analysis (PCA). We will not be going into this in detail in this book, because it is not particularly important for you to understand to be a deep learning practitioner, but if you are interested then we suggest you check out the fast.ai course, Computational Linear Algebra for Coders. &lt;&gt; shows what our movies look like based on two of the strongest PCA components.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; We can see here that the model seems to have discovered a concept of classic versus pop culture movies, or perhaps it is critically acclaimed that is represented here. . j: no matter how many models I train, I never stop getting moved and surprised by how these randomly initialised bunches of numbers, trained with such simple mechanics, managed to discover things about my data all by themselves. It almost seems like cheating, that I can create code which does useful things, without ever actually telling it how to do those things! . We defined our model from scratch to teach you what is inside, but you can directly use the fastai library to build it. . Using fastai.collab . fastai can create and train a collaborative filtering model using the exact structure shown above by using collab_learner: . learn = collab_learner(dls, n_factors=50, y_range=(0, 5.5)) . learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 0.931751 | 0.953806 | 00:13 | . 1 | 0.851826 | 0.878119 | 00:13 | . 2 | 0.715254 | 0.834711 | 00:13 | . 3 | 0.583173 | 0.821470 | 00:13 | . 4 | 0.496625 | 0.821688 | 00:13 | . The names of the layers can be seen by printing the model . learn.model . EmbeddingDotBias( (u_weight): Embedding(944, 50) (i_weight): Embedding(1635, 50) (u_bias): Embedding(944, 1) (i_bias): Embedding(1635, 1) ) . We can use these to replicate any of the analyses we did in the previous section, for instance: . movie_bias = learn.model.i_bias.weight.squeeze() idxs = movie_bias.argsort(descending=True)[:5] [dls.classes[&#39;title&#39;][i] for i in idxs] . [&#39;Titanic (1997)&#39;, &#34;Schindler&#39;s List (1993)&#34;, &#39;Shawshank Redemption, The (1994)&#39;, &#39;L.A. Confidential (1997)&#39;, &#39;Silence of the Lambs, The (1991)&#39;] . An other interesting thing we can do with these learned embeddings is to look at distance. . Embedding distance . On a two-dimensional map we can calculate the distance between two coordinates using the formula of Pythagoras: $ sqrt{x^{2}+y^{2}}$ (assuming that X and Y are the distances between the coordinates on each axis). For a 50 dimensional embedding we can do exactly the same thing, except that we add up the squares of all 50 of the coordinate distances. . If there were two movies that were nearly identical, then there embedding vectors would also have to be nearly identical, because the users that would like them would be nearly exactly the same. There is a more general idea here: movie similarity can be defined by the similarity of users that like those movies. And that directly means that the distance between two movies&#39; embedding vectors can define that similarity. We can use this to find the most similar movie to Silence of the Lambs: . movie_factors = learn.model.i_weight.weight idx = dls.classes[&#39;title&#39;].o2i[&#39;Silence of the Lambs, The (1991)&#39;] distances = nn.CosineSimilarity(dim=1)(movie_factors, movie_factors[idx][None]) idx = distances.argsort(descending=True)[1] dls.classes[&#39;title&#39;][idx] . &#39;Dial M for Murder (1954)&#39; . Now that we have succesfully trained a model, let&#39;s see how to deal whwn we have no data for a new user, to be able to make recommandations to them. . Boot strapping a collaborative filtering model . The biggest challenge with using collaborative filtering models in practice is the bootstrapping problem. The most extreme version of this problem is when you have no users, and therefore no history to learn from. What product do you recommend to your very first user? . But even if you are a well-established company with a long history of user transactions, you still have the question: what do you do when a new user signs up? And indeed, what do you do when you add a new product to your portfolio? There is no magic solution to this problem, and really the solutions that we suggest are just variations of the form use your common sense. You can start your new users such that they have the mean of all of the embedding vectors of your other users — although this has the problem that that particular combination of latent factors may be not at all common (for instance the average for the science-fiction factor may be high, and the average for the action factor may be low, but it is not that common to find people who like science-fiction without action). Better would probably be to pick some particular user to represent average taste. . Better still is to use a tabular model based on user meta data to construct your initial embedding vector. When a user signs up, think about what questions you could ask them which could help you to understand their tastes. Then you can create a model where the dependent variable is a user&#39;s embedding vector, and the independent variables are the results of the questions that you ask them, along with their signup meta data. We will learn in the next section how to create these kinds of tabular models. You may have noticed that when you sign up for services such as Pandora and Netflix that they tend to ask you a few questions about what genres of movie or music that you like; this is how they come up with your initial collaborative filtering recommendations. . One thing to be careful of is that a small number of extremely enthusiastic users may end up effectively setting the recommendations for your whole user base. This is a very common problem, for instance, in movie recommendation systems. People that watch anime tend to watch a whole lot of it, and don&#39;t watch very much else, and spend a lot of time putting their ratings into websites. As a result, a lot of best ever movies lists tend to be heavily overrepresented with anime. In this particular case, it can be fairly obvious that you have a problem of representation bias, but if the bias is occurring in the latent factors then it may not be obvious at all. . Such a problem can change the entire make up of your user base, and the behaviour of your system. This is particularly true because of positive feedback loops. If a small number of your users tend to set the direction of your recommendation system, then they are naturally going to end up attracting more people like them to your system. And that will, of course, amplify the original representation bias. This is a natural tendency to be amplified exponentially. You may have seen examples of company executives expressing surprise at how their online platforms rapidly deteriorate in such a way that they express values that are at odds with the values of the founders. In the presence of these kinds of feedback loops, it is easy to see how such a divergence can happen both quickly, and in a way that is hidden until it is too late. . In a self-reinforcing system like this, we should probably expect these kinds of feedback loops to be the norm, not the exception. Therefore, you should assume that you will see them, plan for that, and identify upfront how you will deal with these issues. Try to think about all of the ways in which feedback loops may be represented in your system, and how you might be able to identify them in your data. In the end, this is coming back to our original advice about how to avoid disaster when rolling out any kind of machine learning system. It&#39;s all about ensuring that there are humans in the loop, that there is careful monitoring, and gradual and thoughtful rollout. . Our dot product model works quite well, and it is the basis of many successful real-world recommendation systems. This approach to collaborative filtering is known as probabilistic matrix factorisation (PMF). Another approach, which generally works similarly well given the same data, is deep learning. . Deep learning for collaborative filtering . To turn our architecture into a deep learning model the first step is to take the results of the embedding look up, and concatenating those activations together. This gives us a matrix which we can then pass through linear layers and nonlinearities in the usual way. . Since we&#39;ll be concatenating the embedding matrices, rather than taking their dot product, that means that the two embedding matrices can have different sizes (i.e. different numbers of latent factors). fastai has a function get_emb_sz that returns recommended sizes for embedding matrices for your data, based on a heuristic that fast.ai has found tends to work well in practice: . embs = get_emb_sz(dls) embs . [(944, 74), (1635, 101)] . Let&#39;s implement this class: . class CollabNN(Module): def __init__(self, user_sz, item_sz, y_range=(0,5.5), n_act=100): self.user_factors = Embedding(*user_sz) self.item_factors = Embedding(*item_sz) self.layers = nn.Sequential( nn.Linear(user_sz[1]+item_sz[1], n_act), nn.ReLU(), nn.Linear(n_act, 1)) self.y_range = y_range def forward(self, x): embs = self.user_factors(x[:,0]),self.item_factors(x[:,1]) x = self.layers(torch.cat(embs, dim=1)) return sigmoid_range(x, *self.y_range) . ...and use it to create a model: . model = CollabNN(*embs) . CollabNN creates our Embedding layers in the same way as previous classes in this chapter, except that we now use the embs sizes. Then self.layers is identical to the mini neural net we created in &lt;&gt; for MNIST. Then, in forward, we apply the embeddings, concatenate the results, and pass it through the mini neural net. Finally, we apply sigmoid_range as we have in previous models.&lt;/p&gt; Let&#39;s see if it trains: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.01) . epoch train_loss valid_loss time . 0 | 0.940104 | 0.959786 | 00:15 | . 1 | 0.893943 | 0.905222 | 00:14 | . 2 | 0.865591 | 0.875238 | 00:14 | . 3 | 0.800177 | 0.867468 | 00:14 | . 4 | 0.760255 | 0.867455 | 00:14 | . Fastai provides this model in fastai.collab, if you pass use_nn=True in your call to collab_learner (including calling get_emb_sz for you), plus lets you easily create more layers. For instance, here we&#39;re creating two hidden layers, of size 100 and 50, respectively: . learn = collab_learner(dls, use_nn=True, y_range=(0, 5.5), layers=[100,50]) learn.fit_one_cycle(5, 5e-3, wd=0.1) . epoch train_loss valid_loss time . 0 | 1.002747 | 0.972392 | 00:16 | . 1 | 0.926903 | 0.922348 | 00:16 | . 2 | 0.877160 | 0.893401 | 00:16 | . 3 | 0.838334 | 0.865040 | 00:16 | . 4 | 0.781666 | 0.864936 | 00:16 | . learn.model is an object of type EmbeddingNN. Let&#39;s take a look at fastai&#39;s code for this class: . @delegates(TabularModel) class EmbeddingNN(TabularModel): def __init__(self, emb_szs, layers, **kwargs): super().__init__(emb_szs, layers=layers, n_cont=0, out_sz=1, **kwargs) . Wow that&#39;s not a lot of code! This class inherits from TabularModel, which is where it gets all its functionality from. In __init__ is calls the same method in TabularModel, passing n_cont=0 and out_sz=1; other than that, it only passes along whatever arguments it received. . Sidebar: kwargs and delegates . EmbeddingNN includes **kwargs as a parameter to __init__. In python **kwargs in a parameter like means &quot;put any additional keyword arguments into a dict called kwarg. And **kwargs in an argument list means &quot;insert all key/value pairs in the kwargs dict as named arguments here&quot;. This approach is used in many popular libraries, such as matplotlib, in which the main plot function simply has the signature plot(*args, **kwargs). The plot documentation says &quot;The kwargs are Line2D properties&quot; and then lists those properties. . We&#39;re using **kwargs in EmbeddingNN to avoid having to write all the arguments to TabularModel a second time, and keep them in sync. However, this makes our API quite difficult to work with, because now Jupyter Notebook doesn&#39;t know what parameters are available, so things like tab-completion of parameter names and popup lists of signatures won&#39;t work. . Fastai resolves this by providing a special @delegates decorator, which automatically changes the signature of the class or function (EmbeddingNN in this case) to insert all of its keyword arguments into the signature . End sidebar . Although the results of EmbeddingNN are a bit worse than the dot product approach (which shows the power of carefully using an architecture for a domain), it does allow us to do something very important: we can now directly incorporate other user and movie information, time, and other information that may be relevant to the recommendation. That&#39;s exactly what TabularModel does. In fact, we&#39;ve now seen that EmbeddingNN is just a TabularModel, with n_cont=0 and out_sz=1. So we better spend some time learning about TabularModel, and how to use it to get great results! . TK Add a conclusion . Questionnaire . What problem does collaborative filtering solve? | How does it solve it? | Why might a collaborative filtering predictive model fail to be a very useful recommendation system? | What does a crosstab representation of collaborative filtering data look like? | Write the code to create a crosstab representation of the MovieLens data (you might need to do some web searching!) | What is a latent factor? Why is it &quot;latent&quot;? | What is a dot product? Calculate a dot product manually using pure python with lists. | What does pandas.DataFrame.merge do? | What is an embedding matrix? | What is the relationship between an embedding and a matrix of one-hot encoded vectors? | Why do we need Embedding if we could use one-hot encoded vectors for the same thing? | What does an embedding contain before we start training (assuming we&#39;re not using a prertained model)? | Create a class (without peeking, if possible!) and use it. | What does x[:,0] return? | Rewrite the DotProduct class (without peeking, if possible!) and train a model with it | What is a good loss function to use for MovieLens? Why? | What would happen if we used CrossEntropy loss with MovieLens? How would we need to change the model? | What is the use of bias in a dot product model? | What is another name for weight decay? | Write the equation for weight decay (without peeking!) | Write the equation for the gradient of weight decay. Why does it help reduce weights? | Why does reducing weights lead to better generalization? | What does argsort do in PyTorch? | Does sorting the movie biases give the same result as averaging overall movie ratings by movie? Why / why not? | How do you print the names and details of the layers in a model? | What is the &quot;bootstrapping problem&quot; in collaborative filtering? | How could you deal with the bootstrapping problem for new users? For new movies? | How can feedback loops impact collaborative filtering systems? | When using a neural network in collaborative filtering, why can we have different number of factors for movie and user? | Why is there a nn.Sequential in the CollabNN model? | What kind of model should be use if we want to add metadata about users and items, or information such as date and time, to a collaborative filter model? | Further research . Take a look at all the differences between the Embedding version of DotProductBias and the create_params version, and try to understand why each of those changes is required. If you&#39;re not sure, try reverting each change, to see what happens. (NB: even the type of brackets used in forward has changed!) | Find three other areas where collaborative filtering is being used, and find out what pros and cons of this approach in those areas. | Complete this notebook using the full MovieLens dataset, and compare your results to online benchmarks. See if you can improve your accuracy. Look on the book website and forum for ideas. Note that there are more columns in the full dataset--see if you can use those too (the next chapter might give you ideas) | Create a model for MovieLens with works with CrossEntropy loss, and compare it to the model in this chapter. | &lt;/div&gt; .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_collab.html",
            "relUrl": "/2020/03/19/_collab.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Title",
            "content": "[[chapter_arch_details]] Application architectures deep dive . We are now in the exciting position that we can fully understand the entire architectures that we have been using for our state-of-the-art models for computer vision, natural language processing, and tabular analysis. In this chapter, we&#39;re going to fill in all the missing details on how fastai&#39;s application models work and show you how to build the models they use. . We will also go back to the custom data preprocessing pipeline we saw in &lt;&gt; for Siamese networks and show you how you can use the components in the fastai library to build custom pretrained models for new tasks.&lt;/p&gt; We will go voer each application in turn, starting with computer vision. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Computer vision . In computer vision, we used the functions cnn_learner and unet_learner to build our models, depending on the task. Let&#39;s see how they start from a pretrained ResNet to build the Learner objects we have used in part 1 and 2 of this book. . cnn_learner . Let&#39;s take a look at what happens when we use the cnn_learner function. We pass it an architecture to use for the body of the network. Most of the time we use a resnet, which we already know how to create, so we don&#39;t need to delve into that any further. Pretrained weights are downloaded as required and loaded into the resnet. . Then, for transfer learning, the network needs to be cut. This refers to slicing off the final layer, which is only responsible for ImageNet-specific categorisation. In fact, we do not only slice off this layer, but everything from the adaptive average pooling layer onwards. The reason for this will become clear in just a moment. Since different architectures might use different types of pooling layers, or even completely different kinds of heads, we don&#39;t just search for the adaptive pooling layer to decide where to cut the pretrained model. Instead, we have a dictionary of information that is used for each model to know where its body ends, and its head starts. We call this model_meta — here it is for resnet 50: . model_meta[resnet50] . {&#39;cut&#39;: -2, &#39;split&#39;: &lt;function fastai2.vision.learner._resnet_split(m)&gt;, &#39;stats&#39;: ([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])} . jargon: Body and Head: The &quot;head&quot; of a neural net is the part that is specialized for a particular task. For a convnet, it&#39;s generally the part after the adaptive average pooling layer. The &quot;body&quot; is everything else, and includes the &quot;stem&quot; (which we learned about in &lt;&gt;).&lt;/p&gt; &lt;/blockquote&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; If we take all of the layers prior to the cutpoint of -2, we get the part of the model which fastai will keep for transfer learning. Now, we put on our new head. This is created using the function create_head: . #hide_output create_head(20,2) . Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): full: False (2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=20, out_features=512, bias=False) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=2, bias=False) ) . Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten() (2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=20, out_features=512, bias=False) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=2, bias=False) ) . With this function you can choose how many additional linear layers are added to the end, how much dropout to use after each one, and what kind of pooling to use. By default, fastai will apply both average pooling, and max pooling, and will concatenate the two together (this is the AdaptiveConcatPool2d layer). This is not a particularly common approach, but it was developed independently at fastai and at other research labs in recent years, and tends to provide some small improvement over using just average pooling. . Fastai is also a bit different to most libraries in adding two linear layers, rather than one, by default in the CNN head. The reason for this is that transfer learning can still be useful even, as we have seen, and transferring two very different domains to the pretrained model. However, just using a single linear layer is unlikely to be enough. So we have found that using two linear layers can allow transfer learning to be used more quickly and easily, in more situations. . . Note: One parameter to create_head that is worth looking at is bn_final. Setting this to true will cause a batchnorm layer to be added as your final layer. This can be useful in helping your model to more easily ensure that it is scaled appropriately for your output activations. We haven&#8217;t seen this approach published anywhere, as yet, but we have found that it works well in practice, wherever we have used it. . Let&#39;s now have a look at what unet_learner did in the segmentation problem we showed in &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; unet_learner . One of the most interesting architectures in deep learning is the one that we used for segmentation in &lt;&gt;. Segmentation is a challenging task, because the output required is really an image, or a pixel grid, containing the predicted label for every pixel. There are other tasks which share a similar basic design, such as increasing the resolution of an image (super resolution), adding colour to a black-and-white image (colorization), or converting a photo into a synthetic painting (style transfer)--these tasks are covered by an online chapter of this book, so be sure to check it out after you&#39;ve read this chapter. In each case, we are starting with an image, and converting it to some other image of the same dimensions or aspect ratio, but with the pixels converted in some way. We refer to these as generative vision models.&lt;/p&gt; The way we do this is to start with the exact same approach to developing a CNN head as we saw above. We start with a ResNet, for instance, and cut off the adaptive pooling layer and everything after that. And then we replace that with our custom head which does the generative task. . There was a lot of handwaving in that last sentence! How on earth do we create a CNN head which generates an image? If we start with, say, a 224 pixel input image, then at the end of the resnet body we will have a 7x7 grid of convolutional activations. How can we convert that into a 224 pixel segmentation mask? . We will (naturally) do this with a neural network! So we need some kind of layer which can increase the grid size in a CNN. One very simple approach to this is to replace every pixel in the 7x7 grid with four pixels in a 2x2 square. Each of those four pixels would have the same value — this is known as nearest neighbour interpolation. PyTorch provides a layer which does this for us, so we could create a head which contains stride one convolutional layers (along with batchnorm and ReLU as usual) interspersed with 2x2 nearest neighbour interpolation layers. In fact, you could try this now! See if you can create a custom head designed like this, and see if it can complete the CamVid segmentation task. You should find that you get some reasonable results, although it won&#39;t be as good as our &lt;&gt; results.&lt;/p&gt; Another approach is to replace the nearest neighbour and convolution combination with a transposed convolution otherwise known as a stride half convolution. This is identical to a regular convolution, but first zero padding is inserted between every pixel in the input. This is easiest to see with a picture — &lt;&gt; shows a diagram from the excellent convolutional arithmetic paper we have seen before, showing a 3x3 transposed convolution applied to a 3x3 image.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; A transposed convolution . As you see, the result of this is to increase the size of the input. You can try this out now, by using fastai&#39;s ConvLayer class; pass the parameter transpose=True to create a transposed convolution, instead of a regular one, in your custom head. . Neither of these approaches, however, works really well. The problem is that our 7x7 grid simply doesn&#39;t have enough information to create a 224x224 pixel output. It&#39;s asking an awful lot of the activations of each of those grid cells to have enough information to fully regenerate every pixel in the output. The solution to this problem is to use skip connections, like in a resnet, but skipping from the activations in the body of the resnet all the way over to the activations of the transposed convolution on the opposite side of the architecture. This is known as a U-Net, and it was developed in the 2015 paper U-Net: Convolutional Networks for Biomedical Image Segmentation. Although the paper focussed on medical applications, the U-Net has revolutionized all kinds of generation vision models. . &lt;&gt; shows the U-Net architecture (form the paper).&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The U-net architecture . This picture shows the CNN body on the left (in this case, it&#39;s a regular CNN, not a ResNet, and they&#39;re using 2x2 max pooling instead of stride 2 convolutions, since this paper was written before ResNets came along) and it shows the transposed convolutional layers on the right (they&#39;re called &quot;up-conv&quot; in this picture). Then then extra skip connections are shown as grey arrows crossing from left to right (these are sometimes called cross connections). You can see why it&#39;s called a &quot;U-net&quot; when you see this picture! . With this architecture, the input to the transposed convolutions is not just the lower resolution grid in the preceding layer, but also the higher resolution grid in the resnet head. This allows the U-Net to use all of the information of the original image, as it is needed. One challenge with U-Nets is that the exact architecture depends on the image size. fastai has a unique DynamicUnet class which auto-generates an architecture of the right size based on the data provided. . Let&#39;s focus now on an example where we leverage the fastai library to write a custom model: . A Siamese network . Let&#39;s go back to the input pipeline we set up in &lt;&gt; for a Siamese network. If your remember, it consisted of pair of images with the label being True or False, depending on if they were in the same class or not.&lt;/p&gt; Using what we just saw, let&#39;s build a custom model for this task and train it. How? We will use a pretrained architecture and pass our two images throught it. Then we can concatenate the results and send them to a custom head that will return two predictions. In terms of modules, this looks like this: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; class SiameseModel(Module): def __init__(self, encoder, head): self.encoder,self.head = encoder,head def forward(self, x1, x2): ftrs = torch.cat([self.encoder(x1), self.encoder(x2)], dim=1) return self.head(ftrs) . To create our encoder, we just need to take a pretrained model and cut it, as we explained before. The function create_body does that for us, we just have to pass it the place we want to cut. If we remember our look in the dictionary of metadata for pretrained models, the cut value for a resnet is -2: . encoder = create_body(resnet34, cut=-2) . Then we can create our head. A look at the encoder tells us the last layer has 512 features, so this head will need to receive 512*4. Why 4? First we have to multiply by 2 because we have two images. Then we need a second multiplication by 2 because of our concat-pool trick. . head = create_head(512*4, 2, ps=0.5) . With our encoder and head, we can now build our model. . model = SiameseModel(encoder, head) . Before using Learner, we have two more things to define. First, we must define the loss function we want to use. It&#39;s regular cross entropy, but since our targets are booleans, we need to convert them to integers or PyTorch will throw an error. . def loss_func(out, targ): return nn.CrossEntropyLoss()(out, targ.long()) . More importantly, to take full advantage of transfer learning, we have to define a custom splitter. A splitter is a function that tells the fastai library how to split the model in several parameter groups. This is what is used behind the scenes not only train the head of a model when we do transfert learning. . Here we want two parameter groups: one for the encoder and one for the head. We can thus define the following splitter (params is jsut a function that returns all parameters of a given module): . def siamese_splitter(model): return [params(model.encoder), params(model.head)] . Then we can define our Learner by passing the data, model, loss function, spliiter and any metric we want. Since we are not using a convenience function from fastai for transfer learning (like cnn_learner), we have to call learn.freeze manually. This will make sure only the last parameter groups (in this case, the head) is trained. . learn = Learner(dls, model, loss_func=loss_func, splitter=siamese_splitter, metrics=accuracy) learn.freeze() . Then we can directly train our model with the usual methods: . learn.fit_one_cycle(4, 3e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.367015 | 0.281242 | 0.885656 | 00:26 | . 1 | 0.307688 | 0.214721 | 0.915426 | 00:26 | . 2 | 0.275221 | 0.170615 | 0.936401 | 00:26 | . 3 | 0.223771 | 0.159633 | 0.943843 | 00:26 | . Before unfreezing and training a bit more with discriminative learning rates... . learn.unfreeze() learn.fit_one_cycle(4, slice(1e-6,1e-4)) . epoch train_loss valid_loss accuracy time . 0 | 0.212744 | 0.159033 | 0.944520 | 00:35 | . 1 | 0.201893 | 0.159615 | 0.942490 | 00:35 | . 2 | 0.204606 | 0.152338 | 0.945196 | 00:36 | . 3 | 0.213203 | 0.148346 | 0.947903 | 00:36 | . 94.8% is very good when we remember a classifier trained the same way (with no data augmentation) had an arror rate of 7%. . Now that we&#39;ve seen how to create complete state of the art computer vision models, let&#39;s move on to NLP. . Natural language processing . Converting an AWD-LSTM language model into a transfer learning classifier as we have done in &lt;&gt; follows a very similar process to what we saw for cnn_learner in the first section of this chapter. We do not need a &quot;meta&quot; dictionary in this case, because we do not have such a variety of architectures to support in the body. All we need to do is to select the stacked RNN for the encoder in the language model, which is a single PyTorch module. This encoder will provide an activation for every word of the input, because a language model needs to output a prediction for every next word.&lt;/p&gt; To create a classifier from this we use an approach described in the ULMFiT paper as &quot;BPTT for Text Classification (BPT3C)&quot;. The paper describes this: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; : In order to make fine-tuning a classifier for large documents feasible, we propose BPTT for Text Classification (BPT3C): We divide the document into fixed-length batches of size b. At the beginning of each batch, the model is initialized with the final state of the previous batch; we keep track of the hidden states for mean and max-pooling; gradients are back-propagated to the batches whose hidden states contributed to the final prediction. In practice, we use variable length backpropagation sequences. . In practice, what this is saying is that the classifier contains a for loop, which loops over each batch of a sequence. The state is maintained across batches, and the activations of each batch are stored. At the end, we use the same average and max concatenated pooling trick that we use for computer vision models — but this time, we do not pool over CNN grid cells, but over RNN sequences. . For this for loop we need to gather our data in batches, but each text needs to be treated separately, as they each have their own label. However, it&#39;s very likely that those texts won&#39;t have the good taste of being all of the same length, which means we won&#39;t be able to put them all in the same array, like we did with the language model. . That&#39;s where padding is going to help: when grabbing a bunch of texts, we determine the one with the greater length, then we fill the ones that are shorter with a special token called xxpad. To avoid having an extreme case where we have a text with 2,000 tokens in the same batch as a text with 10 tokens (so a lot of padding, and a lot of wasted computation) we alter the randomness by making sure texts of comparable size are put together. It will still be in a somewhat random order for the training set (for the validation set we can simply sort them by order of length), but not completely random. . This is done automatically behind the scenes by the fastai library when creating our DataLoaders. . The last application where we used fastai&#39;s model we haven&#39;t shown you yet is tabular. . Tabular . Finally, we can look at fastai.tabular models. (We don&#39;t need to look at collaborative filtering separately, since we&#39;ve already seen that these models are just tabular models, or use dot product, which we&#39;ve implemented earlier from scratch. . Here is the forward method for TabularModel: . if self.n_emb != 0: x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)] x = torch.cat(x, 1) x = self.emb_drop(x) if self.n_cont != 0: x_cont = self.bn_cont(x_cont) x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont return self.layers(x) . We won&#39;t show __init__ here, since it&#39;s not that interesting, but will look at each line of code in turn in forward: . if self.n_emb != 0: . This is just testing whether there are any embeddings to deal with — we can skip this section if we only have continuous variables. . x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)] . self.embeds contains the embedding matrices, so this gets the activations of each… . x = torch.cat(x, 1) . …and concatenates them into a single tensor. . x = self.emb_drop(x) . Then dropout is applied. You can pass emb_drop to __init__ to change this value. . if self.n_cont != 0: . Now we test whether there are any continuous variables to deal with. . x_cont = self.bn_cont(x_cont) . They are passed through a batchnorm layer… . x = torch.cat([x, x_cont], 1) if self.n_emb != 0 else x_cont . …and concatenated with the embedding activations, if there were any. . return self.layers(x) . Finally, this is passed through the linear layers (each of which includes batchnorm, if use_bn is True, and dropout, if ps is set to some value or list of values). . Congratulations! Now, you know every single piece of the architectures used in the fastai library! . Wrapping up architectures . As you can see, the details of deep learning architectures need not scare you now. You can look inside the code of fastai and PyTorch and see just what is going on. More importantly, try to understand why that is going on. Take a look at the papers that are being implemented in the code, and try to see how the code matches up to the algorithms that are described. . Now that we have investigated all of the pieces of a model and the data that is passed into it, we can consider what this means for practical deep learning. If you have unlimited data, unlimited memory, and unlimited time, then the advice is easy: train a huge model on all of your data for a really long time. The reason that deep learning is not straightforward is because your data, memory, and time is limited. If you are running out of memory or time, then the solution is to train a smaller model. If you are not able to train for long enough to overfit, then you are not taking advantage of the capacity of your model. . So step one is to get to the point that you can overfit. Then, the question is how to reduce that overfitting. &lt;&gt; shows how we recommend prioritising the steps from there.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Steps to reducing over-fitting . Many practitioners when faced with an overfitting model start at exactly the wrong end of this diagram. Their starting point is to use a smaller model, or more regularisation. Using a smaller model should be absolutely the last step you take, unless your model is taking up too much time or memory. Reducing the size of your model as reducing the ability of your model to learn subtle relationships in your data. . Instead, your first step should be to seek to create more data. That could involve adding more labels to data that you already have in your organisation, finding additional tasks that your model could be asked to solve (or to think of it another way, identifying different kinds of labels that you could model), or creating additional synthetic data via using more or different data augmentation. Thanks to the development of mixup and similar approaches, effective data augmentation is now available for nearly all kinds of data. . Once you&#39;ve got as much data as you think you can reasonably get a hold of, and are using it as effectively as possible by taking advantage of all of the labels that you can find, and all of the augmentation that make sense, if you are still overfitting and you should think about using more generalisable architectures. For instance, adding batch normalisation may improve generalisation. . If you are still overfitting after doing the best you can at using your data and tuning your architecture, then you can take a look at regularisation. Generally speaking, adding dropout to the last layer or two will do a good job of regularising your model. However, as we learnt from the story of the development of AWD-LSTM, it is often the case that adding dropout of different types throughout your model can help regularise even better. Generally speaking, a larger model with more regularisation is more flexible, and can therefore be more accurate, and a smaller model with less regularisation. . Only after considering all of these options would be recommend that you try using smaller versions of your architectures. . Questionnaire . What is the head of a neural net? | What is the body of a neural net? | What is &quot;cutting&quot; a neural net? Why do we need to do this for transfer learning? | What is &quot;model_meta&quot;? Try printing it to see what&#39;s inside. | Read the source code for create_head and make sure you understand what each line does. | Look at the output of create_head and make sure you understand why each layer is there, and how the create_head source created it. | Figure out how to change the dropout, layer size, and number of layers created by create_cnn, and see if you can find values that result in better accuracy from the pet recognizer. | What does AdaptiveConcatPool2d do? | What is nearest neighbor interpolation? How can it be used to upsample convolutional activations? | What is a transposed convolution? What is another name for it? | Create a conv layer with transpose=True and apply it to an image. Check the output shape. | Draw the u-net architecture. | What is BPTT for Text Classification (BPT3C)? | How do we handle different length sequences in BPT3C? | Try to run each line of TabularModel.forward separately, one line per cell, in a notebook, and look at the input and output shapes at each step. | How is self.layers defined in TabularModel? | What are the five steps for preventing over-fitting? | Why don&#39;t we reduce architecture complexity before trying other approaches to preventing over-fitting? | Further research . Write your own custom head and try training the pet recognizer with it. See if you can get a better result than fastai&#39;s default. | Try switching between AdaptiveConcatPool2d and AdaptiveAvgPool2d in a CNN head and see what difference it makes. | Write your own custom splitter to create a separate parameter group for every resnet block, and a separate group for the stem. Try training with it, and see if it improves the pet recognizer. | Read the online chapter about generative image models, and create your own colorizer, super resolution model, or style transfer model. | Create a custom head using nearest neighbor interpolation and use it to do segmentation on Camvid. | &lt;/div&gt; . . .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_arch_details.html",
            "relUrl": "/2020/03/19/_arch_details.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Title",
            "content": "[[chapter_accel_sgd]] The training process . Since we now know how to create state-of-the-art architectures for computer vision, natural image processing, tabular analysis, and collaborative filtering, and we know how to train them quickly, we&#39;re done, right? Not quite yet. We still have to explorea little bit more the training process. . We explained in &lt;&gt; the basis of Stochastic Gradient Descent: pass a minibatch in the model, compare it to our target with the loss function then compute the gradients of this loss function with regards to each weight before updating the weights with the formula:&lt;/p&gt; new_weight = weight - lr * weight.grad . We implemented this from scratch in a training loop, and also saw that Pytorch provides a simple nn.SGD class that does this calculation for each parameter for us. In this chapter, we will build some faster optimizers, using a flexible foundation. But that&#39;s not all what we might want to change in the training process. For any tweak of the training loop, we will need a way to add some code to the basis of SGD. The fastai library has a system of callbacks to do this, and we will teach you all about it. . Firs things first, let&#39;s start with standard SGD to get a baseline, then we will introduce most commonly used optimizers. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Let&#39;s start with SGD . First, we&#39;ll create a baseline, using plain SGD, and compare it to fastai&#39;s default optimizer. We&#39;ll start by grabbing Imagenette with the same get_data we used in &lt;&gt;:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; dls = get_data(URLs.IMAGENETTE_160, 160, 128) . We&#39;ll create a ResNet34 without pretraining, and pass along any arguments received: . def get_learner(**kwargs): return cnn_learner(dls, resnet34, pretrained=False, metrics=accuracy, **kwargs).to_fp16() . Here&#39;s the default fastai optimizer, with the usual 3e-3 learning rate: . learn = get_learner() learn.fit_one_cycle(3, 0.003) . epoch train_loss valid_loss accuracy time . 0 | 2.571932 | 2.685040 | 0.322548 | 00:11 | . 1 | 1.904674 | 1.852589 | 0.437452 | 00:11 | . 2 | 1.586909 | 1.374908 | 0.594904 | 00:11 | . Now let&#39;s try plain SGD. We can pass opt_func (optimization function) to cnn_learner to get fastai to use any optimizer: . learn = get_learner(opt_func=SGD) . The first thing to look at is lr_find: . learn.lr_find() . (0.017378008365631102, 3.019951861915615e-07) . It looks like we&#39;ll need to use a higher learning rate than we normally use: . learn.fit_one_cycle(3, 0.03, moms=(0,0,0)) . epoch train_loss valid_loss accuracy time . 0 | 2.969412 | 2.214596 | 0.242038 | 00:09 | . 1 | 2.442730 | 1.845950 | 0.362548 | 00:09 | . 2 | 2.157159 | 1.741143 | 0.408917 | 00:09 | . (Because accelerated SGD using momentum with is such a good idea, fastai uses it by default in fit_one_cycle, so we turn it off with moms=(0,0,0); we&#39;ll be learning about momentum shortly.) . Clearly, plain SGD isn&#39;t training as fast as we&#39;d like. So let&#39;s learn the tricks to get accelerated training! . A generic optimizer . In order to build up our accelerated SGD tricks, we&#39;ll need to start with a nice flexible optimizer foundation. No library prior to fastai provided such a foundation, but during fastai&#39;s development we realized that all optimizer improvements we&#39;d seen in the academic literature could be handled using optimizer callbacks. These are small pieces of code that an optimizer can add to the optimizer step. They are called by fastai&#39;s Optimizer class. This is a small class (less than a screen of code); these are the definitions in Optimizer of the two key methods that we&#39;ve been using in this book: . def zero_grad(self): for p,*_ in self.all_params(): p.grad.detach_() p.grad.zero_() def step(self): for p,pg,state,hyper in self.all_params(): for cb in self.cbs: state = _update(state, cb(p, **{**state, **hyper})) self.state[p] = state . As we saw when training an MNIST model from scratch, zero_grad just loops through the parameters of the model and sets the gradients to zero. It also calls detach_, which removes any history of gradient computation, since it won&#39;t be needed after zero_grad. . The more interesting method is step, which loops through the callbacks (cbs) and calls them to update the parameters (the _update function just calls state.update if there&#39;s anything returned by cb(...)). As you can see, Optimizer doesn&#39;t actually do any SGD steps itself. Let&#39;s see how we can add SGD to Optimizer. . Here&#39;s an optimizer callback that does a single SGD step, by multiplying -lr by the gradients, and adding that to the parameter (when Tensor.add_ in PyTorch is passed two parameters, they are multiplied together before the addition): . def sgd_cb(p, lr, **kwargs): p.data.add_(-lr, p.grad.data) . We can pass this to Optimizer using the cbs parameter; we&#39;ll need to use partial since Learner will call this function to create our optimizer later: . opt_func = partial(Optimizer, cbs=[sgd_step]) . Let&#39;s see if this trains: . learn = get_learner(opt_func=opt_func) learn.fit(3, 0.03) . epoch train_loss valid_loss accuracy time . 0 | 2.730918 | 2.009971 | 0.332739 | 00:09 | . 1 | 2.204893 | 1.747202 | 0.441529 | 00:09 | . 2 | 1.875621 | 1.684515 | 0.445350 | 00:09 | . It&#39;s working! So that&#39;s how we create SGD from scratch in fastai. Now let&#39;s see see what this momentum is exactly. . Momentum . SGD is the idea of taking a step in the direction of the steepest slope at each point of time. But what if we have a ball rolling down the mountain? It won&#39;t, at each given point, exactly follow the direction of the gradient, as it will have momentum. A ball with more momentum (for instance, a heavier ball) will skip over little bumps and holes, and be more likely to get to the bottom of a bumpy mountain. A ping pong ball, on the other hand, will get stuck in every little crevice. . So how could we bring this idea over to SGD? We can use a moving average, instead of only the current gradient, to make our step: . weight.avg = beta * weight.avg + (1-beta) * weight.grad new_weight = weight - lr * weight.avg . Here beta is some number we choose which defines how much momentum to use. If beta is zero, then the first equation above becomes weight.avg = weight.grad, so we end up with plain SGD. But if it&#39;s a number close to one, then the main direction chosen is an average of previous steps. (If you have done a bit of statistics, you may recognize in the first equation an exponentially weighted moving average, which is very often used to denoise data and get the underlying tendency.) . Note that we are writing weight.avg to highlight the fact we need to store thoe moving averages for each parameter of the model (and they all their own independent moving averages). . &lt;&gt; shows an example of noisy data for a single parameter, with the momentum curve plotted in red, and the gradients of the parameter plotted in blue. The gradients increase, and then decrease, and the momentum does a good job of following the general trend, without getting too influenced by noise.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; It works particularly well if the loss function has narrow canyons we need to navigate: vanilla SGD would send us from one side to the other while SGD with momentum will average those to roll down inside. The parameter beta determines the strength of that momentum we are using: with a small beta we stay closer to the actual gradient values whereas with a high beta, we will mostly go in the direction of the average of the gradients and it will take a while before any change in the gradients makes that trend move. . With a large beta, we might miss that the gradients have changed directions and roll over a small local minima which is a desired side-effect: intuitively, when we show a new picture/text/data to our model, it will look like something in the training set but won&#39;t be exactly like it. That means it will correspond to a point in the loss function that is closest to the minimum we ended up with at the end of training, but not exactly at that minimum. We then would rather end up training in a wide minimum, where nearby points have approximately the same loss (or if you prefer, a point where the loss is as flat as possible). &lt;&gt; shows how the chart in &lt;&gt; varies as we change beta.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; We can see in these examples that a beta that&#39;s too high results in the overall changes in gradient getting ignored. In SGD with momentum, a value of beta that is often used is 0.9. . fit_one_cycle by default starts with a beta of 0.95, gradually adjusts it to 0.85, and then gradually moves it back to 0.95 at the end of training. Let&#39;s see how our training goes with momentum added to plain SGD: . In order to add momentum to our optimizer, we&#39;ll first need to keep track of the moving average gradient, which we can do with another callback. When an optimizer callback returns a dict, it is used to update the state of the optimizer, and is passed back to the optimizer on the next step. So this callback will keep track of the gradient averages in a parameter called grad_avg: . def average_grad(p, mom, grad_avg=None, **kwargs): if grad_avg is None: grad_avg = torch.zeros_like(p.grad.data) return {&#39;grad_avg&#39;: grad_avg*mom + p.grad.data} . To use it, we just have to replace p.grad.data with grad_avg in our step function: . def momentum_step(p, lr, grad_avg, **kwargs): p.data.add_(-lr, grad_avg) . opt_func = partial(Optimizer, cbs=[average_grad,momentum_step], mom=0.9) . Learner will automatically schedule mom and lr, so fit_one_cycle will even work with our custom Optimizer: . learn = get_learner(opt_func=opt_func) learn.fit_one_cycle(3, 0.03) . epoch train_loss valid_loss accuracy time . 0 | 2.856000 | 2.493429 | 0.246115 | 00:10 | . 1 | 2.504205 | 2.463813 | 0.348280 | 00:10 | . 2 | 2.187387 | 1.755670 | 0.418853 | 00:10 | . learn.recorder.plot_sched() . We&#39;re still not getting great results, so let&#39;s see what else we can do. . RMSProp . RMSProp is another variant of SGD introduced by Geoffrey Hinton in Lecture 6e of his Coursera class. The main difference with SGD is that it uses an adaptive learning rate: instead of using the same learning rate for every parameter, each parameter gets it&#39;s own specific learning rate controlled by a global learning rate. That way we can speed up training by giving a high learning rate to the weights that needs to change a lot while the ones that are good enough get a lower learning rate. . How do we decide which parameter should have a high learning rate and which should not? We can look at the gradients to get an idea. Not just the one we computed, but all of them: if they have been close to 0 for a while, it means this parameter will need a higher learning rate because the loss is very flat. On the opposite, if they are all over the place, we should probably be careful and pick a low learning rate to avoid divergence. We can&#39;t just average the gradients to see if they&#39;re changing a lot, since the average of a large positive and a large negative number is close to zero. So we can use the usual trick of either taking the absolute value, or the squared values (and then taking the square root after the mean). . Once again, to pick the general tendency behind the noise, we will use a moving average, specifically the moving average of the gradients squared. Then, we will update the corresponding weight by using the current gradient (for the direction) divided by the square root of this moving average (that way if it&#39;s low, the effective learning rate will be higher, and if it&#39;s big, the effective learning rate will be lower). . w.square_avg = alpha * w.square_avg + (1-alpha) * (w.grad ** 2) new_w = w - lr * w.grad / math.sqrt(w.square_avg + eps) . The eps (epsilon) is added for numerical stability (usually set at 1e-8) and the default value for alpha is usually 0.99. . We can add this to Optimizer by doing much the same thing we did for avg_grad, but with an extra **2: . def average_sqr_grad(p, sqr_mom, sqr_avg=None, **kwargs): if sqr_avg is None: sqr_avg = torch.zeros_like(p.grad.data) return {&#39;sqr_avg&#39;: sqr_avg*sqr_mom + p.grad.data**2} . And we can define our step function and optimizer as before: . def rms_prop_step(p, lr, sqr_avg, eps, grad_avg=None, **kwargs): denom = sqr_avg.sqrt().add_(eps) p.data.addcdiv_(-lr, p.grad, denom) opt_func = partial(Optimizer, cbs=[average_sqr_grad,rms_prop_step], sqr_mom=0.99, eps=1e-7) . Let&#39;s try it out: . learn = get_learner(opt_func=opt_func) learn.fit_one_cycle(3, 0.003) . epoch train_loss valid_loss accuracy time . 0 | 2.766912 | 1.845900 | 0.402548 | 00:11 | . 1 | 2.194586 | 1.510269 | 0.504459 | 00:11 | . 2 | 1.869099 | 1.447939 | 0.544968 | 00:11 | . Much better! Now we just have to bring these ideas together, and we have Adam, fastai&#39;s default optimizer. . Adam . Adam mixes the ideas of SGD with momentum and RMSProp together: it uses the moving average of the gradients as a direction and divides by the square root of the moving average of the gradients squared to give an adaptive learning rate to each parameter. . There is one other difference with how Adam calculates moving averages, is that it takes the unbiased moving average which is: . w.avg = beta * w.avg + (1-beta) * w.grad unbias_avg = w.avg / (1 - (beta**(i+1))) . if we are the i-th iteration (starting at 0 like python does). This divisor of 1 - (beta**(i+1)) makes sure the unbiased average looks more like the gradients at the beginning (since beta &lt; 1 the denominator is very quickly very close to 1). . Putting everything together, our update step looks like: . w.avg = beta1 * w.avg + (1-beta1) * w.grad unbias_avg = w.avg / (1 - (beta1**(i+1))) w.sqr_avg = beta2 * w.sqr_avg + (1-beta2) * (w.grad ** 2) new_w = w - lr * unbias_avg / sqrt(w.sqr_avg + eps) . Like for RMSProp, eps is usually set to 1e-8, and the default for (beta1,beta2) suggested by the literature (0.9,0.999). . In fastai, Adam is the default optimizer we use since it allows faster training, but we found that beta2=0.99 is better suited for the type of schedule we are using. beta1 is the momentum parameter, which we specify with the argument moms in our call to fit_one_cycle. As for eps, fastai uses a default of 1e-5. eps is not just useful for numerical stability. A higher eps limits the maximum value of the adjusted learning rate. To take an extreme example, if eps is 1, then the adjusted learning will never be higher than the base learning rate. . Rather than show all the code for this in the book, we&#39;ll let you look at the optimizer notebook in fastai&#39;s GitHub repository--you&#39;ll see all the code we&#39;ve seen so far, along with Adam and other optimizers, and lots of examples and tests. . One thing that changes when we go from SGD to Adam is the way we apply weight decay, and it can have important consequences. . Decoupled weight_decay . We&#39;ve discussed weight decay before, which is equivalent to (in the case of vanilla SGD) updating the parameters with: . new_weight = weight - lr*weight.grad - lr*wd*weight . This last formula explains why the name of this technique is weight decay, as each weight is decayed by a factor lr * wd. . However, this only works correctly for standard SGD, because we have seen that with momentum, RMSProp or in Adam, the update has some additional formulas around the gradient. In those cases, the formula that comes from L2 regularization: . weight.grad += wd*weight . is different than weight decay: . new_weight = weight - lr*weight.grad - lr*wd*weight . Most libraries use the first formulation, but it was pointed out in Decoupled Weight Regularization by Ilya Loshchilov and Frank Hutter, second one is the only correct approach with the Adam optimizer or momentum, which is why fastai makes it its default. . Now you know everything that is hidden behind the line learn.fit_one_cycle! . OPtimizers are only one part of the training process. When you need to change the training loop with fastai, you can&#39;t directly change the code inside the library. Instead, we have designed a system of callbacks to let you write any tweak in independent blocks you can then mix and match. . Callbacks . Sometimes you need to change how things work a little bit. In fact, we have already seen examples of this: mixup, FP16 training, resetting the model after each epoch for training RNNs, and so forth. How do we go about making these kinds of tweaks to the training process? . We&#39;ve seen the basic training loop, which, with the help of the Optimizer class, looks like this for a single epoch: . for xb,yb in dl: loss = loss_func(model(xb), yb) loss.backward() opt.step() opt.zero_grad() . &lt;&gt; shows how to picture that.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Basic training loop . The usual way for deep learning practitioners to customise the training loop is to make a copy of an existing training loop, and then insert their code necessary for their particular changes into it. This is how nearly all code that you find online will look. But it has some very serious problems. . It&#39;s not very likely that some particular tweaked training loop is going to meet your particular needs. There are hundreds of changes that can be made to a training loop, which means there are billions and billions of possible permutations. You can&#39;t just copy one tweak from a training loop here, another from a training loop there, and expect them all to work together. Each will be based on different assumptions about the environment that it&#39;s working in, use different naming conventions, and expect the data to be in different formats. . We need a way to allow users to insert their own code at any part of the training loop, but in a consistent and well-defined way. Computer scientists have already come up with an answer to this question: the callback. A callback is a piece of code that you write, and inject into another piece of code at some predefined point. In fact, callbacks have been used with deep learning training loops for years. The problem is that only a small subset of places that may require code injection have been available in previous libraries, and, more importantly, callbacks were not able to do all the things they needed to do. . In order to be just as flexible as manually copying and pasting a training loop and directly inserting code into it, a callback must be able to read every possible piece of information available in the training loop, modify all of it as needed, and fully control when a batch, epoch, or even all the whole training loop should be terminated. fastai is the first library to provide all of this functionality. It modifies the training loop so it looks like &lt;&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Training loop with callbacks . The real test of whether this works has been borne out over the last couple of years — it has turned out that every single new paper implemented, or use a request fulfilled, for modifying the training loop has successfully been achieved entirely by using the fastai callback system. The training loop itself has not required modifications. &lt;&gt; shows just a few of the callbacks that have been added.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Some fastai callbacks . The reason that this is important for all of us is that it means that whatever idea we have in our head, we can implement it. We need never dig into the source code of PyTorch or fastai and act together some one-off system to try out our ideas. And when we do implement our own callbacks to develop our own ideas, we know that they will work together with all of the other functionality provided by fastai – so we will get progress bars, mixed precision training, hyperparameter annealing, and so forth. . Another advantage is that it makes it easy to gradually remove or add functionality and perform ablation studies. You just need to adjust the list of callbacks you pass along to your fit function. . As an example, here is the fastai source code that is run for each batch of the training loop: . try: self._split(b); self(&#39;begin_batch&#39;) self.pred = self.model(*self.xb); self(&#39;after_pred&#39;) self.loss = self.loss_func(self.pred, *self.yb); self(&#39;after_loss&#39;) if not self.training: return self.loss.backward(); self(&#39;after_backward&#39;) self.opt.step(); self(&#39;after_step&#39;) self.opt.zero_grad() except CancelBatchException: self(&#39;after_cancel_batch&#39;) finally: self(&#39;after_batch&#39;) . The calls of the form self(&#39;...&#39;) are where the callbacks are called. As you see, after every step a callback is called. The callback will receive the entire state of training, and can also modify it. For instance, as you see above, the input data and target labels are in self.xb and self.yb respectively. A callback can modify these to modify the data the training loop sees. It can also modify self.loss, or even modify the gradients. . Let&#39;s see how this work in practice by writing a Callback. . Creating a callback . When you want to write your own callback, the full list of available events is: . begin_fit:: called before doing anything, ideal for initial setup. | begin_epoch:: called at the beginning of each epoch, useful for any behavior you need to reset at each epoch. | begin_train:: called at the beginning of the training part of an epoch. | begin_batch:: called at the beginning of each batch, just after drawing said batch. It can be used to do any setup necessary for the batch (like hyper-parameter scheduling) or to change the input/target before it goes in the model (change of the input with techniques like mixup for instance). | after_pred:: called after computing the output of the model on the batch. It can be used to change that output before it&#39;s fed to the loss. | after_loss:: called after the loss has been computed, but before the backward pass. It can be used to add any penalty to the loss (AR or TAR in RNN training for instance). | after_backward:: called after the backward pass, but before the update of the parameters. It can be used to do any change to the gradients before said update (gradient clipping for instance). | after_step:: called after the step and before the gradients are zeroed. | after_batch:: called at the end of a batch, for any clean-up before the next one. | after_train:: called at the end of the training phase of an epoch. | begin_validate:: called at the beginning of the validation phase of an epoch, useful for any setup needed specifically for validation. | after_validate:: called at the end of the validation part of an epoch. | after_epoch:: called at the end of an epoch, for any clean-up before the next one. | after_fit:: called at the end of training, for final clean-up. | . This list is available as attributes of the special variable event; so just type event. and hit Tab in your notebook to see a list of all the options . Let&#39;s take a look at an example. Do you recall how in &lt;&gt; we needed to ensure that our special reset method was called at the start of training and validation for each epoch? We used the ModelReseter callback provided by fastai to do this for us. But how did ModelReseter do that exactly? Here&#39;s the full actual source code to that class:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; class ModelReseter(Callback): def begin_train(self): self.model.reset() def begin_validate(self): self.model.reset() . Yes, that&#39;s actually it! It just does what we said in the paragraph above: after completing training and epoch or validation for an epoch, call a method named reset. . Callbacks are often &quot;short and sweet&quot; like this one. In fact, let&#39;s look at one more. Here&#39;s the fastai source for the callback that add RNN regularization (AR and TAR): . class RNNRegularizer(Callback): def __init__(self, alpha=0., beta=0.): self.alpha,self.beta = alpha,beta def after_pred(self): self.raw_out,self.out = self.pred[1],self.pred[2] self.learn.pred = self.pred[0] def after_loss(self): if not self.training: return if self.alpha != 0.: self.learn.loss += self.alpha * self.out[-1].float().pow(2).mean() if self.beta != 0.: h = self.raw_out[-1] if len(h)&gt;1: self.learn.loss += self.beta * (h[:,1:] - h[:,:-1] ).float().pow(2).mean() . stop: Go back to where we discussed TAR and AR regularization, and compare to the code here. Made sure you understand what it&#39;s doing, and why. . In both of these examples, notice how we can access attributes of the training loop by directly checking self.model or self.pred. That&#39;s because a Callback will always try to get an attribute it doesn&#39;t have inside the Learner associated to it. This is a shortcut for self.learn.model or self.learn.pred. Note that this shortcut works for reading attributes, but not for writing them, which is why when RNNRegularizer changes the loss or the predictions, you see self.learn.loss = or self.learn.pred =. . When writing a callback, the following attributes of Learner are available: . model: the model used for training/validation | data: the underlying DataLoaders | loss_func: the loss function used | opt: the optimizer used to udpate the model parameters | opt_func: the function used to create the optimizer | cbs: the list containing all Callbacks | dl: current DataLoader used for iteration | x/xb: last input drawn from self.dl (potentially modified by callbacks). xb is always a tuple (potentially with one element) and x is detuplified. You can only assign to xb. | y/yb: last target drawn from self.dl (potentially modified by callbacks). yb is always a tuple (potentially with one element) and y is detuplified. You can only assign to yb. | pred: last predictions from self.model (potentially modified by callbacks) | loss: last computed loss (potentially modified by callbacks) | n_epoch: the number of epochs in this training | n_iter: the number of iterations in the current self.dl | epoch: the current epoch index (from 0 to n_epoch-1) | iter: the current iteration index in self.dl (from 0 to n_iter-1) | . The following attributes are added by TrainEvalCallback and should be available unless you went out of your way to remove that callback: . train_iter: the number of training iterations done since the beginning of this training | pct_train: from 0. to 1., the percentage of training iterations completed | training: flag to indicate if we&#39;re in training mode or not | . The following attribute is added by Recorder and should be available unless you went out of your way to remove that callback: . smooth_loss: an exponentially-averaged version of the training loss | . Callbacks can also interrupt any part of the training loop by using a system of exceptions. . Callback ordering and exceptions . Sometimes, callbacks need to be able to tell fastai to skip over a batch, or an epoch, or stop training altogether. For instance, consider TerminateOnNaNCallback. This handy callback will automatically stop training any time the loss becomes infinite or NaN (not a number). Here&#39;s the fastai source for this callback: . class TerminateOnNaNCallback(Callback): run_before=Recorder def after_batch(self): if torch.isinf(self.loss) or torch.isnan(self.loss): raise CancelFitException . The way it tells the training loop to interrupt training at this point is to raise CancelFitException. The training loop catches this exception and does not run any further training or validation. The callback control flow exceptions available are: . CancelFitException:: Skip the rest of this batch and go to `after_batch | CancelEpochException:: Skip the rest of the training part of the epoch and go to `after_train | CancelTrainException:: Skip the rest of the validation part of the epoch and go to `after_validate | CancelValidException:: Skip the rest of this epoch and go to `after_epoch | CancelBatchException:: Interrupts training and go to `after_fit | . You can detect one of those exceptions occurred and add code that executes right after with the following events: . after_cancel_batch:: reached immediately after a CancelBatchException before proceeding to after_batch | after_cancel_train:: reached immediately after a CancelTrainException before proceeding to after_epoch | after_cancel_valid:: reached immediately after a CancelValidException before proceeding to after_epoch | after_cancel_epoch:: reached immediately after a CancelEpochException before proceeding to after_epoch | after_cancel_fit:: reached immediately after a CancelFitException before proceeding to after_fit | . Sometimes, callbacks need to be called in a particular order. In the case of TerminateOnNaNCallback, it&#39;s important that Recorder runs its after_batch after this callback, to avoid registering an NaN loss. You can specify run_before (this callback must run before ...) or run_after (this callback must run after ...) in your callback to ensure the ordering that you need. . Now that we have seen how to tweak the training loop of fastai to do anything we need, let&#39;s take a step back and dig a little bit deeper in the foundations of that training loop. . TK Write a conclusion . Questionnaire . What is the equation for a step of SGD, in math or code (as you prefer)? | What do we pass to cnn_learner to use a non-default optimizer? | What are optimizer callbacks? | What does zero_grad do in an optimizer? | What does step do in an optimizer? How is it implemented in the general optimizer? | Rewrite sgd_cb to use the += operator, instead of add_. | What is momentum? Write out the equation. | What&#39;s a physical analogy for momentum? How does it apply in our model training settings? | What does a bigger value for momentum do to the gradients? | What are the default values of momentum for 1cycle training? | What is RMSProp? Write out the equation. | What do the squared values of the gradients indicate? | How does Adam differ from momentum and RMSProp? | Write out the equation for Adam. | Calculate the value of unbias_avg and w.avg for a few batches of dummy values. | What&#39;s the impact of having a high eps in Adam? | Read through the optimizer notebook in fastai&#39;s repo, and execute it. | In what situations do dynamic learning rate methods like Adam change the behaviour of weight decay? | What are the four steps of a training loop? | Why is the use of callbacks better than writing a new training loop for each tweak you want to add? | What are the necessary points in the design of the fastai&#39;s callback system that make it as flexible as copying and pasting bits of code? | How can you get the list of events available to you when writing a callback? | Write the ModelResetter callback (without peeking). | How can you access the necessary attributes of the training loop inside a callback? When can you use or not use the shortcut that goes with it? | How can a callback influence the control flow of the training loop. | Write the TerminateOnNaN callback (without peeking if possible). | How do you make sure your callback runs after or before another callback? | Further research . Look up the &quot;rectified Adam&quot; paper and implement it using the general optimizer framework, and try it out. Search for other recent optimizers that work well in practice, and pick one to implement. | Look at the mixed precision callback with the documentation. Try to understand what each event and line of code does. | Implement your own version of ther learning rate finder from scratch. Compare it with fastai&#39;s version. | Look at the source code of the callbacks that ship with fastai. See if you can find one that&#39;s similar to what you&#39;re looking to do, to get some inspiration. | Foundations of Deep Learning: Wrap up . Congratulations, you have made it to the end of the &quot;foundations of deep learning&quot; section. You now understand how all of fastai&#39;s applications and most important architectures are built, and the recommended ways to train them, and have all the information you need to build these from scratch. Whilst you probably won&#39;t need to create your own training loop, or batchnorm layer, for instance, knowing what is going on behind the scenes is very helpful for debugging, profiling, and deploying your solutions. . Since you understand all of the foundations of fastai&#39;s applications now, be sure to spend some time digging through fastai&#39;s source notebooks, and running and experimenting with parts of them, since you can and see exactly how everything in fastai is developed. . In the next section, we will be looking even further under the covers, to see how the actual forward and backward passes of a neural network are done, and we will see what tools are at our disposal to get better performance. We will then finish up with a project that brings together everything we have learned throughout the book, which we will use to build a method for interpreting convolutional neural networks. . &lt;/div&gt; . .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_accel_sgd.html",
            "relUrl": "/2020/03/19/_accel_sgd.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Title",
            "content": "[[chapter_cam]] CNN interpretation with CAM . Now that we know how to build up pretty much anything from scratch, let&#39;s use that knowledge to create entirely new (and very useful!) functionality: the class activation map. It gives a us an hindsight of why a CNN made the predictions it did. . In the process, we&#39;ll learn about one handy feature of PyTorch we haven&#39;t seen before, the hook, and we&#39;ll apply many of the concepts classes we&#39;ve learned in the rest of the book. If you want to really test out your understanding of the material in this book, after you&#39;ve finished this chapter, try putting the book aside, and recreate the ideas here yourself from scratch (no peaking!) . CAM and hooks . Class Activation Mapping (or CAM) was introduced by Zhou et al. in Learning Deep Features for Discriminative Localization. It uses the output of the last convolutional layer (just before our average pooling) together with the predictions to give us some heatmap visulaization of why the model made its decision. This is a useful tool for intepretation. . More precisely, at each position of our final convolutional layer we have has many filters as the last linear layer. We can then compute the dot product of those activations by the final weights to have, for each location on our feature map, the score of the feature that was used to make a decision. . We&#39;re going to need a way to get access to the activations inside the model while it&#39;s training. In PyTorch this can be done with a hook. Hooks are PyTorch&#39;s equivalent of fastai&#39;s callbacks. However rather than allowing you to inject code to the training loop like a fastai Learner callback, hooks allow you to inject code into the forward and backward calculations themselves. We can attach a hook to any layer of the model, and it will be executed when we compute the outputs (forward hook) or during backpropagation (backward hook). A forward hook has to be a function that takes three things: a module, its input and its output, and it can perform any behavior you want. (fastai also provides a handy HookCallback that we won&#39;t cover here, so take a look at the fastai docs; it makes working with hooks a little easier.) . We&#39;ll use the same cats and dogs model we trained in &lt;&gt;:&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; path = untar_data(URLs.PETS)/&#39;images&#39; def is_cat(x): return x[0].isupper() dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(224)) learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(1) . epoch train_loss valid_loss error_rate time . 0 | 0.181760 | 0.032238 | 0.009472 | 00:14 | . epoch train_loss valid_loss error_rate time . 0 | 0.059119 | 0.014090 | 0.002706 | 00:18 | . And we&#39;ll grab a cat picture and a batch of data: . img = PILImage.create(&#39;images/chapter1_cat_example.jpg&#39;) x, = first(dls.test_dl([img])) . For CAM we want to store the activations of the last convolutional layer. We put our hook function in a class so it has a state that we can access later, and just store a copy of the output: . class Hook(): def hook_func(self, m, i, o): self.stored = o.detach().clone() . We can then instantiate a Hook and attach it to the layer we want, which is the last layer of the CNN body. . hook_output = Hook() hook = learn.model[0].register_forward_hook(hook_output.hook_func) . We can then grab a batch and feed it through our model: . with torch.no_grad(): output = learn.model.eval()(x) . And we can access out stored activations! . act = hook_output.stored[0] . Let&#39;s also double-check our predictions: . F.softmax(output, dim=-1) . tensor([[2.7374e-09, 1.0000e+00]], device=&#39;cuda:5&#39;) . We know 0 is dog (for False) because the classes are automatically sorted in fastai. We can still double check by looking at dls.vocab: . dls.vocab . (#2) [False,True] . So our model is very confident this was a picture of a cat. . To do the dot product of our weight matrix (2 by number of activations) with the activations (batch size by activations by rows by cols) we use a custom einsum: . x.shape . torch.Size([1, 3, 224, 224]) . cam_map = torch.einsum(&#39;ck,kij-&gt;cij&#39;, learn.model[1][-1].weight, act) cam_map.shape . torch.Size([2, 7, 7]) . For each image in our batch, and for each class, we get a 7 by 7 feature map that tells us where the activations were higher vs lower. This will let us see which area of the pictures made the model take its decision. . For instance, the model decided this animal was a cat based on those areas (note that we need to decode the input x since it&#39;s been normalized by the DataLoader, and we need to cast to TensorImage since at the time this book is written PyTorch does not maintain types when indexing--this may be fixed by the time you are reading this): . x_dec = TensorImage(dls.train.decode((x,))[0][0]) _,ax = plt.subplots() x_dec.show(ctx=ax) ax.imshow(cam_map[0].detach().cpu(), alpha=0.6, extent=(0,224,224,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); . So the eye and the right ear were the two main areas that made the model decide it was a picture of a cat. . Once you&#39;re done with your hook, you should remove it otherwise it might leak some memory. . hook.remove() . That&#39;s why it&#39;s usually a good idea to have the Hook class be a context manager, registering the hook when you enter it and removing it when you exit. A &quot;context manager&quot; is a Python construct that calls __enter__ when the object is created in a with clause, and __exit__ at the end of the with clause. For instance, this is how Python handles the with open(...) as f: construct that you&#39;ll often see for opening files in Python, and not requiring an explicit close(f) at the end. . class Hook(): def __init__(self, m): self.hook = m.register_forward_hook(self.hook_func) def hook_func(self, m, i, o): self.stored = o.detach().clone() def __enter__(self, *args): return self def __exit__(self, *args): self.hook.remove() . That way, you can safely use it this way: . with Hook(learn.model[0]) as hook: with torch.no_grad(): output = learn.model.eval()(x.cuda()) act = hook.stored . fastai provides this Hook class for you, as well as some other handy classes to make working with hooks easier. . This method is useful but only works for the last layer. Gradient CAM is a variant that addreses this problem. . Gradient CAM . The method we just saw only lets us compute a heatmap with the last activations, since once we have our features, we have to multiply them by the last weight matrix. This won&#39;t work for inner layers in the network. A variant introduced in the paper Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization in 2016 uses the gradients of the final activation for the desired class: if you remember a little bit about the backward pass, the gradients of the output of the last layer with respect to the input of that layer is equal to the layer weights, since it is a linear layer. . With deeper layers, we still want the gradients, but they won&#39;t just be equal to the weights any more. We have to calculate them. The gradients of every layer are calculated for us by PyTorch during the backward pass, but they&#39;re not stored (except for tensors where requires_grad is True). We can, however, register a hook on the backward pass, which PyTorch will give the gradients to as a parameter, so we can store them there. We&#39;ll use a HookBwd class that will work like Hook, but intercepts and stores gradients, instead of activations: . class HookBwd(): def __init__(self, m): self.hook = m.register_backward_hook(self.hook_func) def hook_func(self, m, gi, go): self.stored = go[0].detach().clone() def __enter__(self, *args): return self def __exit__(self, *args): self.hook.remove() . Then for the class index 1 (for True, which is &#39;cat&#39;) we intercept the features of the last convolutional layer , as before, and compute the gradients of the output activation of our class. We can&#39;t just call output.backward(), because gradients only make sense with respect to a scalar (which is normally our loss), but output is a rank-2 tensor. But if we pick a single image (we&#39;ll use 0), and a single class (we&#39;ll use 1), then we can calculate the gradients of any weight or activation we like, with respect to that single value, using output[0,cls].backward(). Our hook intercepts the gradients that we&#39;ll use as weights. . cls = 1 with HookBwd(learn.model[0]) as hookg: with Hook(learn.model[0]) as hook: output = learn.model.eval()(x.cuda()) act = hook.stored output[0,cls].backward() grad = hookg.stored . The weights for our grad cam are given by the average of our gradients accross the feature map, then it&#39;s exactly the same as before: . w = grad[0].mean(dim=[1,2], keepdim=True) cam_map = (w * act[0]).sum(0) . _,ax = plt.subplots() x_dec.show(ctx=ax) ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); . The novelty with gradCAM is that we can use it on any layer, here the output of the second to last ResNet group: . with HookBwd(learn.model[0][-2]) as hookg: with Hook(learn.model[0][-2]) as hook: output = learn.model.eval()(x.cuda()) act = hook.stored output[0,cls].backward() grad = hookg.stored . w = grad[0].mean(dim=[1,2], keepdim=True) cam_map = (w * act[0]).sum(0) . ...and we can now view the activation map for this layer: . _,ax = plt.subplots() x_dec.show(ctx=ax) ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); . TK Write conclusion . Questionnaire . What is a hook in PyTorch? | Which layer does CAM use the outputs of? | Why does CAM require a hook? | Look at the source code of ActivationStats class and see how it uses hooks. | Write a hook that stores the activation of a given layer in a model (without peaking, if possible). | Why do we call eval before getting the activations? Why do we use no_grad? | Use torch.einsum to compute the &quot;dog&quot; or &quot;cat&quot; score of each of the locations in the last activation of the body of the model. | How do you check which orders the categories are in (i.e. the correspondence of index-&gt;category)? | Why are we using decode when displaying the input image? | What is a &quot;context manager&quot;? What special methods need to be defined to create one? | Why can&#39;t we use plain CAM for the inner layers of a network? | Why do we need to hook the backward pass in order to do GradCAM? | Why can&#39;t we call output.backward() when output is a rank-2 tensor of output activations per image per class? | Further research . Try removing keepdim and see what happens. Look up this parameter in the PyTorch docs. Why do we need it in this notebook? | Create a notebook like this one, but for NLP, and use it to find which words in a movie review are most significant in assessing sentiment of a particular movie review. | &lt;/div&gt; .",
            "url": "https://maxlein.github.io/fastbook/2020/03/19/_CAM.html",
            "relUrl": "/2020/03/19/_CAM.html",
            "date": " • Mar 19, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Chapter 3",
            "content": "Data Ethics . Sidebar: Acknowledgement: Dr Rachel Thomas . This chapter was co-authored by Dr Rachel Thomas, the co-founder of fast.ai, and founding director of the Center for Applied Data Ethics at the University of San Francisco. It largely follows a subset of the syllabus she developed for the &quot;Introduction to Data Ethics&quot; course. . End sidebar . As we discussed in Chapters 1 and 2, sometimes, machine learning models can go wrong. They can have bugs. They can be presented with data that they haven&#39;t seen before, and behave in ways we don&#39;t expect. Or, they could work exactly as designed, but be used for something that you would much prefer they were never ever used for. . Because deep learning is such a powerful tool and can be used for so many things, it becomes particularly important that we consider the consequences of our choices. The philosophical study of ethics is the study of right and wrong, including how we can define those terms, recognise right and wrong actions, and understand the connection between actions and consequences. The field of data ethics has been around for a long time, and there are many academics focused on this field. It is being used to help define policy in many jurisdictions; it is being used in companies big and small to consider how best to ensure good societal outcomes from product development; and it is being used by researchers who want to make sure that the work they are doing is used for good, and not for bad. . As a deep learning practitioner, therefore, it is likely that at some point you are going to be put in a situation where you need to consider data ethics. So what is data ethics? It&#39;s a subfield of ethics, so let&#39;s start there. . j: At university, philosophy of ethics was my main thing (it would have been the topic of my thesis, if I&#39;d finished it, instead of dropping out to join the real-world). Based on the years I spent studying ethics, I can tell you this: no one really agrees on what right and wrong are, whether they exist, how to spot them, which people are good, and which bad, or pretty much anything else. So don&#39;t expect too much from the theory! We&#39;re going to focus on examples and thought starters here, not theory. . In answering the question What is Ethics, The Markkula Center for Applied Ethics says that ethics refers to: . Well-founded standards of right and wrong that prescribe what humans ought to do, and | The study and development of one&#39;s ethical standards. | . There is no list of right answers for ethics. There is no list of dos and don&#39;ts. Ethics is complicated, and context-dependent. It involves the perspectives of many stakeholders. Ethics is a muscle that you have to develop and practice. In this chapter, our goal is to provide some signposts to help you on that journey. . Spotting ethical issues is best to do as part of a collaborative team. This is the only way you can really incorporate different perspectives. Different people&#39;s backgrounds will help them to see things which may not be obvious to you. Working with a team is helpful for many &quot;muscle building&quot; activities, including this one. . This chapter is certainly not the only part of the book where we talk about data ethics, but it&#39;s good to have a place where we focus on it for a while. To get oriented, it&#39;s perhaps easiest to look at a few examples. So we picked out three that we think illustrate effectively some of the key topics. . Key examples for data ethics . We are going to start with three specific examples that illustrate three common ethical issues in tech: . Recourse processes: Arkansas&#39;s buggy healthcare algorithms left patients stranded | Feedback loops: YouTube&#39;s recommendation system helped unleash a conspiracy theory boom | Bias: When a traditionally African-American name is searched for on Google, it displays ads for criminal background checks. | In fact, for every concept that we introduce in this chapter, we are going to provide at least one specific example. For each one, have a think about what you could have done in this situation, and think about what kinds of obstructions there might have been to you getting that done. How would you deal with them? What would you look out for? . Bugs and recourse: Buggy algorithm used for healthcare benefits . The Verge investigated software used in over half of the U.S. states to determine how much healthcare people receive, and documented their findings in an article What Happens When an Algorithm Cuts Your Healthcare. After implementation of the algorithm in Arkansas, people (many with severe disabilities) drastically had their healthcare cut. For instance, Tammy Dobbs, a woman with cerebral palsy who needs an aid to help her to get out of bed, to go to the bathroom, to get food, and more, had her hours of help suddenly reduced by 20 hours a week. She couldn’t get any explanation for why her healthcare was cut. Eventually, a court case revealed that there were mistakes in the software implementation of the algorithm, negatively impacting people with diabetes or cerebral palsy. However, Dobbs and many other people reliant on these health care benefits live in fear that their benefits could again be cut suddenly and inexplicably. . Feedback loops: YouTube&#39;s recommendation system . Feedback loops can occur when your model is controlling the next round of data you get. The data that is returned quickly becomes flawed by the software itself. . For instance, in &lt;&gt; we briefly mentioned the reinforcement learning algorithm which Google introduced for YouTube&#39;s recommendation system. YouTube has 1.9bn users, who watch over 1 billion hours of YouTube videos a day. Their algorithm, which was designed to optimise watch time, is responsible for around 70% of the content that is watched. It led to out-of-control feedback loops, leading the New York Times to run the headline &quot;YouTube Unleashed a Conspiracy Theory Boom. Can It Be Contained?&quot;. Ostensibly recommendation systems are predicting what content people will like, but they also have a lot of power in determining what content people even see.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Bias: Professor Lantanya Sweeney &quot;arrested&quot; . Dr. Latanya Sweeney is a professor at Harvard and director of their data privacy lab. In the paper Discrimination in Online Ad Delivery (see &lt;&gt;) she describes her discovery that googling her name resulted in advertisements saying &quot;Latanya Sweeney arrested&quot; even although she is the only Latanya Sweeney and has never been arrested. However when she googled other names, such as Kirsten Lindquist, she got more neutral ads, even though Kirsten Lindquist has been arrested three times.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Google search showing Professor Lantanya Sweeney &#39;arrested&#39; . Being a computer scientist, she studied this systematically, and looked at over 2000 names. She found that this pattern held where historically black names received advertisements suggesting that the person had a criminal record. Whereas, white names had more neutral advertisements. . This is an example of bias. It can make a big difference to people&#39;s lives — for instance, if a job applicant is googled then it may appear that they have a criminal record when they do not. . Why does this matter? . One very natural reaction to considering these issues is: &quot;So what? What&#39;s that got to do with me? I&#39;m a data scientist, not a politician. I&#39;m not one of the senior executives at my company who make the decisions about what we do. I&#39;m just trying to build the most predictive model I can.&quot; . These are very reasonable questions. But we&#39;re going to try to convince you that the answer is: everybody who is training models absolutely needs to consider how their model will be used. And to consider how to best ensure that it is used as positively as possible. There are things you can do. And if you don&#39;t do these things, then things can go pretty bad. . One particularly hideous example of what happens when technologists focus on technology at all costs is the story of IBM and Nazi Germany. A Swiss judge ruled &quot;It does not thus seem unreasonable to deduce that IBM&#39;s technical assistance facilitated the tasks of the Nazis in the commission of their crimes against humanity, acts also involving accountancy and classification by IBM machines and utilized in the concentration camps themselves.&quot; . IBM, you see, supplied the Nazis with data tabulation products necessary to track the extermination of Jews and other groups on a massive scale. This was driven from the top of the company, with marketing to Hitler and his leadership team. Company President Thomas Watson personally approved the 1939 release of special IBM alphabetizing machines to help organize the deportation of Polish Jews. Pictured here is Adolf Hitler (far left) meeting with IBM CEO Tom Watson Sr. (2nd from left), shortly before Hitler awarded Watson a special “Service to the Reich” medal in 1937: . IBM CEO Tom Watson Sr. meeting with Adolf Hitler . But it also happened throughout the organization. IBM and its subsidiaries provided regular training and maintenance on-site at the concentration camps: printing off cards, configuring machines, and repairing them as they broke frequently. IBM set up categorizations on their punch card system for the way that each person was killed, which group they were assigned to, and the logistical information necessary to track them through the vast Holocaust system. IBM&#39;s code for Jews in the concentration camps was 8, where around 6,000,000 were killed. Its code for Romanis was 12 (they were labeled by the Nazis as &quot;asocials&quot;, with over 300,000 killed in the Zigeunerlager, or “Gypsy camp”). General executions were coded as 4, death in the gas chambers as 6. . A punch card used by IBM in concentration camps . Of course, the project managers and engineers and technicians involved were just living their ordinary lives. Caring for their families, going to the church on Sunday, doing their jobs as best as they could. Following orders. The marketers were just doing what they could to meet their business development goals. Edwin Black, author of &quot;IBM and the Holocaust&quot;, said: &quot;To the blind technocrat, the means were more important than the ends. The destruction of the Jewish people became even less important because the invigorating nature of IBM&#39;s technical achievement was only heightened by the fantastical profits to be made at a time when bread lines stretched across the world.&quot; . Step back for a moment and consider: how would you feel if you discovered that you had been part of a system that ending up hurting society? Would you even know? Would you be open to finding out? How can you help make sure this doesn&#39;t happen? We have described the most extreme situation here in Nazi Germany, but there are many negative societal consequences happening due to AI and machine learning right now, some of which we&#39;ll describe in this chapter. . It&#39;s not just a moral burden either. Sometimes, technologists pay very directly for their actions. For instance, the first person who was jailed as a result of the Volkswagen scandal, where the car company cheated on their diesel emissions tests, was not the manager that oversaw the project, or an executive at the helm of the company. It was one of the engineers, James Liang, who just did what he was told. . On the other hand, if a project you are involved in turns out to make a huge positive impact on even one person, this is going to make you feel pretty great! . Okay, so hopefully we have convinced you that you ought to care. But what should you do? As data scientists, we&#39;re naturally inclined to focus on making our model better at optimizing some metric. But optimizing that metric may not actually lead to better outcomes. And even if optimizing that metric does help create better outcomes, it almost certainly won&#39;t be the only thing that matters. Consider the pipeline of steps that occurs between the development of a model or an algorithm by a researcher or practitioner, and the point at which this work is actually used to make some decision. This entire pipeline needs to be considered as a whole if we&#39;re to have a hope of getting the kinds of outcomes we want. . Normally there is a very long chain from one end to the other. This is especially true if you are a researcher where you don&#39;t even know if your research will ever get used for anything, or if you&#39;re involved in data collection, which is even earlier in the pipeline. But no-one is better placed to . Data often ends up being used for different purposes than why it was originally collected. IBM began selling to Nazi Germany well before the Holocaust, including helping with Germany’s 1933 census conducted by Adolf Hitler, which was effective at identifying far more Jewish people than had previously been recognized in Germany. US census data was used to round up Japanese-Americans (who were US citizens) for internment during World War II. It is important to recognize how data and images collected can be weaponized later. Columbia professor Tim Wu wrote that “You must assume that any personal data that Facebook or Android keeps are data that governments around the world will try to get or that thieves will try to steal.” . Integrating machine learning with product design . Presumably the reason you&#39;re doing this work is because you hope it will be used for something. Otherwise, you&#39;re just wasting your time. So, let&#39;s start with the assumption that your work will end up somewhere. Now, as you are collecting your data and developing your model, you are making lots of decisions. What level of aggregation will you store your data at? What loss function should you use? What validation and training sets should you use? Should you focus on simplicity of implementation, speed of inference, or accuracy of the model? How will your model handle out of domain data items? Can it be fine-tuned, or must it be retrained from scratch over time? . These are not just algorithm questions. They are data product design questions. But the product managers, executives, judges, journalists, doctors… whoever ends up developing and using the system of which your model is a part will not be well-placed to understand the decisions that you made, let alone change them. . For instance, two studies found that Amazon’s facial recognition software produced inaccurate and racially biased results. Amazon claimed that the researchers should have changed the default parameters, they did not explain how it would change the racially baised results. Furthermore, it turned out that Amazon was not instructing police departments that used its software to do this either. There was, presumably, a big distance between the researchers that developed these algorithms, and the Amazon documentation staff that wrote the guidelines provided to the police. A lack of tight integration led to serious problems for society, the police, and Amazon themselves. It turned out that their system erroneously matched 28 members of congress to criminal mugshots! (And these members of congress wrongly matched to criminal mugshots disproportionately included people of color as seen in &lt;&gt;.)&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Congressmen matched to criminal mugshots by Amazon software . Data scientists need to be part of a cross disciplinary team. And researchers need to work closely with the kinds of people who will end up using their research. Better still is if the domain experts themselves have learnt enough to be able to train and debug some models themselves — hopefully there&#39;s a few of you reading this book right now! . The modern workplace is a very specialised place. Everybody tends to have very well-defined jobs to perform. Especially in large companies, it can be very hard to know what all the pieces of the puzzle are. Sometimes companies even intentionally obscure the overall project goals that are being worked on, if they know that their employees are not going to like the answers. This is sometimes done by compartmentalising pieces as much as possible . In other words, we&#39;re not saying that any of this is easy. It&#39;s hard. It&#39;s really hard. We all have to do our best. And we have often seen that the people who do get involved in the higher-level context of these projects, and attempt to develop cross disciplinary capabilities and teams, become some of the most important and well rewarded parts of their organisations. It&#39;s the kind of work that tends to be highly appreciated by senior executives, even if it is considered, sometimes, rather uncomfortable by middle management. . Topics in Data Ethics . Data ethics is a big field, and we can&#39;t cover everything. Instead, we&#39;re going to pick a few topics which we think are particularly relevant: . need for recourse and accountability | feedback loops | bias | disinformation | . Let&#39;s look at each in turn. . Recourse and accountability . In a complex system, it is easy for no one person to feel responsible for outcomes. While this is understandable, it does not lead to good results. In the earlier example of the Arkansas healthcare system in which a bug led to people with cerebral palsy losing access to needed care, the creator of the algorithm blamed government officials, and government officials could blame those who implemented the software. NYU professor Danah Boyd described this phenomenon: &quot;bureaucracy has often been used to evade responsibility, and today&#39;s algorithmic systems are extending bureaucracy.&quot; . An additional reason why recourse is so necessary, is because data often contains errors. Mechanisms for audits and error-correction are crucial. A database of suspected gang members maintained by California law enforcement officials was found to be full of errors, including 42 babies who had been added to the database when they were less than 1 year old (28 of whom were marked as “admitting to being gang members”). In this case, there was no process in place for correcting mistakes or removing people once they’ve been added. Another example is the US credit report system; in a large-scale study of credit reports by the FTC (Federal Trade Commission) in 2012, it was found that 26% of consumers had at least one mistake in their files, and 5% had errors that could be devastating. Yet, the process of getting such errors corrected is incredibly slow and opaque. When public-radio reporter Bobby Allyn discovered that he was erroneously listed as having a firearms conviction, it took him &quot;more than a dozen phone calls, the handiwork of a county court clerk and six weeks to solve the problem. And that was only after I contacted the company’s communications department as a journalist.&quot; (as covered in the article How the careless errors of credit reporting agencies are ruining people’s lives) . As machine learning practitioners, we do not always think of it as our responsibility to understand how our algorithms and up being implemented in practice. But we need to. . Feedback loops . We have already explained in &lt;&gt; how an algorithm can interact with its enviromnent to create a feedback loop, making predictions that reinforce actions taken in the real world, which lead to predictions even more pronounced in the same direction. As an example, we&#39;ll discuss YouTube&#39;s recommendation system. A couple of years ago Google talked about how they had introduced reinforcement learning (closely related to deep learning, but where your loss function represents a result which could be a long time after an action occurs) to improve their recommendation system. They described how they used an algorithm which made recommendations such that watch time would be optimised.&lt;/p&gt; However, human beings tend to be drawn towards controversial content. This meant that videos about things like conspiracy theories started to get recommended more and more by the recommendation system. Furthermore, it turns out that the kinds of people that are interested in conspiracy theories are also people that watch a lot of online videos! So, they started to get drawn more and more towards YouTube. The increasing number of conspiracy theorists watching YouTube resulted in the algorithm recommending more and more conspiracy theories and other extremist content, which resulted in more extremists watching videos on YouTube, and more people watching YouTube developing extremist views, which led to the algorithm recommending more extremist content... The system became so out of control that in February 2019 it led the New York Times to run the headline &quot;YouTube Unleashed a Conspiracy Theory Boom. Can It Be Contained?&quot;footnote:[https://www.nytimes.com/2019/02/19/technology/youtube-conspiracy-stars.html] . The New York Times published another article on YouTube&#39;s recommendation system, titled On YouTube’s Digital Playground, an Open Gate for Pedophiles. The article started with this chilling story: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; : Christiane C. didn’t think anything of it when her 10-year-old daughter and a friend uploaded a video of themselves playing in a backyard pool… A few days later… the video had thousands of views. Before long, it had ticked up to 400,000... “I saw the video again and I got scared by the number of views,” Christiane said. She had reason to be. YouTube’s automated recommendation system… had begun showing the video to users who watched other videos of prepubescent, partially clothed children, a team of researchers has found.&gt; : On its own, each video might be perfectly innocent, a home movie, say, made by a child. Any revealing frames are fleeting and appear accidental. But, grouped together, their shared features become unmistakable. . YouTube&#39;s recommendation algorithm had begun curating playlists for pedophiles, picking out innocent home videos that happened to contain prepubescent, partially clothed children. . No one at Google planned to create a system that turned family videos into porn for pedophiles. So what happened? . Part of the problem here is the centrality of metrics in driving a financially important system. When an algorithm has a metric to optimise, as you have seen, it will do everything it can to optimise that number. This tends to lead to all kinds of edge cases, and humans interacting with a system will search for, find, and exploit these edge cases and feedback loops for their advantage. . There are signs that this is exactly what has happened with YouTube&#39;s recommendation system. The Guardian ran an article How an ex-YouTube insider investigated its secret algorithm about Guillaume Chaslot, an ex-YouTube engineer who created AlgoTransparency, which tracks these issues. Chaslot published the chart in &lt;&gt;, following the release of Robert Mueller&#39;s &quot;Report on the Investigation Into Russian Interference in the 2016 Presidential Election&quot;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Coverage of the Mueller report . Russia Today&#39;s coverage of the Mueller report was an extreme outlier in how many channels were recommending it. This suggests the possibility that Russia Today, a state-owned Russia media outlet, has been successful in gaming YouTube&#39;s recommendation algorithm. The lack of transparency of systems like this make it hard to uncover the kinds of problems that we&#39;re discussing. . One of our reviewers for this book, Aurélien Géron, led YouTube&#39;s video classification team from 2013 to 2016 (well before the events discussed above). He pointed out that it&#39;s not just feedback loops involving humans that are a problem. There can also be feedback loops without humans! He told us about an example from YouTube: . : &quot;One important signal to classify the main topic of a video is the channel it comes from. For example, a video uploaded to a cooking channel is very likely to be a cooking video. But how do we know what topic a channel is about? Well… in part by looking at the topics of the videos it contains! Do you see the loop? For example, many videos have a description which indicates what camera was used to shoot the video. As a result, some of these videos might get classified as videos about “photography”. If a channel has such as misclassified video, it might be classified as a “photography” channel, making it even more likely for future videos on this channel to be wrongly classified as “photography”. This could even lead to runaway virus-like classifications! One way to break this feedback loop is to classify videos with and without the channel signal. Then when classifying the channels, you can only use the classes obtained without the channel signal. This way, the feedback loop is broken.&quot; There are positive examples of people and organizations attempting to combat these problems. Evan Estola, lead machine learning engineer at Meetup, discussed the example of men expressing more interest than women in tech meetups. Meetup’s algorithm could recommend fewer tech meetups to women, and as a result, fewer women would find out about and attend tech meetups, which could cause the algorithm to suggest even fewer tech meetups to women, and so on in a self-reinforcing feedback loop. Evan and his team made the ethical decision for their recommendation algorithm to not create such a feedback loop, but explicitly not using gender for that part of their model. It is encouraging to see a company not just unthinkingly optimize a metric, but to consider their impact. &quot;You need to decide which feature not to use in your algorithm… the most optimal algorithm is perhaps not the best one to launch into production&quot;, he said. . While Meetup chose to avoid such an outcome, Facebook provides an example of allowing a runaway feedback loop to run wild. Facebook radicalizes users interested in one conspiracy theory by introducing them to more. As Renee DiResta, a researcher on proliferation of disinformation, writes: . : &quot;once people join a single conspiracy-minded [Facebook] group, they are algorithmically routed to a plethora of others. Join an anti-vaccine group, and your suggestions will include anti-GMO, chemtrail watch, flat Earther (yes, really), and ‘curing cancer naturally’ groups. Rather than pulling a user out of the rabbit hole, the recommendation engine pushes them further in.&quot; . It is extremely important to keep in mind this kind of behavior can happen, and to either anticipate a feedback loop or take positive action to break it when you can the first signs of it in your own projects. Another thing to keep in mind is bias, which, as we discussed in the previous chapter, can interact with feedback loops in very troublesome ways. . Bias . Discussions of bias online tend to get pretty confusing pretty fast. The word bias means so many different things. Statisticians often think that when data ethicists are talking about bias that they&#39;re talking about the statistical definition of the term bias. But they&#39;re not. And they&#39;re certainly not talking about the biases that appear in the weights and biases which are the parameters of your model! . What they&#39;re talking about is the social science concept of bias. In A Framework for Understanding Unintended Consequences of Machine Learning MIT&#39;s Suresh and Guttag describe six types of bias in machine learning, summarized in &lt;&gt; from their paper.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Bias in machine learning can come from multiple sources . We&#39;ll discuss four of these types of bias, those that we&#39;ve found most helpful in our own work (see the paper for details on the others). . Historical bias . Historical bias comes from the fact that people are biased, processes are biased, and society is biased. Suresh and Guttag say: &quot;Historical bias is a fundamental, structural issue with the first step of the data generation process and can exist even given perfect sampling and feature selection&quot;. . For instance, here&#39;s a few examples of historical race bias in the US, from the NY Times article Racial Bias, Even When We Have Good Intentions, by the University of Chicago&#39;s Sendhil Mullainathan: . When doctors were shown identical files, they were much less likely to recommend cardiac catheterization (a helpful procedure) to Black patients | When bargaining for a used car, Black people were offered initial prices $700 higher and received far smaller concessions | Responding to apartment-rental ads on Craigslist with a Black name elicited fewer responses than with a white name | An all-white jury was 16 percentage points more likely to convict a Black defendant than a white one, but when a jury had 1 Black member, it convicted both at same rate. | . The COMPAS algorithm, widely used for sentencing and bail decisions in the US, is an example of an important algorithm which, when tested by ProPublica, showed clear racial bias in practice: . Results of the COMPAS algorithm . Any dataset involving humans can have this kind of bias, such as medical data, sales data, housing data, political data, and so on. Because underlying bias is so pervasive, bias in datasets is very pervasive. Racial bias even turns up in computer vision, as shown in this example of auto-categorized photos shared on Twitter by a Google Photos user: . One of these labels is very wrong... . Yes, that is showing what you think it is: Google Photos classified a Black user&#39;s photo with their friend as &quot;gorillas&quot;! This algorithmic mis-step got a lot of attention in the media. “We’re appalled and genuinely sorry that this happened,” a company spokeswoman said. “There is still clearly a lot of work to do with automatic image labeling, and we’re looking at how we can prevent these types of mistakes from happening in the future.” . Unfortunately, fixing problems in machine learning systems when the input data has problems is hard. Google&#39;s first attempt didn&#39;t inspire confidence, as covered by The Guardian: . Google first response to the problem . These kinds of problem are certainly not limited to just Google. MIT researchers studied the most popular online computer vision APIs to see how accurate they were. But they didn&#39;t just calculate a single accuracy number—instead, they looked at the accuracy across four different groups: . Error rate per gender and race for various facial recognition systems . IBM&#39;s system, for instance, had a 34.7% error rate for darker females, vs 0.3% for lighter males—over 100 times more errors! Some people incorrectly reacted to these experiments by claiming that the difference was simply because darker skin is harder for computers to recognise. However, what actually happened, is after the negative publicity that this result created, all of the companies in question dramatically improved their models for darker skin, such that one year later they were nearly as good as for lighter skin. So what this actually showed is that the developers failed to utilise datasets containing enough darker faces, or test their product with darker faces. . One of the MIT researchers, Joy Buolamwini, warned, &quot;We have entered the age of automation overconfident yet underprepared. If we fail to make ethical and inclusive artificial intelligence, we risk losing gains made in civil rights and gender equity under the guise of machine neutrality&quot;. . Part of the issue appears to be a systematic imbalance in the make up of popular datasets used for training models. The abstract to the paper No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World states, &quot;We analyze two large, publicly available image data sets to assess geo-diversity and find that these data sets appear to exhibit an observable amerocentric and eurocentric representation bias. Further, we analyze classifiers trained on these data sets to assess the impact of these training distributions and find strong differences in the relative performance on images from different locales&quot;. &lt;&gt; shows one of the charts from the paper, showing the geographic make up of what was, at the time (and still, as this book is being written), the two most important image datasets for training models.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Image provenance in popular training sets . The vast majority of the images are from the United States and other Western countries, leading to models trained on ImageNet performing worse on scenes from other countries and cultures. For instance, research found that such models are worse at identifying household items (such as soap, spices, sofas, or beds) from lower-income countries. &lt;&gt; shows an image from the paper, Does Object Recognition Work for Everyone?.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Object detection in action . In this example, we can see that the lower-income soap example is a very long way away from being accurate, with every commercial image recognition service predicting &quot;food&quot; as the most likely answer! . As we will discuss shortly, in addition, the vast majority of AI researchers and developers are young white men. Most projects that we have seen do most user testing using friends and families of the immediate product development group. Given this, the kinds of problems we just discussed should not be surprising. . Similar historical bias is found in the texts used as data for natural language processing models. This crops up in downstream machine learning tasks in many ways. For instance, it was widely reported that until last year Google Translate showed systematic bias in how it translated the Turkish gender-neutral pronoun &quot;o&quot; into English. For instance, when applied to jobs which are often associated with males, it used &quot;he&quot;, and when applied to jobs which are often associated with females, it used &quot;she&quot;: . Gender bias in text data sets . We also see this kind of bias in online advertisements. For instance, a study in 2019 found that even when the person placing the ad does not intentionally discriminate, Facebook will show the ad to very different audiences based on race and gender. Housing ads with the same text, but changing the picture between a white or black family, were shown to racially different audiences. . Measurement bias . In the paper Does Machine Learning Automate Moral Hazard and Error in American Economic Review, the authors look at a model that tries to answer the question: using historical EHR data, what factors are most predictive of stroke? These are the top predictors from the model: . Prior Stroke | Cardiovascular disease | Accidental injury | Benign breast lump | Colonoscopy | Sinusitis | . However, only the top two have anything to do with a stroke! Based on what we&#39;ve studied so far, you can probably guess why. We haven’t really measured stroke, which occurs when a region of the brain is denied oxygen due to an interruption in the blood supply. What we’ve measured is who: had symptoms, went to a doctor, got the appropriate tests, AND received a diagnosis of stroke. Actually having a stroke is not the only thing correlated with this complete list — it&#39;s also correlated with being the kind of person who actually goes to the doctor (which is influenced by who has access to healthcare, can afford their co-pay, doesn&#39;t experience racial or gender-based medical discrimination, and more)! If you are likely to go to the doctor for an accidental injury, then you are likely to also go the doctor when you are having a stroke. . This is an example of measurement bias. It occurs when our models make mistakes because we are measuring the wrong thing, or measuring it in the wrong way, or incorporating that measurement into our model inappropriately. . Aggregation Bias . Aggregation bias occurs when models do not aggregate data in a way that incorporates all of the appropriate factors, or when a model does not include the necessary interaction terms, nonlinearities, or so forth. This can particularly occur in medical settings. For instance, the way diabetes is treated is often based on simple univariate statistics and studies involving small groups of heterogeneous people. Analysis of results is often done in a way that does not take account of different ethnicities or genders. However it turns out that diabetes patients have different complications across ethnicities, and HbA1c levels (widely used to diagnose and monitor diabetes) differ in complex ways across ethnicities and genders. This can result in people being misdiagnosed or incorrectly treated because medical decisions are based on a model which does not include these important variables and interactions. . Representation Bias . The abstract of the paper Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting notes that there is gender imbalance in occupations (e.g. females are more likely to be nurses, and males are more likely to be pastors), and says that: &quot;differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances&quot;. . What this is saying is that the researchers noticed that models predicting occupation did not only reflect the actual gender imbalance in the underlying population, but actually amplified it! This is quite common, particularly for simple models. When there is some clear, easy to see underlying relationship, a simple model will often simply assume that that relationship holds all the time. As &lt;&gt; from the paper shows, for occupations which had a higher percentage of females, the model tended to overestimate the prevalence of that occupation.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Model error in predicting occupation plotted against percentage of women in said occupation . For example, in the training dataset, 14.6% of surgeons were women, yet in the model predictions, only 11.6% of the true positives were women. The model is thus amplifying the bias existing in the training set. . Now that we saw those biases existed, what can we do to mitigate them? . Addressing different types of bias . Different types of bias require different approaches for mitigation. While gathering a more diverse dataset can address representation bias, this would not help with historical bias or measurement bias. All datasets contain bias. There is no such thing as a completely de-biased dataset. Many researchers in the field have been converging on a set of proposals towards better documenting the decisions, context, and specifics about how and why a particular dataset was created, what scenarios it is appropriate to use in, and what the limitations are. This way, those using the dataset will not be caught off-guard by its biases and limitations. . Humans are biased, so does algorithmic bias matter? . We often hear this question — &quot;humans are biased, so does algorithmic bias even matter?&quot; This comes up so often, there must be some reasoning that makes sense to the people that ask it, but it doesn&#39;t seem very logically sound to us! Independently of whether this is logically sound, it&#39;s important to realise that algorithms and people are different. Machine learning, particularly so. Consider these points about machine learning algorithms: . Machine learning can create feedback loops:: small amounts of bias can very rapidly, exponentially increase due to feedback loops | Machine learning can amplify bias:: human bias can lead to larger amounts of machine learning bias | Algorithms &amp; humans are used differently:: human decision makers and algorithmic decision makers are not used in a plug-and-play interchangeable way in practice. For instance, algorithmic decisions are more likely to be implemented at scale and without a process for recourse. Furthermore, people are more likely to mistakenly believe that the result of an algorithm is objective and error-free. | Technology is power:: And with that comes responsibility. | . As the Arkansas healthcare example showed, machine learning is often implemented in practice not because it leads to better outcomes, but because it is cheaper and more efficient. Cathy O&#39;Neill, in her book Weapons of Math Destruction, described the pattern of how the privileged are processed by people, the poor are processed by algorithms. This is just one of a number of ways that algorithms are used differently than human decision makers. Others include: . People are more likely to assume algorithms are objective or error-free (even if they’re given the option of a human override) | Algorithms are more likely to be implemented with no appeals process in place | Algorithms are often used at scale | Algorithmic systems are cheap. | . Even in the absence of bias, algorithms (and deep learning especially, since it is such an effective and scalable algorithm) can lead to negative societal problems, such as when used for disinformation. . Disinformation . Disinformation has a history stretching back hundreds or even thousands of years. It is not necessarily about getting someone to believe something false, but rather, often to sow disharmony and uncertainty, and to get people to give up on seeking the truth. Receiving conflicting accounts can lead people to assume that they can never know what to trust. . Some people think disinformation is primarily about false information or fake news, but in reality, disinformation can often contain seeds of truth, or involve half-truths taken out of context. Ladislav Bittman was an intelligence officer in the USSR who later defected to the United States and wrote some books in the 1970s and 1980s on the role of disinformation in Soviet propaganda operations. He said, &quot;Most campaigns are a carefully designed mixture of facts, half-truths, exaggerations, &amp; deliberate lies.&quot; . In the United States this has hit close to home in recent years, with the FBI detailing a massive disinformation campaign linked to Russia in the 2016 US election. Understanding the disinformation that was used in this campaign is very educational. For instance, the FBI found that the Russian disinformation campaign often organized two separate fake grass roots protests, one for each side of an issue, and got them to protest at the same time! The Houston Chronicle reported on one of these odd events: . : A group that called itself the &quot;Heart of Texas&quot; had organized it on social media — a protest, they said, against the &quot;Islamization&quot; of Texas. On one side of Travis Street, I found about 10 protesters. On the other side, I found around 50 counterprotesters. But I couldn&#39;t find the rally organizers. No &quot;Heart of Texas.&quot; I thought that was odd, and mentioned it in the article: What kind of group is a no-show at its own event? Now I know why. Apparently, the rally&#39;s organizers were in Saint Petersburg, Russia, at the time. &quot;Heart of Texas&quot; is one of the internet troll groups cited in Special Prosecutor Robert Mueller&#39;s recent indictment of Russians attempting to tamper with the U.S. presidential election. . Event organized by the group Heart of Texas . Disinformation often involves coordinated campaigns of inauthentic behavior. For instance, fraudulent accounts may try to make it seem like many people hold a particular viewpoint. While most of us like to think of ourselves as independent-minded, in reality we evolved to be influenced by others in our in-group, and in opposition to those in our out-group. Online discussions can influence our viewpoints, or alter the range of what we consider acceptable viewpoints. Humans are social animals, and as social animals we are extremely influenced by the people around us. Increasingly, radicalisation occurs in online environments. So influence is coming from people in the virtual space of online forums and social networks. . Disinformation through auto-generated text is a particularly significant issue, due to the greatly increased capability provided by deep learning. We discuss this issue in depth when we learn to create language models, in &lt;&gt;.&lt;/p&gt; One proposed approach is to develop some form of digital signature, implement it in a seamless way, and to create norms that we should only trust content which has been verified. Head of the Allen Institute on AI, Oren Etzioni, wrote such a proposal in an article titled How Will We Prevent AI-Based Forgery?, &quot;AI is poised to make high-fidelity forgery inexpensive and automated, leading to potentially disastrous consequences for democracy, security, and society. The specter of AI forgery means that we need to act to make digital signatures de rigueur as a means of authentication of digital content.&quot; . Whilst we can&#39;t hope to discuss all the ethical issues that deep learning, and algorithms more generally, bring up, hopefully this brief introduction has been a useful starting point you can build on. We&#39;ll now move on to the questions of how to identify ethical issues, and what to do about them. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Identifying and addressing ethical issues . Mistakes happen. Finding out about them, and dealing with them, needs to be part of the design of any system that includes machine learning (and many other systems too). The issues raised within data ethics are often complex and interdisciplinary, but it is crucial that we work to address them. . So what can we do? This is a big topic, but a few steps towards addressing ethical issues are: . analyze a project you are working on | implement processes at your company to find and address ethical risks | support good policy | increase diversity | . Let&#39;s walk through each step next, staring with analyzing a project you are working on. . Analyze a project you are working on . It&#39;s easy to miss important issues when considered ethical implications of your work. One thing that helps enormously is simply asking the right questions. Rachel Thomas recommends considering the following questions throughout the development of a data project: . Should we even be doing this? | What bias is in the data? | Can the code and data be audited? | What are error rates for different sub-groups? | What is the accuracy of a simple rule-based alternative? | What processes are in place to handle appeals or mistakes? | How diverse is the team that built it? | . These questions may be able to help you identify outstanding issues, and possible alternatives that are easier to understand and control. In addition to asking the right questions, it&#39;s also important to consider practices and processes to implement. . Processes to implement . The Markkula Center has released An Ethical Toolkit for Engineering/Design Practice, which includes some concrete practices to implement at your company, including regularly scheduled ethical risk sweeps to proactively search for ethical risks (in a manner similar to cybersecurity penetration testing), expanding the ethical circle to include the perspectives of a variety of stakeholders, and considering the terrible people (how could bad actors abuse, steal, misinterpret, hack, destroy, or weaponize what you are building?). . Even if you don&#39;t have a diverse team, you can still try to pro-actively include the perspectives of a wider group, considering questions such as these (provided by the Markkula Center): . Whose interests, desires, skills, experiences and values have we simply assumed, rather than actually consulted? | Who are all the stakeholders who will be directly affected by our product? How have their interests been protected? How do we know what their interests really are—have we asked? | Who/which groups and individuals will be indirectly affected in significant ways? | Who might use this product that we didn’t expect to use it, or for purposes we didn’t initially intend? | . Ethical Lenses . Another useful resource from the Markkula Center is Conceptual Frameworks in Technology and Engineering Practice. This considers how different foundational ethical lenses can help identify concrete issues, and lays out the following approaches and key questions: . The Rights Approach:: Which option best respects the rights of all who have a stake? | The Justice Approach:: Which option treats people equally or proportionately? | The Utilitarian Approach:: Which option will produce the most good and do the least harm? | The Common Good Approach:: Which option best serves the community as a whole, not just some members? | The Virtue Approach:: Which option leads me to act as the sort of person I want to be? | . Markkula&#39;s recommendations include a deeper dive into each of these perspectives, including looking at a project based on a focus on its consequences: . Who will be directly affected by this project? Who will be indirectly affected? | Will the effects in aggregate likely create more good than harm, and what types of good and harm? | Are we thinking about all relevant types of harm/benefit (psychological, political, environmental, moral, cognitive, emotional, institutional, cultural)? | How might future generations be affected by this project? | Do the risks of harm from this project fall disproportionately on the least powerful in society? Will the benefits go disproportionately the well-off? | Have we adequately considered ‘dual-use? | . The alternative lens to this is the deontological perspective, which focuses on basic right and wrong: . What rights of others &amp; duties to others must we respect? | How might the dignity &amp; autonomy of each stakeholder be impacted by this project? | What considerations of trust &amp; of justice are relevant to this design/project? | Does this project involve any conflicting moral duties to others, or conflicting stakeholder rights? How can we prioritize these? | . One of the best ways to help come up with complete and thoughtful answers to questions like these is to ensure that the people asking the questions are diverse. . The power of diversity . Currently, less than 12% of AI researchers are women, according to a study from element AI. The statistics are similarly dire when it comes to race and age. When everybody on a team has similar backgrounds, they are likely to have similar blindspots around ethical risks. The Harvard Business Review (HBR) has published a number of studies showing many benefits of diverse teams, including: . How Diversity Can Drive Innovation | Teams Solve Problems Faster When They’re More Cognitively Diverse | Why Diverse Teams Are Smarter, and | What Makes a Team Smarter? More Women. | . Diversity can lead to problems being identified earlier, and a wider range of solutions being considered. For instance, Tracy Chou was an early engineer at Quora. She wrote of her experiences, describing how she advocated internally for adding a feature that would allow trolls and other bad actors to be blocked. Chou recounts, “I was eager to work on the feature because I personally felt antagonized and abused on the site (gender isn’t an unlikely reason as to why)... But if I hadn’t had that personal perspective, it’s possible that the Quora team wouldn’t have prioritized building a block button so early in its existence.” Harassment often drives people from marginalised groups off online platforms, so this functionality has been important for maintaining the health of Quora&#39;s community. . A crucial aspect to understand is that women leave the tech industry at over twice the rate that men do, according to the Harvard business review (41% of women working in tech leave, compared to 17% of men). An analysis of over 200 books, white papers, and articles found that the reason they leave is that “they’re treated unfairly; underpaid, less likely to be fast-tracked than their male colleagues, and unable to advance.” . Studies have confirmed a number of the factors that make it harder for women to advance in the workplace. Women receive more vague feedback and personality criticism in performance evaluations, whereas men receive actionable advice tied to business outcomes (which is more useful). Women frequently experience being excluded from more creative and innovative roles, and not receiving high visibility “stretch” assignments that are helpful in getting promoted. One study found that men’s voices are perceived as more persuasive, fact-based, and logical than women’s voices, even when reading identical scripts. . Receiving mentorship has been statistically shown to help men advance, but not women. The reason behind this is that when women receive mentorship, it’s advice on how they should change and gain more self-knowledge. When men receive mentorship, it’s public endorsement of their authority. Guess which is more useful in getting promoted? . As long as qualified women keep dropping out of tech, teaching more girls to code will not solve the diversity issues plaguing the field. Diversity initiatives often end up focusing primarily on white women, even although women of colour face many additional barriers. In interviews with 60 women of color who work in STEM research, 100% had experienced discrimination. . The hiring process is particularly broken in tech. One study indicative of the disfunction comes from Triplebyte, a company that helps place software engineers in companies. They conduct a standardised technical interview as part of this process. They have a fascinating dataset: the results of how over 300 engineers did on their exam, and then the results of how those engineers did during the interview process for a variety of companies. The number one finding from Triplebyte’s research is that “the types of programmers that each company looks for often have little to do with what the company needs or does. Rather, they reflect company culture and the backgrounds of the founders.” . This is a challenge for those trying to break into the world of deep learning, since most companies&#39; deep learning groups today were founded by academics. These groups tend to look for people &quot;like them&quot;--that is, people that can solve complex math problems and understand dense jargon. They don&#39;t always know how to spot people who are actually good at solving real problems using deep learning. . This leaves a big opportunity for companies that are ready to look beyond status and pedigree, and focus on results! . Fairness, accountability, and transparency . The professional society for computer scientists, the ACM, runs a conference on data ethics called the &quot;Conference on Fairness, Accountability, and Transparency&quot;. &quot;Fairness, Accountability, and Transparency&quot; sometimes goes under the acronym FAT, although nowadays it&#39;s changing to FAccT. Microsoft has a group focused on &quot;Fairness, Accountability, Transparency, and Ethics&quot; (FATE). The various versions of this lens have resulted in the acronym &quot;FAT&quot; seeing wide usage. In this section, we&#39;ll use &quot;FAccT&quot; to refer to the concepts of Fairness, Accountability, and Transparency*. . FAccT is another lens that you may find useful in considering ethical issues. One useful resource for this is the free online book Fairness and machine learning; Limitations and Opportunities, which &quot;gives a perspective on machine learning that treats fairness as a central concern rather than an afterthought.&quot; It also warns, however, that it &quot;is intentionally narrow in scope... A narrow framing of machine learning ethics might be tempting to technologists and businesses as a way to focus on technical interventions while sidestepping deeper questions about power and accountability. We caution against this temptation.&quot; Rather than provide an overview of the FAccT approach to ethics (which is better done in books such as the one linked above), our focus here will be on the limitations of this kind of narrow framing. . One great way to consider whether an ethical lens is complete, is to try to come up with an example where the lens and our own ethical intuitions give diverging results. Os Keyes explored this in a graphic way in their paper A Mulching Proposal Analysing and Improving an Algorithmic System for Turning the Elderly into High-Nutrient Slurry. The paper&#39;s abstract says: . : The ethical implications of algorithmic systems have been much discussed in both HCI and the broader community of those interested in technology design, development and policy. In this paper, we explore the application of one prominent ethical framework - Fairness, Accountability, and Transparency - to a proposed algorithm that resolves various societal issues around food security and population aging. Using various standardised forms of algorithmic audit and evaluation, we drastically increase the algorithm&#39;s adherence to the FAT framework, resulting in a more ethical and beneficent system. We discuss how this might serve as a guide to other researchers or practitioners looking to ensure better ethical outcomes from algorithmic systems in their line of work. . In this paper, the rather controversial proposal (&quot;Turning the Elderly into High-Nutrient Slurry&quot;) and the results (&quot;drastically increase the algorithm&#39;s adherence to the FAT framework, resulting in a more ethical and beneficent system&quot;) are at odds... to say the least! . In philosophy, and especially philosophy of ethics, this is one of the most effective tools: first, come up with a process, definition, set of questions, etc, which is designed to resolve some problem. Then try to come up with an example where that apparent solution results in a proposal that no-one would consider acceptable. This can then lead to a further refinement of the solution. . So far, we&#39;ve focused on things that you and your organization can do. But sometimes individual or organizational action is not enough. Sometimes, governments also need to consider policy implications. . Role of Policy . The ethical issues that arise in the use of automated decision systems, such as machine learning, can be complex and far-reaching. To better address them, we will need thoughtful policy, in addition to the ethical efforts of those in industry. Neither is sufficient on its own. . Policy is the appropriate tool for addressing: . Negative externalities | Misaligned economic incentives | “Race to the bottom” situations | Enforcing accountability. | . Ethical behavior in industry is necessary as well, since: . Law will not always keep up | Edge cases will arise in which practitioners must use their best judgement. | . Conclusion . Coming from a background of working with binary logic, the lack of clear answers in ethics can be frustrating at first. Yet, the implications of how our work impacts the world, including unintended consequences and the work becoming weaponization by bad actors, are some of the most important questions we can (and should!) consider. Even though there aren&#39;t any easy answers, there are definite pitfalls to avoid and practices to move towards more ethical behavior. . One of our reviewers for this book, Fred Monroe, used to work in hedge fund trading. He told us, after reading this chapter, that many of the issues discussed here (distribution of data being dramatically different than what was trained on, impact of model and feedback loops once deployed and at scale, and so forth) were also key issues for building profitable trading models. The kinds of things you need to do to consider societal consequences are going to have a lot of overlap with things you need to do to consider organizational, market, and customer consequences too--so thinking carefully about ethics can also help you think carefully about how to make your data product successful more generally! . Questionnaire . Does ethics provide a list of &quot;right answers&quot;? | How can working with people of different backgrounds help when considering ethical questions? | What was the role of IBM in Nazi Germany? Why did the company participate as they did? Why did the workers participate? | What was the role of the first person jailed in the VW diesel scandal? | What was the problem with a database of suspected gang members maintained by California law enforcement officials? | Why did YouTube&#39;s recommendation algorithm recommend videos of partially clothed children to pedophiles, even although no employee at Google programmed this feature? | What are the problems with the centrality of metrics? | Why did Meetup.com not include gender in their recommendation system for tech meetups? | What are the six types of bias in machine learning, according to Suresh and Guttag? | Give two examples of historical race bias in the US | Where are most images in Imagenet from? | In the paper &quot;Does Machine Learning Automate Moral Hazard and Error&quot; why is sinusitis found to be predictive of a stroke? | What is representation bias? | How are machines and people different, in terms of their use for making decisions? | Is disinformation the same as &quot;fake news&quot;? | Why is disinformation through auto-generated text a particularly significant issue? | What are the five ethical lenses described by the Markkula Center? | Where is policy an appropriate tool for addressing data ethics issues? | Further research: . Read the article &quot;What Happens When an Algorithm Cuts Your Healthcare&quot;. How could problems like this be avoided in the future? | Research to find out more about YouTube&#39;s recommendation system and its societal impacts. Do you think recommendation systems must always have feedback loops with negative results? What approaches could Google take? What about the government? | Read the paper &quot;Discrimination in Online Ad Delivery&quot;. Do you think Google should be considered responsible for what happened to Dr Sweeney? What would be an appropriate response? | How can a cross-disciplinary team help avoid negative consequences? | Read the paper &quot;Does Machine Learning Automate Moral Hazard and Error&quot; in American Economic Review. What actions do you think should be taken to deal with the issues identified in this paper? | Read the article &quot;How Will We Prevent AI-Based Forgery?&quot; Do you think Etzioni&#39;s proposed approach could work? Why? | Complete the section &quot;Analyze a project you are working on&quot; in this chapter. | Consider whether your team could be more diverse. If so, what approaches might help? | Section 1: that&#39;s a wrap! . Congratulations! You&#39;ve made it to the end of the first section of the book. In this section we&#39;ve tried to show you what deep learning can do, and how you can use it to create real applications and products. At this point, you will get a lot more out of the book if you spend some time trying out what you&#39;ve learnt. Perhaps you have already been doing this as you go along — in which case, great! But if not, that&#39;s no problem either… Now is a great time to start experimenting yourself. . If you haven&#39;t been to the book website yet, head over there now. Remember, you can find it here: book.fast.ai. It&#39;s really important that you have got yourself set up to run the notebooks. Becoming an effective deep learning practitioner is all about practice. So you need to be training models. So please go get the notebooks running now if you haven&#39;t already! And also have a look on the website for any important updates or notices; deep learning changes fast, and we can&#39;t change the words that are printed in this book, so the website is where you need to look to ensure you have the most up-to-date information. . Make sure that you have completed the following steps: . Connected to one of the GPU Jupyter servers recommended on the book website | Run the first notebook yourself | Uploaded an image that you find in the first notebook; then try a few different images of different kinds to see what happens | Run the second notebook, collecting your own dataset based on image search queries that you come up with | Thought about how you can use deep learning to help you with your own projects, including what kinds of data you could use, what kinds of problems may come up, and how you might be able to mitigate these issues in practice. | . In the next section of the book we will learn about how and why deep learning works, instead of just seeing how we can use it in practice. Understanding the how and why is important for both practitioners and researchers, because in this fairly new field nearly every project requires some level of customisation and debugging. The better you understand the foundations of deep learning, the better your models will be. These foundations are less important for executives, product managers, and so forth (although still useful, so feel free to keep reading!), but they are critical for anybody who is actually training and deploying models themselves. . &lt;/div&gt; . .",
            "url": "https://maxlein.github.io/fastbook/2020/03/07/chapter-03-ethics.html",
            "relUrl": "/2020/03/07/chapter-03-ethics.html",
            "date": " • Mar 7, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "https://maxlein.github.io/fastbook/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Test Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://maxlein.github.io/fastbook/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://maxlein.github.io/fastbook/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}