{
  
    
        "post0": {
            "title": "Title",
            "content": "# Fastpages Notebook Blog Post &gt; A tutorial of fastpages for Jupyter notebooks. - toc: true - badges: true - comments: true - categories: [jupyter] - image: images/chart-preview.png[[chapter_ethics]] Data Ethics . Sidebar: Acknowledgement: Dr Rachel Thomas . This chapter was co-authored by Dr Rachel Thomas, the co-founder of fast.ai, and founding director of the Center for Applied Data Ethics at the University of San Francisco. It largely follows a subset of the syllabus she developed for the &quot;Introduction to Data Ethics&quot; course. . End sidebar . As we discussed in Chapters 1 and 2, sometimes, machine learning models can go wrong. They can have bugs. They can be presented with data that they haven&#39;t seen before, and behave in ways we don&#39;t expect. Or, they could work exactly as designed, but be used for something that you would much prefer they were never ever used for. . Because deep learning is such a powerful tool and can be used for so many things, it becomes particularly important that we consider the consequences of our choices. The philosophical study of ethics is the study of right and wrong, including how we can define those terms, recognise right and wrong actions, and understand the connection between actions and consequences. The field of data ethics has been around for a long time, and there are many academics focused on this field. It is being used to help define policy in many jurisdictions; it is being used in companies big and small to consider how best to ensure good societal outcomes from product development; and it is being used by researchers who want to make sure that the work they are doing is used for good, and not for bad. . As a deep learning practitioner, therefore, it is likely that at some point you are going to be put in a situation where you need to consider data ethics. So what is data ethics? It&#39;s a subfield of ethics, so let&#39;s start there. . j: At university, philosophy of ethics was my main thing (it would have been the topic of my thesis, if I&#39;d finished it, instead of dropping out to join the real-world). Based on the years I spent studying ethics, I can tell you this: no one really agrees on what right and wrong are, whether they exist, how to spot them, which people are good, and which bad, or pretty much anything else. So don&#39;t expect too much from the theory! We&#39;re going to focus on examples and thought starters here, not theory. . In answering the question What is Ethics, The Markkula Center for Applied Ethics says that ethics refers to: . Well-founded standards of right and wrong that prescribe what humans ought to do, and | The study and development of one&#39;s ethical standards. | . There is no list of right answers for ethics. There is no list of dos and don&#39;ts. Ethics is complicated, and context-dependent. It involves the perspectives of many stakeholders. Ethics is a muscle that you have to develop and practice. In this chapter, our goal is to provide some signposts to help you on that journey. . Spotting ethical issues is best to do as part of a collaborative team. This is the only way you can really incorporate different perspectives. Different people&#39;s backgrounds will help them to see things which may not be obvious to you. Working with a team is helpful for many &quot;muscle building&quot; activities, including this one. . This chapter is certainly not the only part of the book where we talk about data ethics, but it&#39;s good to have a place where we focus on it for a while. To get oriented, it&#39;s perhaps easiest to look at a few examples. So we picked out three that we think illustrate effectively some of the key topics. . Key examples for data ethics . We are going to start with three specific examples that illustrate three common ethical issues in tech: . Recourse processes: Arkansas&#39;s buggy healthcare algorithms left patients stranded | Feedback loops: YouTube&#39;s recommendation system helped unleash a conspiracy theory boom | Bias: When a traditionally African-American name is searched for on Google, it displays ads for criminal background checks. | In fact, for every concept that we introduce in this chapter, we are going to provide at least one specific example. For each one, have a think about what you could have done in this situation, and think about what kinds of obstructions there might have been to you getting that done. How would you deal with them? What would you look out for? . Bugs and recourse: Buggy algorithm used for healthcare benefits . The Verge investigated software used in over half of the U.S. states to determine how much healthcare people receive, and documented their findings in an article What Happens When an Algorithm Cuts Your Healthcare. After implementation of the algorithm in Arkansas, people (many with severe disabilities) drastically had their healthcare cut. For instance, Tammy Dobbs, a woman with cerebral palsy who needs an aid to help her to get out of bed, to go to the bathroom, to get food, and more, had her hours of help suddenly reduced by 20 hours a week. She couldn’t get any explanation for why her healthcare was cut. Eventually, a court case revealed that there were mistakes in the software implementation of the algorithm, negatively impacting people with diabetes or cerebral palsy. However, Dobbs and many other people reliant on these health care benefits live in fear that their benefits could again be cut suddenly and inexplicably. . Feedback loops: YouTube&#39;s recommendation system . Feedback loops can occur when your model is controlling the next round of data you get. The data that is returned quickly becomes flawed by the software itself. . For instance, in &lt;&gt; we briefly mentioned the reinforcement learning algorithm which Google introduced for YouTube&#39;s recommendation system. YouTube has 1.9bn users, who watch over 1 billion hours of YouTube videos a day. Their algorithm, which was designed to optimise watch time, is responsible for around 70% of the content that is watched. It led to out-of-control feedback loops, leading the New York Times to run the headline &quot;YouTube Unleashed a Conspiracy Theory Boom. Can It Be Contained?&quot;. Ostensibly recommendation systems are predicting what content people will like, but they also have a lot of power in determining what content people even see.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Bias: Professor Lantanya Sweeney &quot;arrested&quot; . Dr. Latanya Sweeney is a professor at Harvard and director of their data privacy lab. In the paper Discrimination in Online Ad Delivery (see &lt;&gt;) she describes her discovery that googling her name resulted in advertisements saying &quot;Latanya Sweeney arrested&quot; even although she is the only Latanya Sweeney and has never been arrested. However when she googled other names, such as Kirsten Lindquist, she got more neutral ads, even though Kirsten Lindquist has been arrested three times.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Google search showing Professor Lantanya Sweeney &#39;arrested&#39; . Being a computer scientist, she studied this systematically, and looked at over 2000 names. She found that this pattern held where historically black names received advertisements suggesting that the person had a criminal record. Whereas, white names had more neutral advertisements. . This is an example of bias. It can make a big difference to people&#39;s lives — for instance, if a job applicant is googled then it may appear that they have a criminal record when they do not. . Why does this matter? . One very natural reaction to considering these issues is: &quot;So what? What&#39;s that got to do with me? I&#39;m a data scientist, not a politician. I&#39;m not one of the senior executives at my company who make the decisions about what we do. I&#39;m just trying to build the most predictive model I can.&quot; . These are very reasonable questions. But we&#39;re going to try to convince you that the answer is: everybody who is training models absolutely needs to consider how their model will be used. And to consider how to best ensure that it is used as positively as possible. There are things you can do. And if you don&#39;t do these things, then things can go pretty bad. . One particularly hideous example of what happens when technologists focus on technology at all costs is the story of IBM and Nazi Germany. A Swiss judge ruled &quot;It does not thus seem unreasonable to deduce that IBM&#39;s technical assistance facilitated the tasks of the Nazis in the commission of their crimes against humanity, acts also involving accountancy and classification by IBM machines and utilized in the concentration camps themselves.&quot; . IBM, you see, supplied the Nazis with data tabulation products necessary to track the extermination of Jews and other groups on a massive scale. This was driven from the top of the company, with marketing to Hitler and his leadership team. Company President Thomas Watson personally approved the 1939 release of special IBM alphabetizing machines to help organize the deportation of Polish Jews. Pictured here is Adolf Hitler (far left) meeting with IBM CEO Tom Watson Sr. (2nd from left), shortly before Hitler awarded Watson a special “Service to the Reich” medal in 1937: . IBM CEO Tom Watson Sr. meeting with Adolf Hitler . But it also happened throughout the organization. IBM and its subsidiaries provided regular training and maintenance on-site at the concentration camps: printing off cards, configuring machines, and repairing them as they broke frequently. IBM set up categorizations on their punch card system for the way that each person was killed, which group they were assigned to, and the logistical information necessary to track them through the vast Holocaust system. IBM&#39;s code for Jews in the concentration camps was 8, where around 6,000,000 were killed. Its code for Romanis was 12 (they were labeled by the Nazis as &quot;asocials&quot;, with over 300,000 killed in the Zigeunerlager, or “Gypsy camp”). General executions were coded as 4, death in the gas chambers as 6. . A punch card used by IBM in concentration camps . Of course, the project managers and engineers and technicians involved were just living their ordinary lives. Caring for their families, going to the church on Sunday, doing their jobs as best as they could. Following orders. The marketers were just doing what they could to meet their business development goals. Edwin Black, author of &quot;IBM and the Holocaust&quot;, said: &quot;To the blind technocrat, the means were more important than the ends. The destruction of the Jewish people became even less important because the invigorating nature of IBM&#39;s technical achievement was only heightened by the fantastical profits to be made at a time when bread lines stretched across the world.&quot; . Step back for a moment and consider: how would you feel if you discovered that you had been part of a system that ending up hurting society? Would you even know? Would you be open to finding out? How can you help make sure this doesn&#39;t happen? We have described the most extreme situation here in Nazi Germany, but there are many negative societal consequences happening due to AI and machine learning right now, some of which we&#39;ll describe in this chapter. . It&#39;s not just a moral burden either. Sometimes, technologists pay very directly for their actions. For instance, the first person who was jailed as a result of the Volkswagen scandal, where the car company cheated on their diesel emissions tests, was not the manager that oversaw the project, or an executive at the helm of the company. It was one of the engineers, James Liang, who just did what he was told. . On the other hand, if a project you are involved in turns out to make a huge positive impact on even one person, this is going to make you feel pretty great! . Okay, so hopefully we have convinced you that you ought to care. But what should you do? As data scientists, we&#39;re naturally inclined to focus on making our model better at optimizing some metric. But optimizing that metric may not actually lead to better outcomes. And even if optimizing that metric does help create better outcomes, it almost certainly won&#39;t be the only thing that matters. Consider the pipeline of steps that occurs between the development of a model or an algorithm by a researcher or practitioner, and the point at which this work is actually used to make some decision. This entire pipeline needs to be considered as a whole if we&#39;re to have a hope of getting the kinds of outcomes we want. . Normally there is a very long chain from one end to the other. This is especially true if you are a researcher where you don&#39;t even know if your research will ever get used for anything, or if you&#39;re involved in data collection, which is even earlier in the pipeline. But no-one is better placed to . Data often ends up being used for different purposes than why it was originally collected. IBM began selling to Nazi Germany well before the Holocaust, including helping with Germany’s 1933 census conducted by Adolf Hitler, which was effective at identifying far more Jewish people than had previously been recognized in Germany. US census data was used to round up Japanese-Americans (who were US citizens) for internment during World War II. It is important to recognize how data and images collected can be weaponized later. Columbia professor Tim Wu wrote that “You must assume that any personal data that Facebook or Android keeps are data that governments around the world will try to get or that thieves will try to steal.” . Integrating machine learning with product design . Presumably the reason you&#39;re doing this work is because you hope it will be used for something. Otherwise, you&#39;re just wasting your time. So, let&#39;s start with the assumption that your work will end up somewhere. Now, as you are collecting your data and developing your model, you are making lots of decisions. What level of aggregation will you store your data at? What loss function should you use? What validation and training sets should you use? Should you focus on simplicity of implementation, speed of inference, or accuracy of the model? How will your model handle out of domain data items? Can it be fine-tuned, or must it be retrained from scratch over time? . These are not just algorithm questions. They are data product design questions. But the product managers, executives, judges, journalists, doctors… whoever ends up developing and using the system of which your model is a part will not be well-placed to understand the decisions that you made, let alone change them. . For instance, two studies found that Amazon’s facial recognition software produced inaccurate and racially biased results. Amazon claimed that the researchers should have changed the default parameters, they did not explain how it would change the racially baised results. Furthermore, it turned out that Amazon was not instructing police departments that used its software to do this either. There was, presumably, a big distance between the researchers that developed these algorithms, and the Amazon documentation staff that wrote the guidelines provided to the police. A lack of tight integration led to serious problems for society, the police, and Amazon themselves. It turned out that their system erroneously matched 28 members of congress to criminal mugshots! (And these members of congress wrongly matched to criminal mugshots disproportionately included people of color as seen in &lt;&gt;.)&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Congressmen matched to criminal mugshots by Amazon software . Data scientists need to be part of a cross disciplinary team. And researchers need to work closely with the kinds of people who will end up using their research. Better still is if the domain experts themselves have learnt enough to be able to train and debug some models themselves — hopefully there&#39;s a few of you reading this book right now! . The modern workplace is a very specialised place. Everybody tends to have very well-defined jobs to perform. Especially in large companies, it can be very hard to know what all the pieces of the puzzle are. Sometimes companies even intentionally obscure the overall project goals that are being worked on, if they know that their employees are not going to like the answers. This is sometimes done by compartmentalising pieces as much as possible . In other words, we&#39;re not saying that any of this is easy. It&#39;s hard. It&#39;s really hard. We all have to do our best. And we have often seen that the people who do get involved in the higher-level context of these projects, and attempt to develop cross disciplinary capabilities and teams, become some of the most important and well rewarded parts of their organisations. It&#39;s the kind of work that tends to be highly appreciated by senior executives, even if it is considered, sometimes, rather uncomfortable by middle management. . Topics in Data Ethics . Data ethics is a big field, and we can&#39;t cover everything. Instead, we&#39;re going to pick a few topics which we think are particularly relevant: . need for recourse and accountability | feedback loops | bias | disinformation | . Let&#39;s look at each in turn. . Recourse and accountability . In a complex system, it is easy for no one person to feel responsible for outcomes. While this is understandable, it does not lead to good results. In the earlier example of the Arkansas healthcare system in which a bug led to people with cerebral palsy losing access to needed care, the creator of the algorithm blamed government officials, and government officials could blame those who implemented the software. NYU professor Danah Boyd described this phenomenon: &quot;bureaucracy has often been used to evade responsibility, and today&#39;s algorithmic systems are extending bureaucracy.&quot; . An additional reason why recourse is so necessary, is because data often contains errors. Mechanisms for audits and error-correction are crucial. A database of suspected gang members maintained by California law enforcement officials was found to be full of errors, including 42 babies who had been added to the database when they were less than 1 year old (28 of whom were marked as “admitting to being gang members”). In this case, there was no process in place for correcting mistakes or removing people once they’ve been added. Another example is the US credit report system; in a large-scale study of credit reports by the FTC (Federal Trade Commission) in 2012, it was found that 26% of consumers had at least one mistake in their files, and 5% had errors that could be devastating. Yet, the process of getting such errors corrected is incredibly slow and opaque. When public-radio reporter Bobby Allyn discovered that he was erroneously listed as having a firearms conviction, it took him &quot;more than a dozen phone calls, the handiwork of a county court clerk and six weeks to solve the problem. And that was only after I contacted the company’s communications department as a journalist.&quot; (as covered in the article How the careless errors of credit reporting agencies are ruining people’s lives) . As machine learning practitioners, we do not always think of it as our responsibility to understand how our algorithms and up being implemented in practice. But we need to. . Feedback loops . We have already explained in &lt;&gt; how an algorithm can interact with its enviromnent to create a feedback loop, making predictions that reinforce actions taken in the real world, which lead to predictions even more pronounced in the same direction. As an example, we&#39;ll discuss YouTube&#39;s recommendation system. A couple of years ago Google talked about how they had introduced reinforcement learning (closely related to deep learning, but where your loss function represents a result which could be a long time after an action occurs) to improve their recommendation system. They described how they used an algorithm which made recommendations such that watch time would be optimised.&lt;/p&gt; However, human beings tend to be drawn towards controversial content. This meant that videos about things like conspiracy theories started to get recommended more and more by the recommendation system. Furthermore, it turns out that the kinds of people that are interested in conspiracy theories are also people that watch a lot of online videos! So, they started to get drawn more and more towards YouTube. The increasing number of conspiracy theorists watching YouTube resulted in the algorithm recommending more and more conspiracy theories and other extremist content, which resulted in more extremists watching videos on YouTube, and more people watching YouTube developing extremist views, which led to the algorithm recommending more extremist content... The system became so out of control that in February 2019 it led the New York Times to run the headline &quot;YouTube Unleashed a Conspiracy Theory Boom. Can It Be Contained?&quot;footnote:[https://www.nytimes.com/2019/02/19/technology/youtube-conspiracy-stars.html] . The New York Times published another article on YouTube&#39;s recommendation system, titled On YouTube’s Digital Playground, an Open Gate for Pedophiles. The article started with this chilling story: . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; : Christiane C. didn’t think anything of it when her 10-year-old daughter and a friend uploaded a video of themselves playing in a backyard pool… A few days later… the video had thousands of views. Before long, it had ticked up to 400,000... “I saw the video again and I got scared by the number of views,” Christiane said. She had reason to be. YouTube’s automated recommendation system… had begun showing the video to users who watched other videos of prepubescent, partially clothed children, a team of researchers has found.&gt; : On its own, each video might be perfectly innocent, a home movie, say, made by a child. Any revealing frames are fleeting and appear accidental. But, grouped together, their shared features become unmistakable. . YouTube&#39;s recommendation algorithm had begun curating playlists for pedophiles, picking out innocent home videos that happened to contain prepubescent, partially clothed children. . No one at Google planned to create a system that turned family videos into porn for pedophiles. So what happened? . Part of the problem here is the centrality of metrics in driving a financially important system. When an algorithm has a metric to optimise, as you have seen, it will do everything it can to optimise that number. This tends to lead to all kinds of edge cases, and humans interacting with a system will search for, find, and exploit these edge cases and feedback loops for their advantage. . There are signs that this is exactly what has happened with YouTube&#39;s recommendation system. The Guardian ran an article How an ex-YouTube insider investigated its secret algorithm about Guillaume Chaslot, an ex-YouTube engineer who created AlgoTransparency, which tracks these issues. Chaslot published the chart in &lt;&gt;, following the release of Robert Mueller&#39;s &quot;Report on the Investigation Into Russian Interference in the 2016 Presidential Election&quot;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Coverage of the Mueller report . Russia Today&#39;s coverage of the Mueller report was an extreme outlier in how many channels were recommending it. This suggests the possibility that Russia Today, a state-owned Russia media outlet, has been successful in gaming YouTube&#39;s recommendation algorithm. The lack of transparency of systems like this make it hard to uncover the kinds of problems that we&#39;re discussing. . One of our reviewers for this book, Aurélien Géron, led YouTube&#39;s video classification team from 2013 to 2016 (well before the events discussed above). He pointed out that it&#39;s not just feedback loops involving humans that are a problem. There can also be feedback loops without humans! He told us about an example from YouTube: . : &quot;One important signal to classify the main topic of a video is the channel it comes from. For example, a video uploaded to a cooking channel is very likely to be a cooking video. But how do we know what topic a channel is about? Well… in part by looking at the topics of the videos it contains! Do you see the loop? For example, many videos have a description which indicates what camera was used to shoot the video. As a result, some of these videos might get classified as videos about “photography”. If a channel has such as misclassified video, it might be classified as a “photography” channel, making it even more likely for future videos on this channel to be wrongly classified as “photography”. This could even lead to runaway virus-like classifications! One way to break this feedback loop is to classify videos with and without the channel signal. Then when classifying the channels, you can only use the classes obtained without the channel signal. This way, the feedback loop is broken.&quot; There are positive examples of people and organizations attempting to combat these problems. Evan Estola, lead machine learning engineer at Meetup, discussed the example of men expressing more interest than women in tech meetups. Meetup’s algorithm could recommend fewer tech meetups to women, and as a result, fewer women would find out about and attend tech meetups, which could cause the algorithm to suggest even fewer tech meetups to women, and so on in a self-reinforcing feedback loop. Evan and his team made the ethical decision for their recommendation algorithm to not create such a feedback loop, but explicitly not using gender for that part of their model. It is encouraging to see a company not just unthinkingly optimize a metric, but to consider their impact. &quot;You need to decide which feature not to use in your algorithm… the most optimal algorithm is perhaps not the best one to launch into production&quot;, he said. . While Meetup chose to avoid such an outcome, Facebook provides an example of allowing a runaway feedback loop to run wild. Facebook radicalizes users interested in one conspiracy theory by introducing them to more. As Renee DiResta, a researcher on proliferation of disinformation, writes: . : &quot;once people join a single conspiracy-minded [Facebook] group, they are algorithmically routed to a plethora of others. Join an anti-vaccine group, and your suggestions will include anti-GMO, chemtrail watch, flat Earther (yes, really), and ‘curing cancer naturally’ groups. Rather than pulling a user out of the rabbit hole, the recommendation engine pushes them further in.&quot; . It is extremely important to keep in mind this kind of behavior can happen, and to either anticipate a feedback loop or take positive action to break it when you can the first signs of it in your own projects. Another thing to keep in mind is bias, which, as we discussed in the previous chapter, can interact with feedback loops in very troublesome ways. . Bias . Discussions of bias online tend to get pretty confusing pretty fast. The word bias means so many different things. Statisticians often think that when data ethicists are talking about bias that they&#39;re talking about the statistical definition of the term bias. But they&#39;re not. And they&#39;re certainly not talking about the biases that appear in the weights and biases which are the parameters of your model! . What they&#39;re talking about is the social science concept of bias. In A Framework for Understanding Unintended Consequences of Machine Learning MIT&#39;s Suresh and Guttag describe six types of bias in machine learning, summarized in &lt;&gt; from their paper.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Bias in machine learning can come from multiple sources . We&#39;ll discuss four of these types of bias, those that we&#39;ve found most helpful in our own work (see the paper for details on the others). . Historical bias . Historical bias comes from the fact that people are biased, processes are biased, and society is biased. Suresh and Guttag say: &quot;Historical bias is a fundamental, structural issue with the first step of the data generation process and can exist even given perfect sampling and feature selection&quot;. . For instance, here&#39;s a few examples of historical race bias in the US, from the NY Times article Racial Bias, Even When We Have Good Intentions, by the University of Chicago&#39;s Sendhil Mullainathan: . When doctors were shown identical files, they were much less likely to recommend cardiac catheterization (a helpful procedure) to Black patients | When bargaining for a used car, Black people were offered initial prices $700 higher and received far smaller concessions | Responding to apartment-rental ads on Craigslist with a Black name elicited fewer responses than with a white name | An all-white jury was 16 percentage points more likely to convict a Black defendant than a white one, but when a jury had 1 Black member, it convicted both at same rate. | . The COMPAS algorithm, widely used for sentencing and bail decisions in the US, is an example of an important algorithm which, when tested by ProPublica, showed clear racial bias in practice: . Results of the COMPAS algorithm . Any dataset involving humans can have this kind of bias, such as medical data, sales data, housing data, political data, and so on. Because underlying bias is so pervasive, bias in datasets is very pervasive. Racial bias even turns up in computer vision, as shown in this example of auto-categorized photos shared on Twitter by a Google Photos user: . One of these labels is very wrong... . Yes, that is showing what you think it is: Google Photos classified a Black user&#39;s photo with their friend as &quot;gorillas&quot;! This algorithmic mis-step got a lot of attention in the media. “We’re appalled and genuinely sorry that this happened,” a company spokeswoman said. “There is still clearly a lot of work to do with automatic image labeling, and we’re looking at how we can prevent these types of mistakes from happening in the future.” . Unfortunately, fixing problems in machine learning systems when the input data has problems is hard. Google&#39;s first attempt didn&#39;t inspire confidence, as covered by The Guardian: . Google first response to the problem . These kinds of problem are certainly not limited to just Google. MIT researchers studied the most popular online computer vision APIs to see how accurate they were. But they didn&#39;t just calculate a single accuracy number—instead, they looked at the accuracy across four different groups: . Error rate per gender and race for various facial recognition systems . IBM&#39;s system, for instance, had a 34.7% error rate for darker females, vs 0.3% for lighter males—over 100 times more errors! Some people incorrectly reacted to these experiments by claiming that the difference was simply because darker skin is harder for computers to recognise. However, what actually happened, is after the negative publicity that this result created, all of the companies in question dramatically improved their models for darker skin, such that one year later they were nearly as good as for lighter skin. So what this actually showed is that the developers failed to utilise datasets containing enough darker faces, or test their product with darker faces. . One of the MIT researchers, Joy Buolamwini, warned, &quot;We have entered the age of automation overconfident yet underprepared. If we fail to make ethical and inclusive artificial intelligence, we risk losing gains made in civil rights and gender equity under the guise of machine neutrality&quot;. . Part of the issue appears to be a systematic imbalance in the make up of popular datasets used for training models. The abstract to the paper No Classification without Representation: Assessing Geodiversity Issues in Open Data Sets for the Developing World states, &quot;We analyze two large, publicly available image data sets to assess geo-diversity and find that these data sets appear to exhibit an observable amerocentric and eurocentric representation bias. Further, we analyze classifiers trained on these data sets to assess the impact of these training distributions and find strong differences in the relative performance on images from different locales&quot;. &lt;&gt; shows one of the charts from the paper, showing the geographic make up of what was, at the time (and still, as this book is being written), the two most important image datasets for training models.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Image provenance in popular training sets . The vast majority of the images are from the United States and other Western countries, leading to models trained on ImageNet performing worse on scenes from other countries and cultures. For instance, research found that such models are worse at identifying household items (such as soap, spices, sofas, or beds) from lower-income countries. &lt;&gt; shows an image from the paper, Does Object Recognition Work for Everyone?.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Object detection in action . In this example, we can see that the lower-income soap example is a very long way away from being accurate, with every commercial image recognition service predicting &quot;food&quot; as the most likely answer! . As we will discuss shortly, in addition, the vast majority of AI researchers and developers are young white men. Most projects that we have seen do most user testing using friends and families of the immediate product development group. Given this, the kinds of problems we just discussed should not be surprising. . Similar historical bias is found in the texts used as data for natural language processing models. This crops up in downstream machine learning tasks in many ways. For instance, it was widely reported that until last year Google Translate showed systematic bias in how it translated the Turkish gender-neutral pronoun &quot;o&quot; into English. For instance, when applied to jobs which are often associated with males, it used &quot;he&quot;, and when applied to jobs which are often associated with females, it used &quot;she&quot;: . Gender bias in text data sets . We also see this kind of bias in online advertisements. For instance, a study in 2019 found that even when the person placing the ad does not intentionally discriminate, Facebook will show the ad to very different audiences based on race and gender. Housing ads with the same text, but changing the picture between a white or black family, were shown to racially different audiences. . Measurement bias . In the paper Does Machine Learning Automate Moral Hazard and Error in American Economic Review, the authors look at a model that tries to answer the question: using historical EHR data, what factors are most predictive of stroke? These are the top predictors from the model: . Prior Stroke | Cardiovascular disease | Accidental injury | Benign breast lump | Colonoscopy | Sinusitis | . However, only the top two have anything to do with a stroke! Based on what we&#39;ve studied so far, you can probably guess why. We haven’t really measured stroke, which occurs when a region of the brain is denied oxygen due to an interruption in the blood supply. What we’ve measured is who: had symptoms, went to a doctor, got the appropriate tests, AND received a diagnosis of stroke. Actually having a stroke is not the only thing correlated with this complete list — it&#39;s also correlated with being the kind of person who actually goes to the doctor (which is influenced by who has access to healthcare, can afford their co-pay, doesn&#39;t experience racial or gender-based medical discrimination, and more)! If you are likely to go to the doctor for an accidental injury, then you are likely to also go the doctor when you are having a stroke. . This is an example of measurement bias. It occurs when our models make mistakes because we are measuring the wrong thing, or measuring it in the wrong way, or incorporating that measurement into our model inappropriately. . Aggregation Bias . Aggregation bias occurs when models do not aggregate data in a way that incorporates all of the appropriate factors, or when a model does not include the necessary interaction terms, nonlinearities, or so forth. This can particularly occur in medical settings. For instance, the way diabetes is treated is often based on simple univariate statistics and studies involving small groups of heterogeneous people. Analysis of results is often done in a way that does not take account of different ethnicities or genders. However it turns out that diabetes patients have different complications across ethnicities, and HbA1c levels (widely used to diagnose and monitor diabetes) differ in complex ways across ethnicities and genders. This can result in people being misdiagnosed or incorrectly treated because medical decisions are based on a model which does not include these important variables and interactions. . Representation Bias . The abstract of the paper Bias in Bios: A Case Study of Semantic Representation Bias in a High-Stakes Setting notes that there is gender imbalance in occupations (e.g. females are more likely to be nurses, and males are more likely to be pastors), and says that: &quot;differences in true positive rates between genders are correlated with existing gender imbalances in occupations, which may compound these imbalances&quot;. . What this is saying is that the researchers noticed that models predicting occupation did not only reflect the actual gender imbalance in the underlying population, but actually amplified it! This is quite common, particularly for simple models. When there is some clear, easy to see underlying relationship, a simple model will often simply assume that that relationship holds all the time. As &lt;&gt; from the paper shows, for occupations which had a higher percentage of females, the model tended to overestimate the prevalence of that occupation.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Model error in predicting occupation plotted against percentage of women in said occupation . For example, in the training dataset, 14.6% of surgeons were women, yet in the model predictions, only 11.6% of the true positives were women. The model is thus amplifying the bias existing in the training set. . Now that we saw those biases existed, what can we do to mitigate them? . Addressing different types of bias . Different types of bias require different approaches for mitigation. While gathering a more diverse dataset can address representation bias, this would not help with historical bias or measurement bias. All datasets contain bias. There is no such thing as a completely de-biased dataset. Many researchers in the field have been converging on a set of proposals towards better documenting the decisions, context, and specifics about how and why a particular dataset was created, what scenarios it is appropriate to use in, and what the limitations are. This way, those using the dataset will not be caught off-guard by its biases and limitations. . Humans are biased, so does algorithmic bias matter? . We often hear this question — &quot;humans are biased, so does algorithmic bias even matter?&quot; This comes up so often, there must be some reasoning that makes sense to the people that ask it, but it doesn&#39;t seem very logically sound to us! Independently of whether this is logically sound, it&#39;s important to realise that algorithms and people are different. Machine learning, particularly so. Consider these points about machine learning algorithms: . Machine learning can create feedback loops:: small amounts of bias can very rapidly, exponentially increase due to feedback loops | Machine learning can amplify bias:: human bias can lead to larger amounts of machine learning bias | Algorithms &amp; humans are used differently:: human decision makers and algorithmic decision makers are not used in a plug-and-play interchangeable way in practice. For instance, algorithmic decisions are more likely to be implemented at scale and without a process for recourse. Furthermore, people are more likely to mistakenly believe that the result of an algorithm is objective and error-free. | Technology is power:: And with that comes responsibility. | . As the Arkansas healthcare example showed, machine learning is often implemented in practice not because it leads to better outcomes, but because it is cheaper and more efficient. Cathy O&#39;Neill, in her book Weapons of Math Destruction, described the pattern of how the privileged are processed by people, the poor are processed by algorithms. This is just one of a number of ways that algorithms are used differently than human decision makers. Others include: . People are more likely to assume algorithms are objective or error-free (even if they’re given the option of a human override) | Algorithms are more likely to be implemented with no appeals process in place | Algorithms are often used at scale | Algorithmic systems are cheap. | . Even in the absence of bias, algorithms (and deep learning especially, since it is such an effective and scalable algorithm) can lead to negative societal problems, such as when used for disinformation. . Disinformation . Disinformation has a history stretching back hundreds or even thousands of years. It is not necessarily about getting someone to believe something false, but rather, often to sow disharmony and uncertainty, and to get people to give up on seeking the truth. Receiving conflicting accounts can lead people to assume that they can never know what to trust. . Some people think disinformation is primarily about false information or fake news, but in reality, disinformation can often contain seeds of truth, or involve half-truths taken out of context. Ladislav Bittman was an intelligence officer in the USSR who later defected to the United States and wrote some books in the 1970s and 1980s on the role of disinformation in Soviet propaganda operations. He said, &quot;Most campaigns are a carefully designed mixture of facts, half-truths, exaggerations, &amp; deliberate lies.&quot; . In the United States this has hit close to home in recent years, with the FBI detailing a massive disinformation campaign linked to Russia in the 2016 US election. Understanding the disinformation that was used in this campaign is very educational. For instance, the FBI found that the Russian disinformation campaign often organized two separate fake grass roots protests, one for each side of an issue, and got them to protest at the same time! The Houston Chronicle reported on one of these odd events: . : A group that called itself the &quot;Heart of Texas&quot; had organized it on social media — a protest, they said, against the &quot;Islamization&quot; of Texas. On one side of Travis Street, I found about 10 protesters. On the other side, I found around 50 counterprotesters. But I couldn&#39;t find the rally organizers. No &quot;Heart of Texas.&quot; I thought that was odd, and mentioned it in the article: What kind of group is a no-show at its own event? Now I know why. Apparently, the rally&#39;s organizers were in Saint Petersburg, Russia, at the time. &quot;Heart of Texas&quot; is one of the internet troll groups cited in Special Prosecutor Robert Mueller&#39;s recent indictment of Russians attempting to tamper with the U.S. presidential election. . Event organized by the group Heart of Texas . Disinformation often involves coordinated campaigns of inauthentic behavior. For instance, fraudulent accounts may try to make it seem like many people hold a particular viewpoint. While most of us like to think of ourselves as independent-minded, in reality we evolved to be influenced by others in our in-group, and in opposition to those in our out-group. Online discussions can influence our viewpoints, or alter the range of what we consider acceptable viewpoints. Humans are social animals, and as social animals we are extremely influenced by the people around us. Increasingly, radicalisation occurs in online environments. So influence is coming from people in the virtual space of online forums and social networks. . Disinformation through auto-generated text is a particularly significant issue, due to the greatly increased capability provided by deep learning. We discuss this issue in depth when we learn to create language models, in &lt;&gt;.&lt;/p&gt; One proposed approach is to develop some form of digital signature, implement it in a seamless way, and to create norms that we should only trust content which has been verified. Head of the Allen Institute on AI, Oren Etzioni, wrote such a proposal in an article titled How Will We Prevent AI-Based Forgery?, &quot;AI is poised to make high-fidelity forgery inexpensive and automated, leading to potentially disastrous consequences for democracy, security, and society. The specter of AI forgery means that we need to act to make digital signatures de rigueur as a means of authentication of digital content.&quot; . Whilst we can&#39;t hope to discuss all the ethical issues that deep learning, and algorithms more generally, bring up, hopefully this brief introduction has been a useful starting point you can build on. We&#39;ll now move on to the questions of how to identify ethical issues, and what to do about them. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Identifying and addressing ethical issues . Mistakes happen. Finding out about them, and dealing with them, needs to be part of the design of any system that includes machine learning (and many other systems too). The issues raised within data ethics are often complex and interdisciplinary, but it is crucial that we work to address them. . So what can we do? This is a big topic, but a few steps towards addressing ethical issues are: . analyze a project you are working on | implement processes at your company to find and address ethical risks | support good policy | increase diversity | . Let&#39;s walk through each step next, staring with analyzing a project you are working on. . Analyze a project you are working on . It&#39;s easy to miss important issues when considered ethical implications of your work. One thing that helps enormously is simply asking the right questions. Rachel Thomas recommends considering the following questions throughout the development of a data project: . Should we even be doing this? | What bias is in the data? | Can the code and data be audited? | What are error rates for different sub-groups? | What is the accuracy of a simple rule-based alternative? | What processes are in place to handle appeals or mistakes? | How diverse is the team that built it? | . These questions may be able to help you identify outstanding issues, and possible alternatives that are easier to understand and control. In addition to asking the right questions, it&#39;s also important to consider practices and processes to implement. . Processes to implement . The Markkula Center has released An Ethical Toolkit for Engineering/Design Practice, which includes some concrete practices to implement at your company, including regularly scheduled ethical risk sweeps to proactively search for ethical risks (in a manner similar to cybersecurity penetration testing), expanding the ethical circle to include the perspectives of a variety of stakeholders, and considering the terrible people (how could bad actors abuse, steal, misinterpret, hack, destroy, or weaponize what you are building?). . Even if you don&#39;t have a diverse team, you can still try to pro-actively include the perspectives of a wider group, considering questions such as these (provided by the Markkula Center): . Whose interests, desires, skills, experiences and values have we simply assumed, rather than actually consulted? | Who are all the stakeholders who will be directly affected by our product? How have their interests been protected? How do we know what their interests really are—have we asked? | Who/which groups and individuals will be indirectly affected in significant ways? | Who might use this product that we didn’t expect to use it, or for purposes we didn’t initially intend? | . Ethical Lenses . Another useful resource from the Markkula Center is Conceptual Frameworks in Technology and Engineering Practice. This considers how different foundational ethical lenses can help identify concrete issues, and lays out the following approaches and key questions: . The Rights Approach:: Which option best respects the rights of all who have a stake? | The Justice Approach:: Which option treats people equally or proportionately? | The Utilitarian Approach:: Which option will produce the most good and do the least harm? | The Common Good Approach:: Which option best serves the community as a whole, not just some members? | The Virtue Approach:: Which option leads me to act as the sort of person I want to be? | . Markkula&#39;s recommendations include a deeper dive into each of these perspectives, including looking at a project based on a focus on its consequences: . Who will be directly affected by this project? Who will be indirectly affected? | Will the effects in aggregate likely create more good than harm, and what types of good and harm? | Are we thinking about all relevant types of harm/benefit (psychological, political, environmental, moral, cognitive, emotional, institutional, cultural)? | How might future generations be affected by this project? | Do the risks of harm from this project fall disproportionately on the least powerful in society? Will the benefits go disproportionately the well-off? | Have we adequately considered ‘dual-use? | . The alternative lens to this is the deontological perspective, which focuses on basic right and wrong: . What rights of others &amp; duties to others must we respect? | How might the dignity &amp; autonomy of each stakeholder be impacted by this project? | What considerations of trust &amp; of justice are relevant to this design/project? | Does this project involve any conflicting moral duties to others, or conflicting stakeholder rights? How can we prioritize these? | . One of the best ways to help come up with complete and thoughtful answers to questions like these is to ensure that the people asking the questions are diverse. . The power of diversity . Currently, less than 12% of AI researchers are women, according to a study from element AI. The statistics are similarly dire when it comes to race and age. When everybody on a team has similar backgrounds, they are likely to have similar blindspots around ethical risks. The Harvard Business Review (HBR) has published a number of studies showing many benefits of diverse teams, including: . How Diversity Can Drive Innovation | Teams Solve Problems Faster When They’re More Cognitively Diverse | Why Diverse Teams Are Smarter, and | What Makes a Team Smarter? More Women. | . Diversity can lead to problems being identified earlier, and a wider range of solutions being considered. For instance, Tracy Chou was an early engineer at Quora. She wrote of her experiences, describing how she advocated internally for adding a feature that would allow trolls and other bad actors to be blocked. Chou recounts, “I was eager to work on the feature because I personally felt antagonized and abused on the site (gender isn’t an unlikely reason as to why)... But if I hadn’t had that personal perspective, it’s possible that the Quora team wouldn’t have prioritized building a block button so early in its existence.” Harassment often drives people from marginalised groups off online platforms, so this functionality has been important for maintaining the health of Quora&#39;s community. . A crucial aspect to understand is that women leave the tech industry at over twice the rate that men do, according to the Harvard business review (41% of women working in tech leave, compared to 17% of men). An analysis of over 200 books, white papers, and articles found that the reason they leave is that “they’re treated unfairly; underpaid, less likely to be fast-tracked than their male colleagues, and unable to advance.” . Studies have confirmed a number of the factors that make it harder for women to advance in the workplace. Women receive more vague feedback and personality criticism in performance evaluations, whereas men receive actionable advice tied to business outcomes (which is more useful). Women frequently experience being excluded from more creative and innovative roles, and not receiving high visibility “stretch” assignments that are helpful in getting promoted. One study found that men’s voices are perceived as more persuasive, fact-based, and logical than women’s voices, even when reading identical scripts. . Receiving mentorship has been statistically shown to help men advance, but not women. The reason behind this is that when women receive mentorship, it’s advice on how they should change and gain more self-knowledge. When men receive mentorship, it’s public endorsement of their authority. Guess which is more useful in getting promoted? . As long as qualified women keep dropping out of tech, teaching more girls to code will not solve the diversity issues plaguing the field. Diversity initiatives often end up focusing primarily on white women, even although women of colour face many additional barriers. In interviews with 60 women of color who work in STEM research, 100% had experienced discrimination. . The hiring process is particularly broken in tech. One study indicative of the disfunction comes from Triplebyte, a company that helps place software engineers in companies. They conduct a standardised technical interview as part of this process. They have a fascinating dataset: the results of how over 300 engineers did on their exam, and then the results of how those engineers did during the interview process for a variety of companies. The number one finding from Triplebyte’s research is that “the types of programmers that each company looks for often have little to do with what the company needs or does. Rather, they reflect company culture and the backgrounds of the founders.” . This is a challenge for those trying to break into the world of deep learning, since most companies&#39; deep learning groups today were founded by academics. These groups tend to look for people &quot;like them&quot;--that is, people that can solve complex math problems and understand dense jargon. They don&#39;t always know how to spot people who are actually good at solving real problems using deep learning. . This leaves a big opportunity for companies that are ready to look beyond status and pedigree, and focus on results! . Fairness, accountability, and transparency . The professional society for computer scientists, the ACM, runs a conference on data ethics called the &quot;Conference on Fairness, Accountability, and Transparency&quot;. &quot;Fairness, Accountability, and Transparency&quot; sometimes goes under the acronym FAT, although nowadays it&#39;s changing to FAccT. Microsoft has a group focused on &quot;Fairness, Accountability, Transparency, and Ethics&quot; (FATE). The various versions of this lens have resulted in the acronym &quot;FAT&quot; seeing wide usage. In this section, we&#39;ll use &quot;FAccT&quot; to refer to the concepts of Fairness, Accountability, and Transparency*. . FAccT is another lens that you may find useful in considering ethical issues. One useful resource for this is the free online book Fairness and machine learning; Limitations and Opportunities, which &quot;gives a perspective on machine learning that treats fairness as a central concern rather than an afterthought.&quot; It also warns, however, that it &quot;is intentionally narrow in scope... A narrow framing of machine learning ethics might be tempting to technologists and businesses as a way to focus on technical interventions while sidestepping deeper questions about power and accountability. We caution against this temptation.&quot; Rather than provide an overview of the FAccT approach to ethics (which is better done in books such as the one linked above), our focus here will be on the limitations of this kind of narrow framing. . One great way to consider whether an ethical lens is complete, is to try to come up with an example where the lens and our own ethical intuitions give diverging results. Os Keyes explored this in a graphic way in their paper A Mulching Proposal Analysing and Improving an Algorithmic System for Turning the Elderly into High-Nutrient Slurry. The paper&#39;s abstract says: . : The ethical implications of algorithmic systems have been much discussed in both HCI and the broader community of those interested in technology design, development and policy. In this paper, we explore the application of one prominent ethical framework - Fairness, Accountability, and Transparency - to a proposed algorithm that resolves various societal issues around food security and population aging. Using various standardised forms of algorithmic audit and evaluation, we drastically increase the algorithm&#39;s adherence to the FAT framework, resulting in a more ethical and beneficent system. We discuss how this might serve as a guide to other researchers or practitioners looking to ensure better ethical outcomes from algorithmic systems in their line of work. . In this paper, the rather controversial proposal (&quot;Turning the Elderly into High-Nutrient Slurry&quot;) and the results (&quot;drastically increase the algorithm&#39;s adherence to the FAT framework, resulting in a more ethical and beneficent system&quot;) are at odds... to say the least! . In philosophy, and especially philosophy of ethics, this is one of the most effective tools: first, come up with a process, definition, set of questions, etc, which is designed to resolve some problem. Then try to come up with an example where that apparent solution results in a proposal that no-one would consider acceptable. This can then lead to a further refinement of the solution. . So far, we&#39;ve focused on things that you and your organization can do. But sometimes individual or organizational action is not enough. Sometimes, governments also need to consider policy implications. . Role of Policy . The ethical issues that arise in the use of automated decision systems, such as machine learning, can be complex and far-reaching. To better address them, we will need thoughtful policy, in addition to the ethical efforts of those in industry. Neither is sufficient on its own. . Policy is the appropriate tool for addressing: . Negative externalities | Misaligned economic incentives | “Race to the bottom” situations | Enforcing accountability. | . Ethical behavior in industry is necessary as well, since: . Law will not always keep up | Edge cases will arise in which practitioners must use their best judgement. | . Conclusion . Coming from a background of working with binary logic, the lack of clear answers in ethics can be frustrating at first. Yet, the implications of how our work impacts the world, including unintended consequences and the work becoming weaponization by bad actors, are some of the most important questions we can (and should!) consider. Even though there aren&#39;t any easy answers, there are definite pitfalls to avoid and practices to move towards more ethical behavior. . One of our reviewers for this book, Fred Monroe, used to work in hedge fund trading. He told us, after reading this chapter, that many of the issues discussed here (distribution of data being dramatically different than what was trained on, impact of model and feedback loops once deployed and at scale, and so forth) were also key issues for building profitable trading models. The kinds of things you need to do to consider societal consequences are going to have a lot of overlap with things you need to do to consider organizational, market, and customer consequences too--so thinking carefully about ethics can also help you think carefully about how to make your data product successful more generally! . Questionnaire . Does ethics provide a list of &quot;right answers&quot;? | How can working with people of different backgrounds help when considering ethical questions? | What was the role of IBM in Nazi Germany? Why did the company participate as they did? Why did the workers participate? | What was the role of the first person jailed in the VW diesel scandal? | What was the problem with a database of suspected gang members maintained by California law enforcement officials? | Why did YouTube&#39;s recommendation algorithm recommend videos of partially clothed children to pedophiles, even although no employee at Google programmed this feature? | What are the problems with the centrality of metrics? | Why did Meetup.com not include gender in their recommendation system for tech meetups? | What are the six types of bias in machine learning, according to Suresh and Guttag? | Give two examples of historical race bias in the US | Where are most images in Imagenet from? | In the paper &quot;Does Machine Learning Automate Moral Hazard and Error&quot; why is sinusitis found to be predictive of a stroke? | What is representation bias? | How are machines and people different, in terms of their use for making decisions? | Is disinformation the same as &quot;fake news&quot;? | Why is disinformation through auto-generated text a particularly significant issue? | What are the five ethical lenses described by the Markkula Center? | Where is policy an appropriate tool for addressing data ethics issues? | Further research: . Read the article &quot;What Happens When an Algorithm Cuts Your Healthcare&quot;. How could problems like this be avoided in the future? | Research to find out more about YouTube&#39;s recommendation system and its societal impacts. Do you think recommendation systems must always have feedback loops with negative results? What approaches could Google take? What about the government? | Read the paper &quot;Discrimination in Online Ad Delivery&quot;. Do you think Google should be considered responsible for what happened to Dr Sweeney? What would be an appropriate response? | How can a cross-disciplinary team help avoid negative consequences? | Read the paper &quot;Does Machine Learning Automate Moral Hazard and Error&quot; in American Economic Review. What actions do you think should be taken to deal with the issues identified in this paper? | Read the article &quot;How Will We Prevent AI-Based Forgery?&quot; Do you think Etzioni&#39;s proposed approach could work? Why? | Complete the section &quot;Analyze a project you are working on&quot; in this chapter. | Consider whether your team could be more diverse. If so, what approaches might help? | Section 1: that&#39;s a wrap! . Congratulations! You&#39;ve made it to the end of the first section of the book. In this section we&#39;ve tried to show you what deep learning can do, and how you can use it to create real applications and products. At this point, you will get a lot more out of the book if you spend some time trying out what you&#39;ve learnt. Perhaps you have already been doing this as you go along — in which case, great! But if not, that&#39;s no problem either… Now is a great time to start experimenting yourself. . If you haven&#39;t been to the book website yet, head over there now. Remember, you can find it here: book.fast.ai. It&#39;s really important that you have got yourself set up to run the notebooks. Becoming an effective deep learning practitioner is all about practice. So you need to be training models. So please go get the notebooks running now if you haven&#39;t already! And also have a look on the website for any important updates or notices; deep learning changes fast, and we can&#39;t change the words that are printed in this book, so the website is where you need to look to ensure you have the most up-to-date information. . Make sure that you have completed the following steps: . Connected to one of the GPU Jupyter servers recommended on the book website | Run the first notebook yourself | Uploaded an image that you find in the first notebook; then try a few different images of different kinds to see what happens | Run the second notebook, collecting your own dataset based on image search queries that you come up with | Thought about how you can use deep learning to help you with your own projects, including what kinds of data you could use, what kinds of problems may come up, and how you might be able to mitigate these issues in practice. | . In the next section of the book we will learn about how and why deep learning works, instead of just seeing how we can use it in practice. Understanding the how and why is important for both practitioners and researchers, because in this fairly new field nearly every project requires some level of customisation and debugging. The better you understand the foundations of deep learning, the better your models will be. These foundations are less important for executives, product managers, and so forth (although still useful, so feel free to keep reading!), but they are critical for anybody who is actually training and deploying models themselves. . &lt;/div&gt; . .",
            "url": "https://maxlein.github.io/fastbook/2020/03/07/chapter-03-ethics.html",
            "relUrl": "/2020/03/07/chapter-03-ethics.html",
            "date": " • Mar 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "[[chapter_production]] From model to production . The five lines of code we saw in &lt;&gt; are just one small part of the process of using deep learning in practice. In this chapter, we&#39;re going to use a computer vision example to look at the end-to-end process of creating a deep learning application. More specifically: we&#39;re going to build a bear classifier! In the process, we&#39;ll discuss the capabilities and constraints of deep learning, learn about how to create datasets, look at possible gotchas when using deep learning in practice, and more. Many of the key points will apply equally well to other deep learning problems, such as we showed in &lt;&gt;. If you work through a problem similar in key respects to our example problems, we expect you to get excellent results with little code, quickly.&lt;/p&gt; Let&#39;s start with how you should frame your problem. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The practice of deep learning . We&#39;ve seen that deep learning can solve a lot of challenging problems quickly and with little code. As a beginner there&#39;s a sweet spot of problems that are similar enough to our example problems that you can very quickly get extremely useful results. However, deep learning isn&#39;t magic! The same 5 lines of code won&#39;t work on every problem anyone can think of today. Underestimating the constraints and overestimating the capabilities of deep learning may lead to frustratingly poor results. At least until you gain some experience to solve the problems that arise. Overestimating the constraints and underestimating the capabilities of deep learning may mean you do not attempt a solvable problem because you talk yourself out of it. . We often talk to people who overestimate both the constraints, and the capabilities of deep learning. Both of these can be problems: underestimating the capabilities means that you might not even try things which could be very beneficial; underestimating the constraints might mean that you fail to consider and react to important issues. . The best thing to do is to keep an open mind. If you remain open to the possibility that deep learning might solve part of your problem with less data or complexity than you expect, then it is possible to design a process where you can find the specific capabilities and constraints related to your particular problem as you work through the process. This doesn&#39;t mean making any risky bets — we will show you how you can gradually roll out models so that they don&#39;t create significant risks, and can even backtest them prior to putting them in production. . Let&#39;s start with how you should frame your problem. . Starting your project . So where should you start your deep learning journey? The most important thing is to ensure that you have some project that you are working on — it is only through working on your own projects that you will get real experience of building and using models. When selecting a project, the most important consideration is data availability. Regardless of whether you are doing a project just for your own learning, or for practical application in your organization, you want something where you can get started quickly. We have seen many students, researchers, and industry practitioners waste months or years while they attempt to find their perfect dataset. The goal is not to find the perfect dataset, or the perfect project, but just to get started, and iterate from there. . If you take this approach, then you will be on your third iteration of learning and improving whilst the perfectionists are still in the planning stages! . We also suggest that you iterate from end to end in your project; that is, don&#39;t spend months fine tuning your model, or polishing the perfect GUI, or labelling the perfect dataset… Instead, complete every step as well as you can in a reasonable amount of time, all the way to the end. For instance, if your final goal is an application that runs on a mobile phone, then that should be what you have after each iteration. But perhaps in the early iterations you take some shortcuts, for instance by doing all of the processing on a remote server, and using a simple responsive web application. By completing the project and to end, you will see where the most tricky bits are, and which bits make the biggest difference to the final result. . As you work through this book, we suggest that you both complete lots of small experiments, by running and adjusting the notebooks we provide, at the same time that you gradually develop your own projects. That way, you will be getting experience with all of the tools and techniques that were explaining, as we discuss them. . s: To make the most of this book, take the time to experiment between each chapter, be it on your own project or exploring the notebooks we provide. Then try re-writing those notebooks from scratch on a new dataset. It&#39;s only by practicing (and failing) a lot that you will get an intuition on how to train a model. By using the end to end iteration approach you will also get a better understanding of how much data you really need. For instance, you may find you can only easily get 200 labelled data items, and you can&#39;t really know until you try whether that&#39;s enough to get the performance you need for your application to work well in practice. . In an organizational context you will be able to show your colleagues that your idea can really work, by showing them a real working prototype. We have repeatedly observed that this is the secret to getting good organizational buy in for a project. . Since it is easiest to get started on a project where you already have data available, that means it&#39;s probably easiest to get started on a project related to something you are already doing, because you already have data about things that you are doing. For instance, if you work in the music business, you may have access to many recordings. If you work as a radiologist, you probably have access to lots of medical images. If you are interested in wildlife preservation, you may have access to lots of images of wildlife. . Sometimes, you have to get a bit creative. Maybe you can find some previous machine learning project, such as a Kaggle competition, that is related to your field of interest. Sometimes, you have to compromize. Maybe you can&#39;t find the exact data you need for the precise project you have in mind; but you might be able to find something from a similar domain, or measured in a different way, tackling a slightly different problem. Working on these kinds of similar projects will still give you a good understanding of the overall process, and may help you identify other shortcuts, data sources, and so forth. . Especially when you are just starting out with deep learning it&#39;s not a good idea to branch out into very different areas to places that deep learning has not been applied to before. That&#39;s because if your model does not work at first, you will not know whether it is because you have made a mistake, or if the very problem you are trying to solve is simply not solvable with deep learning. And you won&#39;t know where to look to get help. Therefore, it is best at first to start with something where you can find an example online of somebody who has had good results with something that is at least somewhat similar to what you are trying to achieve, or where you can convert your data into a format similar what someone else has used before (such as creating an image from your data). Let&#39;s have a look at the state of deep learning, jsut so you know what kinds of things deep learning is good at right now. . The state of deep learning . Let&#39;s start by considering whether deep learning can be any good at the problem you are looking to work on. In general, here is a summary of the state of deep learning is at the start of 2020. However, things move very fast, and by the time you read this some of these constraints may no longer exist. We will try to keep the book website up-to-date; in addition, a Google search for &quot;what can AI do now&quot; there is likely to provide some up-to-date information. . Computer vision . There are many domains in which deep learning has not been used to analyse images yet, but those where it has been tried have nearly universally shown that computers can recognise what items are in an image at least as well as people can — even specially trained people, such as radiologists. This is known as object recognition. Deep learning is also good at recognizing whereabouts objects in an image are, and can highlight their location and name each found object. This is known as object detection (there is also a variant of this we saw in &lt;&gt;, where every pixel is categorized based on what kind of object it is part of--this is called segmentation). Deep learning algorithms are generally not good at recognizing images that are significantly different in structure or style to those used to train the model. For instance, if there were no black-and-white images in the training data, the model may do poorly on black-and-white images. If the training data did not contain hand-drawn images then the model will probably do poorly on hand-drawn images. There is no general way to check what types of image are missing in your training set, but we will show in this chapter some ways to try to recognize when unexpected image types arise in the data when the model is being used in production (this is known as checking for out of domain data).&lt;/p&gt; One major challenge for object detection systems is that image labelling can be slow and expensive. There is a lot of work at the moment going into tools to try to make this labelling faster and easier, and require less handcrafted labels to train accurate object detection models. One approach which is particularly helpful is to synthetically generate variations of input images, such as by rotating them, or changing their brightness and contrast; this is called data augmentation and also works well for text and other types of model. We will be discussing it in detail in this chapter. . Another point to consider is that although your problem might not look like a computer vision problem, it might be possible with a little imagination to turn it into one. For instance, if what you are trying to classify is sounds, you might try converting the sounds into images of their acoustic waveforms and then training a model on those images. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Text (natural language processing) . Just like in computer vision, computers are very good at categorising both short and long documents based on categories such as spam, sentiment (e.g. is the review positive or negative), author, source website, and so forth. We are not aware of any rigourous work done in this area to compare to human performance, but anecdotally it seems to us that deep learning performance is similar to human performance here. Deep learning is also very good at generating context-appropriate text, such as generating replies to social media posts, and imitating a particular author&#39;s style. It is also good at making this content compelling to humans, and has been shown to be even more compelling than human-generated text. However, deep learning is currently not good at generating correct responses! We don&#39;t currently have a reliable way to, for instance, combine a knowledge base of medical information, along with a deep learning model for generating medically correct natural language responses. This is very dangerous, because it is so easy to create content which appears to a layman to be compelling, but actually is entirely incorrect. . Another concern is that context-appropriate, highly compelling responses on social media can be used at massive scale — thousands of times greater than any troll farm previously seen — to spread disinformation, create unrest, and encourage conflict. As a rule of thumb, text generation will always be technologically a bit ahead of the ability of models to recognize automatically generated text. For instance, it is possible to use a model that can recognize artificially generated content to actually improve the generator that creates that content, until the classification model is no longer able to complete its task. . Despite these issues, deep learning can be used to translate text from one language to another, summarize long documents into something which can be digested more quickly, find all mentions of a concept of interest, and many more. Unfortunately, the translation or summary could well include completely incorrect information! However, it is already good enough that many people are using the systems — for instance Google&#39;s online translation system (and every other online service we are aware of) is based on deep learning. . Combining text and images . The ability of deep learning to combine text and images into a single model is, generally, far better than most people intuitively expect. For example, a deep learning model can be trained on input images, and output captions written in English, and can learn to generate surprisingly appropriate captions automatically for new images! But again, we have the same warning that we discussed in the previous section: there is no guarantee that these captions will actually be correct. . Because of this serious issue we generally recommend that deep learning be used not as an entirely automated process, but as part of a process in which the model and a human user interact closely. This can potentially make humans orders of magnitude more productive than they would be with entirely manual methods, and actually result in more accurate processes than using a human alone. For instance, an automatic system can be used to identify potential strokes directly from CT scans, send a high priority alert to have potential/scans looked at quickly. There is only a three-hour window to treat strokes, so this fast feedback loop could save lives. At the same time, however, all scans could continue to be sent to radiologists in the usual way, so there would be no reduction in human input. Other deep learning models could automatically measure items seen on the scan, and insert those measurements into reports, warning the radiologist about findings that they may have missed, and tell the radiologist about other cases which might be relevant. . Tabular data . For analysing timeseries and tabular data, deep learning has recently been making great strides. However, deep learning is generally used as part of an ensemble of multiple types of model. If you already have a system that is using random forests or gradient boosting machines (popular tabular modelling tools that we will learn about soon) then switching to, or adding, deep learning may not result in any dramatic improvement. Deep learning does greatly increase the variety of columns that you can include, for example columns containing natural language (e.g. book titles, reviews, etc), and high cardinality categorical columns (i.e. something that contains a large number of discrete choices, such as zip code or product id). On the downside, deep learning models generally take longer to train than random forests or gradient boosting machines, although this is changing thanks to libraries such as RAPIDS, which provides GPU acceleration for the whole modeling pipeline. We cover the pros and cons of all these methods in detail in &lt;&gt; in this book.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Recommendation systems . Recommendation systems are really just a special type of tabular data. In particular, they generally have a high cardinality categorical variable representing users, and another one representing products (or something similar). A company like Amazon represents every purchase that has ever been made as a giant sparse matrix, with customers as the rows and products as the columns. Once they have the data in this format, data scientists apply some form of collaborative filtering to fill in the matrix. For example, if customer A buys products 1 and 10, and customer B buys products 1, 2, 4, and 10, the engine will recommend that A buy 2 and 4. Because deep learning models are good at handling high cardinality categorical variables they are quite good at handling recommendation systems. They particularly come into their own, just like for tabular data, when combining these variables with other kinds of data, such as natural language, or images. They can also do a good job of combining all of these types of information with additional meta data represented as tables, such as user information, previous transactions, and so forth. . However, nearly all machine learning approaches have the downside that they only tell you what products a particular user might like, rather than what recommendations would be helpful for a user. Many kinds of recommendations for products a user might like may not be at all helpful, for instance, if the user is already familiar with its products, or if they are simply different packagings of products they have already purchased (such as a boxed set of novels, where they already have each of the items in that set). Jeremy likes reading books by Terry Pratchett, and for a while Amazon was recommending nothing but Terry Pratchett books to him (see &lt;&gt;), which really wasn&#39;t helpful because he already was aware of these books!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; A not-so-useful recommendation . Other data types: Often you will find that domain-specific data types fit very nicely into existing categories. For instance, protein chains look a lot like natural language documents, in that they are long sequences of discrete tokens with complex relationships and meaning throughout the sequence. And indeed, it does turn out that using NLP deep learning methods is the current state of the art approach for many types of protein analysis. As another example: sounds can be represented as spectrograms, which can be treated as images; standard deep learning approaches for images turn out to work really well on spectrograms. . There are many accurate models that are of no use to anyone, and many inaccurate models that are highly useful. To ensure that your modeling work is useful in practice, you need to consider how your work will be used. In 2012 Jeremy, along with Margit Zwemer and Mike Loukides, introduced a method called The Drivetrain Approach for thinking about this issue. . The Drivetrain approach . The Drivetrain approach, illustrated in &lt;&gt;, was described in detail in Designing Great Data Products. The basic idea is to start with considering your objective, then think about what you can actually do to change that objective (&quot;levers&quot;), what data you have that might help you connect potential changes to levers to changes in your objective, and then to build a model of that. You can then use that model to find the best actions (that is, changes to levers) to get the best results in terms of your objective.&lt;/p&gt; Consider a model in an autonomous vehicle, you want to help a car drive safely from point A to point B without human intervention. Great predictive modeling is an important part of the solution, but it doesn&#39;t stand on its own; as products become more sophisticated, it disappears into the plumbing. Someone using a self-driving car is completely unaware of the hundreds (if not thousands) of models and the petabytes of data that make it work. But as data scientists build increasingly sophisticated products, they need a systematic design approach. . We use data not just to generate more data (in the form of predictions), but to produce actionable outcomes. That is the goal of the Drivetrain Approach. Start by defining a clear objective. For instance, Google, when creating their first search engine, considered &quot;What is the user’s main objective in typing in a search query?&quot;, and their answer was &quot;show the most relevant search result&quot;. The next step is to consider what levers you can pull (i.e. what actions could you take) to better achieve that objective. In Google&#39;s case, that was the ranking of the search results. The third step was to consider what new data they would need to produce such a ranking; they realized that the implicit information regarding which pages linked to which other pages could be used for this purpose. Only after these first three steps do we begin thinking about building the predictive models. Our objective and available levers, what data we already have and what additional data we will need to collect, determine the models we can build. The models will take both the levers and any uncontrollable variables as their inputs; the outputs from the models can be combined to predict the final state for our objective. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; The Drivetrain approach . Let&#39;s consider another example: recommendation systems. The objective of a recommendation engine is to drive additional sales by surprising and delighting the customer with recommendations of items they would not have purchased without the recommendation. The lever is the ranking of the recommendations. New data must be collected to generate recommendations that will cause new sales. This will require conducting many randomized experiments in order to collect data about a wide range of recommendations for a wide range of customers. This is a step that few organizations take; but without it, you don&#39;t have the information you need to actually optimize recommendations based on your true objective (more sales!) . Finally, you could build two models for purchase probabilities, conditional on seeing or not seeing a recommendation. The difference between these two probabilities is a utility function for a given recommendation to a customer. It will be low in cases where the algorithm recommends a familiar book that the customer has already rejected (both components are small) or a book that he or she would have bought even without the recommendation (both components are large and cancel each other out). . As you can see, in practice often the practical implementation of your model will require a lot more than just training a model! You&#39;ll often need to run experiments to collect more data, and consider how to incorporate your models into the overall system you&#39;re developing. Speaking of data, let&#39;s now focus on how to find find data for your project. . Gathering data . For many types of projects, you may be able to find all the data you need online. The project we&#39;ll be completing in this chapter is a bear detector. It will discriminate between three types of bear: grizzly, black, and teddy bear. There are many images on the Internet of each type of bear we can use. We just need a way to find them and download them. We&#39;ve provided a tool you can use for this purpose, so you can follow along with this chapter, creating your own image recognition application for whatever kinds of object you&#39;re interested in. In the fast.ai course, thousands of students have presented their work on the course forums, displaying everything from Trinidad hummingbird varieties, to Panama bus types, and even an application that helped one student let his fiancee recognize his sixteen cousins during Christmas vacation! . As at the time of writing, Bing Image Search is the best option we know of for finding and downloading images. It&#39;s free for up to 1000 queries per month, and each query can download up to 150 images. However, something better might have come along between when we wrote this and when you&#39;re reading the book, so be sure to check out book.fast.ai where we&#39;ll let you know our current recommendation. . . Important: Services that can be used for creating datasets come and go all the time, and their features, interfaces, and pricing change regularly too. In this section, we&#8217;ll show how to use one particular provider, Bing Image Search, using the service they have as this book as written. We&#8217;ll be providing more options and more up to date information on the http://book.fast.ai[book website], so be sure to have a look there now to get the most current information on how to download images from the web to create a dataset for deep learning. . To download images with Bing Image Search, you should sign up at Microsoft for Bing Image Search. You will be given a key, which you can either paste here, replacing &quot;XXX&quot;: . key = &#39;XXX&#39; . ...or, if you&#39;re comfortable at the command line, you can set it in your terminal with: . export AZURE_SEARCH_KEY=your_key_here . and then restart jupyter notebooks, and finally execute in this notebook: . key = os.environ[&#39;AZURE_SEARCH_KEY&#39;] . Once you&#39;ve set key, you can use search_images_bing. This function is provided by the small utils class included in the book. Remember, if you&#39;re not sure where a symbol is defined, you can just type it in your notebook to find out (or prefix with ? to get help, including the name of the file where it&#39;s defined, or with ?? to get its source code): . search_images_bing . &lt;function utils.search_images_bing(key, term, min_sz=128)&gt; . results = search_images_bing(key, &#39;grizzly bear&#39;) ims = results.attrgot(&#39;content_url&#39;) len(ims) . 150 . We&#39;ve successfully downloaded the URLs of 150 grizzly bears (or, at least, images that Bing Image Search finds for that search term). Let&#39;s look at one: . dest = &#39;images/grizzly.jpg&#39; download_url(ims[0], dest) . im = Image.open(dest) im.to_thumb(128,128) . This seems to have worked nicely, so let&#39;s use fastai&#39;s download_images to download all the URLs from each of our search terms. We&#39;ll put each in a separate folder. . bear_types = &#39;grizzly&#39;,&#39;black&#39;,&#39;teddy&#39; path = Path(&#39;bears&#39;) . if not path.exists(): path.mkdir() for o in bear_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o} bear&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . Our folder has image files, as we&#39;d expect: . fns = get_image_files(path) fns . (#421) [Path(&#39;bears/black/00000095.jpg&#39;),Path(&#39;bears/black/00000133.jpg&#39;),Path(&#39;bears/black/00000062.jpg&#39;),Path(&#39;bears/black/00000023.jpg&#39;),Path(&#39;bears/black/00000029.jpg&#39;),Path(&#39;bears/black/00000094.jpg&#39;),Path(&#39;bears/black/00000124.jpg&#39;),Path(&#39;bears/black/00000056.jpeg&#39;),Path(&#39;bears/black/00000046.jpg&#39;),Path(&#39;bears/black/00000045.jpg&#39;)...] . j: I just love this about working in Jupyter notebooks! It&#39;s so easy to gradually build what I want, and check my work every step of the way. I make a lot of mistakes, so this is really helpful to me... . Often when we download files from the Internet, there are a few that are corrupt. Let&#39;s check: . failed = verify_images(fns) failed . (#0) [] . To remove the failed images, we can use unlink on each. Note that, like most fastai functions that return a collection, verify_images returns an object of type L, which includes the map method. This calls the passed function on each element of the collection. . failed.map(Path.unlink); . verify_images() . Sidebar: Getting help in jupyter notebooks . Jupyter notebooks are great to easily experiment and immediately see the results of each function, but there is also a lot of functionality to help figure out how to use the functions you have or even directly look at their source code. For instance, if you type in a cell . ??verify_images . a window will pop up with: . Signature: verify_images(fns) Source: def verify_images(fns): &quot;Find images in `fns` that can&#39;t be opened&quot; return L(fns[i] for i,o in enumerate(parallel(verify_image, fns)) if not o) File: ~/git/fastai/fastai/vision/utils.py Type: function . It tells us what argument the function accepts (fns) then shows us the source code and the file it comes from. Looking at that source code, we can see it applies the function verify_image in parallel and only keep the ones for which the result of that function is False, which is consistent with the doc string: it finds the images in fns that can&#39;t be opened. . Here are the commands that are very useful in Jupyter notebooks: . at any point, if you don&#39;t remember the exact spelling of a function or argument name, you can press &quot;tab&quot; to get suggestions of auto-completion. | when inside the parenthesis of a function, pressing &quot;shift&quot; and &quot;tab&quot; simultaneously will display a window with the signature of the function and a short documentation. Pressing it twice will expand the documentation and pressing it three times will open a full window with the same information at the bottom of your screen. | in a cell, typing ?func_name and executing will open a window with the signature of the function and a short documentation. | in a cell, typing ??func_name and executing will open a window with the signature of the function, a short documentation and the source code. | if you are using the fasti library, we added a doc function for you, executing doc(func_name) in a cell will open a window with the signature of the function, a short documentation and links to the source code on GitHub and the full documentation of the funciton in the documentation of the library. | unrelated to the documentation but still very useful to get help, at any point, if you get an error, type %debug in the next cell and execute to open the python debugger that will let you inspect the content of every variable. | . End sidebar . One thing to be aware of in this process: as we discussed in chapter_intro, models can only reflect the data used to train them. And the world is full of biased data, which ends up reflected in, for example, Bing Image Search (which we used to create our dataset). For instance, let&#39;s say you were interested in creating an app which could help users figure out whether they had healthy skin, so you trained a model on the results of searches for (say) healthy skin. &lt;&gt; shows you the results you would get.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Data for a healthy skin detector? . So with this as your training data, you would end up not with a healthy skin detector, but a young white woman touching her face detector! Be sure to think carefully about the types of data that you might expect to see in practice in your application, and check carefully to ensure that all these types are reflected in your model&#39;s source data.footnote:[Thanks to Deb Raji, who came up with the healthy skin example. See her paper Actionable Auditing: Investigating the Impact of Publicly Naming Biased Performance Results of Commercial AI Products for more fascinating insights into model bias.] . Now that we have downloaded some data, we need to assemble it in a format suitable for model training. In fastai, that means creating an object called DataLoaders. . From data to DataLoaders . Now that we have downloaded and verified the data that we want to use, we need to turn it into a DataLoaders object. DataLoaders is a thin class which just stores whatever DataLoader objects you pass to it, and makes them available as train and valid . Although it&#39;s a very simple class, it&#39;s very important in fastai: it provides the data for your model. The key functionality in DataLoaders is provided with just these 4 lines of code (it has some other minor functionality we&#39;ll skip over for now): . class DataLoaders(GetAttr): def __init__(self, *loaders): self.loaders = loaders def __getitem__(self, i): return self.loaders[i] train,valid = add_props(lambda i,self: self[i]) . jargon: DataLoaders: A fastai class which stores whatever DataLoader objects you pass to it, and makes them available as properties. . A DataLoaders object (i.e. the plural) stores multiple DataLoader objects, normally a train and a valid, although it&#39;s possible to have as many as you like. (Later in the book we&#39;ll also learn about the Dataset and Datasets classes, which have the same relationship). . To turn our downloaded data into DataLoaders we need to tell fastai at least four things: . what kinds of data we are working with ; | how to get the list of items ; | how to label these items ; | how to create the validation set. | . So far we have seen a number of factory methods for particular combinations of these things, which are convenient when you have an application and data structure which happens to fit into those predefined methods. For when you don&#39;t, fastai has an extremely flexible system called the data block API. With this API you can fully customize every stage of the creation of your DataLoaders. Here is what we need to create a DataLoaders for the dataset that we just downloaded: . bears = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.3, seed=42), get_y=parent_label, item_tfms=Resize(128)) . Let&#39;s look at each of these sections in turn: . blocks=(ImageBlock, CategoryBlock) . This is a tuple where we specify what types we want for the independent and dependent variables. The independent variable is the thing we are using to make predictions from, and the dependent variable is our target. In this case, our independent variable is a set of images, and our dependent variable are the categories (type of bear) for each image. We will see many other types of block in the rest of this book. . get_items=get_image_files . For this DataLoaders our underlying items will be file paths. We have to tell fastai how to get a list of those files. The get_image_files function takes a path, and returns a list of all of the images in that path (recursively, by default). . splitter=RandomSplitter(valid_pct=0.2, seed=42) . Often, datasets that you download will already have a validation set defined. Sometimes this is done by placing the images for the training and validation sets into different folders. Sometimes it is done by providing a CSV in which each file name is listed along with which dataset it should be in. There are many ways that this can be done, and fastai provides a very general approach which allows you to use one of fastai&#39;s predefined classes for this, or to write your own. In this case, however, we simply want to split our training and validation sets randomly. However, we would like to have the same training/validation split each time we run this notebook, so we fix the random seed. (Computers don&#39;t really know how to create random numbers at all, but simply create lists of numbers which look random. If you provide the same starting point for that list each time — called the seed — then you will get the exact same list each time.) . get_y=parent_label . The independent variable is often referred to as &quot;x&quot; and the dependent variable is often referred to as &quot;y&quot;. So in this section we are telling fastai what function to call to create the labels in our dataset. parent_label is a function provided by fastai which simply gets the name of the folder which a file is in. Because we put each of our bear images into folders based on the type of bear, this is going to give us the labels that we need. . item_tfms=Resize(128) . Our images are all different sizes, and this is a problem for deep learning: we don&#39;t feed the model one image at a time but several (what we call a mini-batch) of them. To group them in a big array (usually called tensor) that is going to go through our model, they all need to be of the same size. So we need to add a transform which will resize these images to the same size. item transforms are pieces of code which run on each individual item, whether it be an image, category, or so forth. fastai includes many predefined transforms; we will use the Resize transform here. . This command has given us a DataBlock object. This is like a template for creating a DataLoaders. We still need to tell fastai the actual source of our data — in this case, the path where the images can be found. . dls = bears.dataloaders(path) . A DataLoaders includes validation and training DataLoaders. A DataLoader is a class which provides batches of a few items at a time to the GPU. We&#39;ll be learning a lot more about this class in the next chapter. When you loop through a DataLoader fastai will give you 64 (by default) items at a time, all stacked up into a single tensor. We can take a look at a few of those items by calling the show_batch method on a DataLoader: . dls.valid.show_batch(max_n=4, rows=1) . By default Resize crops the images to fit a square shape of the size requested, using the full width or height. This can result in losing some important details. Alternatively, you can ask fastai to pad the images with zeros (which is black), or squish/stretch them: . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Squish)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, rows=1) . bears = bears.new(item_tfms=Resize(128, ResizeMethod.Pad, pad_mode=&#39;zeros&#39;)) dls = bears.dataloaders(path) dls.valid.show_batch(max_n=4, rows=1) . All of these approaches seem somewhat wasteful, or problematic. If we squished or stretch the images then they end up unrealistic shapes, leading to a model that learns that things look different to how they actually are, which we would expect to result in lower accuracy. If we crop the images then we remove some of the features that allow us to recognize them. For instance, if we were trying to recognise the breed of dog or cat, we may end up cropping out a key part of the body or the face necessary to distinguish between similar breeds. If we pad the images then we have a whole lot of empty space, which is just wasted computation for our model, and results in a lower effective resolution for the part of the image we actually use. . Instead, what we normally do in practice is to randomly select part of the image, and crop to just that part. On each epoch (which is one complete pass through all of our images in the dataset) we randomly select a different part of each image. This means that our model can learn to focus on, and recognize, different features in our images. It also reflects how images work in the real world; different photos of the same thing may be framed in slightly different ways. . Here is a another copy of the previous examples, but this time we are replacing Resize with RandomResizedCrop, which is the transform that provides the behaviour described above.The most important parameter to pass in is the min_scale parameter, which determines how much of the image to select at minimum each time. . bears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) dls = bears.dataloaders(path) dls.train.get_idxs = lambda: Inf.ones dls.train.show_batch(max_n=4, rows=1) . . Note: The get_idx assignment in this code is a little bit magic, and you absolutely don&#8217;t have to understand it at this point. So feel free to ignore the entirety of this paragraph! This is just if you&#8217;re curious… Showing different randomly varied versions of the same image is not something we normally have to do in deep learning, so it&#8217;s not something that fastai provides directly. Therefore to draw the picture of data augmentation on the same image, we had to take advantage of fastai&#8217;s sophisticated customisation features. DataLoader has a method called get_idx, which is called to decide which items should be selected next. Normally when we are training, this returns a random permutation of all of the indexes in the dataset. But pretty much everything in fastai can be changed, including how the get_idx method is defined, which means we can change how we sample data. So in this case, we are replacing it with a version which always returns the number one. That way, our DataLoader shows the same image again and again! This is a great example of the flexibility that fastai provides. . In fact, an entirely untrained neural network knows nothing whatsoever about how images behave. It doesn&#39;t even recognise that when an object is rotated by one degree, then it still is a picture of the same thing! So actually training the neural network with examples of images that are in slightly different places, and slightly different sizes, helps it to understand the basic concept of what a object is, and how it can be represented in an image. . This is a specific example of a more general technique, called data augmentation. . Data augmentation . Data augmentation refers to creating random variations of our input data, such that they appear different, but are not expected to change the meaning of the data. Examples of common data augmentation for images are rotation, flipping, perspective warping, brightness changes, contrast changes, and much more. For natural photo images such as the ones we are using here, there is a standard set of augmentations which we have found work pretty well, and are provided with the aug_transforms function. Because the images are now all the same size, we can apply these augmentations to an entire batch of them using the GPU, which will save a lot of time. To tell fastai we want to use these transforms to a batch, we use the batch_tfms parameter. (Note that&#39;s we&#39;re not using RandomResizedCrop in this example, so you can see the differences more clearly; we&#39;re also using double the amount of augmentation compared to the default, for the same reason). . bears = bears.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2)) dls = bears.dataloaders(path) dls.train.get_idxs = lambda: Inf.ones dls.train.show_batch(max_n=8, rows=2) . Now that we have assembled our data in a format fit for model training, let&#39;s actually train an image classifier using it. . Training your model, and using it to clean your data . Time to use the same lined of codes as in &lt;&gt; to train our bear classifier.&lt;/p&gt; We don&#39;t have a lot of data for our problem (150 pictures of each sort of bear at most), so to train our model, we&#39;ll use RandomResizedCrop and default aug_transforms for our model, on an image size of 224px, which is fairly standard for image classification. . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; bears = bears.new( item_tfms=RandomResizedCrop(224, min_scale=0.5), batch_tfms=aug_transforms()) dls = bears.dataloaders(path) . We can now create our Learner and fine tune it in the usual way. . learn = cnn_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(4) . epoch train_loss valid_loss error_rate time . 0 | 1.235733 | 0.212541 | 0.087302 | 00:05 | . epoch train_loss valid_loss error_rate time . 0 | 0.213371 | 0.112450 | 0.023810 | 00:05 | . 1 | 0.173855 | 0.072306 | 0.023810 | 00:06 | . 2 | 0.147096 | 0.039068 | 0.015873 | 00:06 | . 3 | 0.123984 | 0.026801 | 0.015873 | 00:06 | . Now let&#39;s see whether the mistakes the model is making is mainly thinking that grizzlies are teddies (that would be bad for safety!), or that grizzlies are black bears, or something else. We can create a confusion matrix: . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . Each row here represents all the black, grizzly, and teddy bears in our dataset, respectively. Each column represents the images which the model predicted as black, grizzly, and teddy bears, respectively. Therefore, the diagonal of the matrix shows the images which were classified correctly, and the other, off diagonal, cells represent those which were classified incorrectly. This is called a confusion matrix and is one of the many ways that fastai allows you to view the results of your model. It is (of course!) calculated using the validation set. With the color coding, the goal is to have white everywhere, except the diagonal where we want dark blue. Our bear classifier isn&#39;t making many mistakes! . It&#39;s helpful to see where exactly our errors are occuring, to see whether it&#39;s due to a dataset problem (e.g. images that aren&#39;t bears at all, or are labelled incorrectly, etc), or a model problem (e.g. perhaps it isn&#39;t handling images taken with unusual lighting, or from a different angle, etc). To do this, we can sort out images by their loss. . The loss is a number that is higher if the model is incorrect (and especially if it&#39;s also confident of its incorrect answer), or if it&#39;s correct, but not confident of its correct answer. In a couple chapters we&#39;ll learn in depth how loss is calculated and used in training process. For now, plot_top_losses shows us the images with the highest loss in our dataset. As the title of the output says, each image is labeled with four things: prediction, actual (target label), loss, and probability. The probability here is the confidence level, from zero to one, that the model has assigned to its prediction. . interp.plot_top_losses(5, rows=1) . This output shows that the highest loss is an image that has been predicted as &quot;grizzly&quot; with high confidence. However, it&#39;s labeled (based on our Bing image search) as &quot;black&quot;. We&#39;re not bear experts, but it sure looks to us like this label is incorrect! We should probably change its label to &quot;grizzly&quot;. . The intuitive approach to doing data cleaning is to do it before you train a model. But as you&#39;ve seen in this case, a model can actually help you find data issues more quickly and easily. So we normally prefer to train a quick and simple model first, and then use it to help us with data cleaning. . fastai includes a handy GUI for data cleaning called ImageClassifierCleaner, which allows you to choose a category, and training vs validation set, and view the highest-loss images (in order), along with menus to allow any images to be selected for removal, or relabeling. . #hide_output cleaner = ImageClassifierCleaner(learn) cleaner . NameError Traceback (most recent call last) &lt;ipython-input-4-45a2af6fc334&gt; in &lt;module&gt; 1 #hide_output -&gt; 2 cleaner = ImageClassifierCleaner(learn) 3 cleaner NameError: name &#39;learn&#39; is not defined . . We can see that amongst our black bears is an image that contain two bears, one grizzly, one black. So we should choose &lt;Delete&gt; in the menu under this image. ImageClassifierCleaner doesn&#39;t actually do the deleting or changing of labels for you; it just returns the indices of items to change. So, for instance, to delete (unlink) all images selected for deletion, we would run: . for idx in cleaner.delete(): cleaner.fns[idx].unlink() . To move images where we&#39;ve selected a different category, we would run: . for idx,cat in cleaner.change(): shutil.move(cleaner.fns[idx], path/cat) . s: Cleaning the data or getting it ready for your model are two of the biggest challenges for data scientists, one they say take 90% of their time. The fastai library aims at providing tools to make it as easy as possible. We&#39;ll be seeing more examples of model-driven data cleaning throughout this book. Once we&#39;ve cleaned up our data, we can retrain our model. Try it yourself, and see if your accuracy improves! . . Note: After cleaning the dataset using the above steps, we generally are seeing 100% accuracy on this task. We even see that result when we download a lot less images than the 150 per class we&#8217;re using here. As you can see, the common complaint you need massive amounts of data to do deep learning can be a very long way from the truth! . Now that we have trained our model, let&#39;s see how we can deploy it to be used in practice. . Turning your model into an online application . We are now going to look at what it takes to take this model and turn it into a working online application. We will just go as far as creating a basic working prototype; we do not have the scope in this book to teach you all the details of web application development generally. . Using the model for inference . Once you&#39;ve got a model you&#39;re happy with, you need to save it, so that you can then copy it over to a server where you&#39;ll use it in production. Remember that a model consists of two parts: the architecture, and the trained parameters. The easiest way to save a model is to save both of these, because that way when you load a model you can be sure that you have the matching architecture and parameters. To save both parts, use the export method. . This method even saves the definition of how to create your DataLoaders. This is important, because otherwise you would have to redefine how to transform your data in order to use your model in production. fastai automatically uses your validation set DataLoader for inference by default, so your data augmentation will not be applied, which is generally what you want. . When you call export, fastai will save a file called export.pkl. . learn.export() . Let&#39;s check that file exists, by using the Path.ls method that fastai adds to Python&#39;s Path class: . path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . You&#39;ll need this file wherever you deploy your app to. For now, let&#39;s try to create a simple app within our notebook. . When we use a model for getting predictions, instead of training, we call it inference. To create our inference learner from the exported file, we use load_learner (in this case, this isn&#39;t really necessary, since we already have a working Learner in our notebook; we&#39;re just doing it here so you can see the whole process end-to-end): . learn_inf = load_learner(path/&#39;export.pkl&#39;) . When we&#39;re doing inference, we&#39;re generally just getting predicitions for one image at a time. To do this, pass a filename to predict: . learn_inf.predict(&#39;images/grizzly.jpg&#39;) . (&#39;grizzly&#39;, tensor(1), tensor([9.0767e-06, 9.9999e-01, 1.5748e-07])) . This has returned three things: the predicted category in the same format you originally provided, in this case that&#39;s a string), the index of the predicted category, and the probabilities of each category. The last two are based on the order of categories in the vocab of the DataLoaders; that is, the stored list of all possible categories. At inference time, you can access the DataLoaders as an attribute of the Learner: . learn_inf.dls.vocab . (#3) [&#39;black&#39;,&#39;grizzly&#39;,&#39;teddy&#39;] . We can see here that if we index into the vocab with the integer returned by predict then we get back &quot;grizzly&quot;, as expected. Also, note that if we index into the list of probabilities, we see a nearly 1.00 probability that this is a grizzly. . We know how to make predictions from our saved model, so we have everything we need to start building our app. We can do it directly in a Jupyter Notenook. . Creating a Notebook app from the model . To use our model in an application we can simply treat the predict method as a regular function. Therefore, creating an app from the model can be done using any of the myriad of frameworks and techniques available to application developers. . However, most data scientists are not familiar with the world of web application development. So let&#39;s try using something that you do, at this point, know: Jupyter notebooks. It turns out that we can create a complete working web application using nothing but Jupyter notebooks! The two things we need to make this happen are: . IPython widgets (ipywidgets) | Voilà | . IPython widgets are GUI components that bring together JavaScript and Python functionality in a web browser, and can be created and used within a Jupyter notebook. For instance, the image cleaner that we saw earlier in this chapter is entirely written with IPython widgets. However, we don&#39;t want to require users of our application to have to run Jupyter themselves. . That is why Voilà exists. It is a system for making applications consisting of IPython widgets available to end-users, without them having to use Jupyter at all. Voila is taking advantage of the fact that a notebook already is a kind of web application, just a rather complex one that depends on another web application Jupyter itself. Essentially, it helps us automatically convert the complex web application which we&#39;ve already implicitly made (the notebook) into a simpler, easier-to-deploy web application, which functions like a normal web application rather than like a notebook. . But we still have the advantage of developing in a notebook. So with ipywidgets, we can build up our GUI step by step. We will use this approach to create a simple image classifier. First, we need a file upload widget: . #hide_output btn_upload = widgets.FileUpload() btn_upload . . Now we can grab the image: . img = PILImage.create(btn_upload.data[-1]) . . We can use an Output widget to display it: . #hide_output out_pl = widgets.Output() out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) out_pl . . Then we can get our predictions: . pred,pred_idx,probs = learn_inf.predict(img) . ...and use a Label to display them: . #hide_output lbl_pred = widgets.Label() lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; lbl_pred . Prediction: grizzly; Probability: 1.0000 . We&#39;ll need a button to do the classification, it looks exactly like the upload button. . #hide_output btn_run = widgets.Button(description=&#39;Classify&#39;) btn_run . ...and a click event handler, that is, a function that will be called when it&#39;s pressed; we can just copy over the lines of code from above: . def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) out_pl.clear_output() with out_pl: display(img.to_thumb(128,128)) pred,pred_idx,probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]:.04f}&#39; btn_run.on_click(on_click_classify) . You can test the button now by pressing it, and you should see the image and predictions above update automatically! . We can now put them all in a vertical box (VBox) to complete our GUI: . #hide_output VBox([widgets.Label(&#39;Select your bear!&#39;), btn_upload, btn_run, out_pl, lbl_pred]) . . We have written all the code necessary for our app. The next step is to convert it in something we can deploy. . Turning your notebook into a real app . Now that we have everything working in this Jupyter notebook, we can create our application. To do this, create a notebook which contains only the code needed to create and show the widgets that you need, and markdown for any text that you want to appear. Have a look at the bear_classifier notebook in the book repo to see the simple notebook application we created. . Next, install Voila if you have not already, by copying these lines into a Notebook cell, and executing it (if you&#39;re comfortable using the command line, you can also execute these two lines in your terminal, without the ! prefix): . !pip install voila !jupyter serverextension enable voila --sys-prefix . Cells which begin with a ! do not contain Python code, but instead contain code which is passed to your shell, such as bash, power shell in windows, or so forth. If you are comfortable using the command line (which we&#39;ll be learning about later in this book), you can of course simply type these two lines (without the ! prefix) directly into your terminal. In this case, the first line installs the voila library and application, and the second connects it to your existing Jupyter notebook. . Voila runs Jupyter notebooks, just like the Jupyter notebook server you are using now does, except that it does something very important: it removes all of the cell inputs, and only shows output (including ipywidgets), along with your markdown cells. So what&#39;s left is a web application! To view your notebook as a voila web application replace the word &quot;notebooks&quot; in your browser&#39;s URL with: &quot;voila/render&quot;. You will see the same content as your notebook, but without any of the code cells. . Of course, you don&#39;t need to use Voila or ipywidgets. Your model is just a function you can call: pred,pred_idx,probs = learn.predict(img) . So you can use it with any framework, hosted on any platform. And you can take something you&#39;ve prototyped in ipywidgets and Voila and later convert it into a regular web application. We&#39;re showing you this approach in the book because we think it&#39;s a great way for data scientists and other folks that aren&#39;t web development experts to create applications from their models. . We have our app, now let&#39;s deploy it! . Deploying your app . As we now know, you need a GPU to train nearly any useful deep learning model. So, do you need a GPU to use that model in production? No! You almost certainly do not need a GPU to serve your model in production. There&#39;s a few reasons for this: . As we&#39;ve seen, GPUs are only useful when they do lots of identical work in parallel. If you&#39;re doing (say) image classification, then you&#39;ll normally be classifying just one user&#39;s image at a time, and there isn&#39;t normally enough work to do in a single image to keep a GPU busy for long enough for it to be very efficient. So a CPU will often be more cost effective. | An alternative could be to wait for a few users to submit their images, and then batch them up, and do them all at once on a GPU. But then you&#39;re asking your users to wait, rather than getting answers straight away! And you need a high volume site for this to be workable. If you do need this functionality, you can use a tool such as Microsoft&#39;s ONNX Runtime, or AWS Sagemaker | The complexities of dealing with GPU inference are significant. In particular, the GPU&#39;s memory will need careful manual management, and you&#39;ll need some careful queueing system to ensure you only do one batch at a time | There&#39;s a lot more market competition in CPU servers than GPU, as a result of which there&#39;s much cheaper options available for CPU servers. | . Because of the complexity of GPU serving, many systems have sprung up to try to automate this. However, managing and running these systems is themselves complex, and generally requires compiling your model into a different form that&#39;s specialized for that system. It doesn&#39;t make sense to deal with this complexity until/unless your app gets popular enough that it makes clear financial sense for you to do so. . For at least the initial prototype of your application, and for any hobby projects that you want to show off, you can easily host them for free. The best place and the best way to do this will vary over time so check the book website for the most up-to-date recommendations. As we&#39;re writing this book in 2020 the simplest (and free!) approach is called Binder. To publish your web app on Binder, you follow these steps: . Add your notebook to a GitHub repository, | Paste the URL of that repo in the URL field of Binder as shown in &lt;&gt;, &lt;/li&gt; Change the &quot;File&quot; dropdown to instead select &quot;URL&quot;, | In the Path field, enter /voila/render/name.ipynb (replacing name.ipynb as appropriate for your notebook): | Click the &quot;Copy the URL&quot; button and paste it somewhere safe. | Click &quot;Launch&quot;. | &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Deploying to Binder . The first time you do this Binder will take around 5 minutes to build your site. In other words, is it finding a virtual machine which can run your app, allocating storage, collecting the files needed for Jupyter, for your notebook, and for presenting your notebook as a web application. It&#39;s doing all of this behind the scenes. . Finally, once it has started the app running, it will navigate your browser to your new web app. You can share the URL you copied to allow others to access your app as well. . For other (both free and paid) options for deploying your web app, be sure to take a look at the book web site. . You may well want to deploy your application onto mobile devices, or edge devices such as a Raspberry Pi. There are a lot of libraries and frameworks to allow you to integrate a model directly into a mobile application. However these approaches tend to require a lot of extra steps and boilerplate, and do not always support all the PyTorch and fastai layers that your model might use. In addition, the work you do will depend on what kind of mobile devices you are targeting for deployment. So you might need to do some work to run on iOS devices, different work to run on newer Android devices, different work for older Android devices, etc.. Instead, we recommend wherever possible that you deploy the model itself to a server, and have your mobile or edge application connect to it as a web service. . There is quite a few upsides to this approach. The initial installation is easier, because you only have to deploy a small GUI application, which connects to the server to do all the heavy lifting. More importantly perhaps, upgrades of that core logic can happen on your server, rather than needing to be distributed to all of your users. Your server can have a lot more memory and processing capacity than most edge devices, and it is far easier to scale those resources if your model becomes more demanding. The hardware that you will have on a server is going to be more standard and more easily supported by fastai and PyTorch, so you don&#39;t have to compile your model into a different form. . There are downsides too, of course. Your application will require a network connection, and there will be some latency each time the model is called. It takes a while for a neural network model to run anyway, so this additional network latency may not make a big difference to your users in practice. In fact, since you can use better hardware on the server, the overall latency may even be less! If your application uses sensitive data then your users may be concerned about an approach which sends that data to a remote server, so sometimes privacy considerations will mean that you need to run the model on the edge device. Sometimes this can be avoided by having an on premise server, such as inside a company&#39;s firewall. Managing the complexity and scaling the server can create additional overhead, whereas if your model runs on the edge devices then each user is bringing their own compute resources, which leads to easier scaling with an increasing number of users (also known as horizontal scaling). . A: I&#39;ve had a chance to see up close how the mobile ML landscape is changing in my work. We offer an iPhone app that depends on computer vision and for years we ran our own computer vision models in the cloud. This was the only way to do it then since those models needed significant memory and compute resources and took minutes to process. This approach required building not only the models (fun!) but infrastructure to ensure a certain number of &quot;compute worker machines&quot; was absolutely always running (scary), that more machines would automatically come online if traffic increased, that there was stable storage for large inputs and outputs, that the iOS app could know and tell the user how their job was doing, etc... Nowadays, Apple provides APIs for converting models to run efficiently on device and most iOS devices have dedicated ML hardware, so we run our new models on device. So, in a few years that strategy has gone from impossible to possible but it&#39;s still not easy. In our case it&#39;s worth it, for a faster user experience and to worry less about servers. What works for you will depend, realistically, on the user experience you&#39;re trying to create and what you personally find it easy to do. If you really know how to run servers, do it. If you really know how to build native mobile apps, do that. There are many roads up the hill. Overall, we&#39;d recommend using a simple CPU-based server approach where possible, for as long as you can get away with it. If you&#39;re lucky enough to have a very successful application, then you&#39;ll be able to justify the investment in more complex deployment approaches at that time. . Congratulations, you have succesfully built a deep learning model and deployed it! Now is a good time to take a pause and think about what could go wrong. . How to avoid disaster . In practice, a deep learning model will be just one piece of a much bigger system. As we discussed at the start of this chapter, a data product requires thinking about the entire end to end process within which our model lives. In this book, we can&#39;t hope to cover all the complexity of managing deployed data products, such as managing multiple versions of models, A/B testing, canarying, refreshing the data (should we just grow and grow our datasets all the time, or should we regularly remove some of the old data), handling data labelling, monitoring all this, detecting model rot, and so forth. However, there is an excellent book that covers many deployment issues, which is Building Machine Learning Powered Applications, by Emmanuel Ameisen. In this section, we will give an overview of some of the most important issues to consider. . One of the biggest issues with this is that understanding and testing the behavior of a deep learning model is much more difficult than most code that you would write. With normal software development you can analyse the exact steps that the software is taking, and carefully study with of these steps match the desired behaviour that you are trying to create. But with a neural network the behavior emerges from the models attempt to match the training data, rather than being exactly defined. . This can result in disaster! For instance, let&#39;s say you really were rolling out a bear detection system which will be attached to video cameras around the campsite, and will warn campers of incoming bears. If we used a model trained with the dataset we downloaded, there are going to be all kinds of problems in practice, such as: . working with video data instead of images ; | handling nighttime images, which may not appear in this dataset ; | dealing with low resolution camera images ; | ensuring results are returned fast enough to be useful in practice ; | recognising bears in positions that are rarely seen in photos that people post online (for example from behind, partially covered by bushes, or when a long way away from the camera). | . A big part of the issue is that the kinds of photos that people are most likely to upload to the Internet are the kinds of photos that do a good job of clearly and artistically displaying their subject matter. So we may need to do a lot of our own data collection and labelling to create a useful system. . This is just one example of the more general problem of out of domain data. That is to say, there may be data that our model sees in production which is very different to what it saw during training. There isn&#39;t really a complete technical solution to this problem; instead we have to be careful about our approach to rolling out the technology. . There are other reasons we need to be careful too. One very common problem is domain shift; this is where the type of data that our model sees changes over time. For instance, an insurance company may use a deep learning model as part of their pricing and risk algorithm, but over time the type of customers that they attract, and the type of risks that they represent, may change so much that the original training data is no longer relevant. . Out of domain data, and domain shift, are examples of the problem that you can never fully know the entire behaviour of your neural network. They have far too many parameters to be able to analytically understand all of their possible behaviours. This is the natural downside of the thing that they&#39;re so good at — their flexibility in being able to solve complex problems where we may not even be able to fully specify our preferred solution approaches. The good news, however, is that there are ways to mitigate these risks using a carefully thought out process. The details of this will vary depending on the details of the problem you are solving, but we will attempt to lay out here a high-level approach summarized in &lt;&gt; which we hope will provide useful guidance.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; Deployment process . Where possible, the first step is to use an entirely manual process, with your deep learning model approach running in parallel, but not being used directly to drive any actions. The humans involved in the manual process should look at the deep learning outputs and check whether they make sense. For instance, with our bear classifier a park ranger could have a screen displaying any time a possible bear sighting occurred in any camera, and simply highlight them in red on the screen. The park ranger would still be expected to be just as alert as before the model was deployed; the model is simply helping to check for problems at this point. . The second step is to try to limit the scope of the model, and have it carefully supervised by people. For instance, do a small geographically and time constrained trial of the model-driven approach. Rather than rolling your bear classifier out in every national park throughout the country, pick a single observation post, for a one-week period, and have a park ranger check each alert before it goes out. . Then, gradually increase the scope of your rollout. As you do so, ensure that you have really good reporting systems in place, to make sure that you are aware of any significant changes to the actions being taken compared to your manual process. For instance, if the number of bear alerts doubles or halves after rollout of the new system in some location we should be very concerned. Try to think about all the ways in which your system could go wrong, and then think about what measure or report or picture could reflect that problem, and then ensure that your regular reporting includes that information. . j: I started a company 20 years ago called Optimal Decisions which used machine learning and optimisation to help giant insurance companies set their pricing, impacting tens of billions of dollars of risks. We used the approaches described above to manage the potential downsides of something that might go wrong. Also, before we worked with our clients to put anything in production, we tried to simulate the impact by testing the end to end system on their previous year&#39;s data. It was always quite a nerve-wracking process, putting these new algorithms in production, but every rollout was successful. . Unforeseen consequences and feedback loops . One of the biggest challenges in rolling out a model is that your model may change the behaviour of the system it is a part of. For instance, consider a &quot;predictive policing&quot; algorithm that predicts more crime in certain neighborhoods, causing more police officers to be sent to those neighborhoods, which can result in more crime being recorded in those neighborhoods, and so on. In the Royal Statiscal Society paper To predict and serve, Kristian Lum and William Isaac write: &quot;predictive policing is aptly named: it is predicting future policing, not future crime&quot;. . Part of the issue in this case is that in the presence of bias (which we&#39;ll discuss in depth in the next chapter), feedback loops can result in negative implications of that bias getting worse and worse. For instance, there are concerns that this is already happening in the US, where there is significant bias in arrest rates on racial grounds. According to the ACLU, &quot;despite roughly equal usage rates, Blacks are 3.73 times more likely than whites to be arrested for marijuana&quot;. The impact of this bias, along with the roll-out of predictive policing algorithms in many parts of the US, led Bärí Williams to write in the NY Times: &quot;The same technology that’s the source of so much excitement in my career is being used in law enforcement in ways that could mean that in the coming years, my son, who is 7 now, is more likely to be profiled or arrested — or worse — for no reason other than his race and where we live.&quot; . A helpful exercise prior to rolling out a significant machine learning system is to consider this question: &quot;what would happen if it went really, really well?&quot; In other words, what if the predictive power was extremely high, and its ability to influence behaviour was extremely significant? In that case, who would be most impacted? What would the most extreme results potentially look like? How would you know what was really going on? . Such a thought exercise might help you to construct a more careful rollout plan, ongoing monitoring systems, and human oversight. Of course, human oversight isn&#39;t useful if it isn&#39;t listened to; so make sure that there are reliable and resilient communication channels so that the right people will be aware of issues, and will have the power to fix them. . Get writing! . One of the things our students have found most helpful to solidify their understanding of this material is to write it down. There is no better test of your understanding of a topic than attempting to teach it to somebody else. This is helpful even if you never show your writing to anybody — but it&#39;s even better if you share it! So we recommend that, if you haven&#39;t already, you start a blog. Now that you&#39;ve finished chapter 2, and have learned how to train and deploy models, you&#39;re well placed to write your first blog post about your deep learning journey. What&#39;s surprised you? What opportunities do you see for deep learning in your field? What obstacles do you see? . Rachel Thomas, co-founder of fast.ai, wrote in the article Why you (yes, you) should blog: . asciidoc ____ The top advice I would give my younger self would be to start blogging sooner. Here are some reasons to blog: * It’s like a resume, only better. I know of a few people who have had blog posts lead to job offers! * Helps you learn. Organizing knowledge always helps me synthesize my own ideas. One of the tests of whether you understand something is whether you can explain it to someone else. A blog post is a great way to do that. * I’ve gotten invitations to conferences and invitations to speak from my blog posts. I was invited to the TensorFlow Dev Summit (which was awesome!) for writing a blog post about how I don’t like TensorFlow. * Meet new people. I’ve met several people who have responded to blog posts I wrote. * Saves time. Any time you answer a question multiple times through email, you should turn it into a blog post, which makes it easier for you to share the next time someone asks. ____ . Perhaps her most important tip is this: &quot;You are best positioned to help people one step behind you. The material is still fresh in your mind. Many experts have forgotten what it was like to be a beginner (or an intermediate) and have forgotten why the topic is hard to understand when you first hear it. The context of your particular background, your particular style, and your knowledge level will give a different twist to what you’re writing about.&quot; . We&#39;ve provided full details on how to set up a blog in an appendix &quot;Creating a blog&quot;. If you don&#39;t have a blog already, jump over to that chapter now, because we&#39;ve got a really great approach set up for you to start blogging, for free, with no ads--and you can even use Jupyter Notebook! . Questionnaire . Provide an example of where the bear classification model might work poorly, due to structural or style differences to the training data | Where do text models currently have a major deficiency? | What are possible negative societal implications of text generation models? | In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process? | What kind of tabular data is deep learning particularly good at? | What&#39;s a key downside of directly using a deep learning model for recommendation systems? | What are the steps of the Drivetrain approach? | How do the steps of the Drivetrain approach map to a recommendation system? | Create an image recognition model using data you curate, and deploy it on the web. | What is DataLoaders? | What four things do we need to tell fastai to create DataLoaders? | What does the splitter parameter to DataBlock do? | How do we ensure a random split always gives the same validation set? | What letters are often used to signify the independent and dependent variables? | What&#39;s the difference between crop, pad, and squish resize approaches? When might you choose one over the other? | What is data augmentation? Why is it needed? | What is the difference between item_tfms and batch_tfms? | What is a confusion matrix? | What does export save? | What is it called when we use a model for getting predictions, instead of training? | What are IPython widgets? | When might you want to use CPU for deployment? When might GPU be better? | What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC? | What are 3 examples of problems that could occur when rolling out a bear warning system in practice? | What is &quot;out of domain data&quot;? | What is &quot;domain shift&quot;? | What are the 3 steps in the deployment process? | For a project you&#39;re interested in applying deep learning to, consider the thought experiment &quot;what would happen if it went really, really well?&quot; | Start a blog, and write your first blog post. For instance, write about what you think deep learning might be useful for in a domain you&#39;re interested in. | Further research . Consider how the Drivetrain approach maps to a project or problem you&#39;re interested in. | When might it be best to avoid certain types of data augmentation? | &lt;/div&gt; . | . .",
            "url": "https://maxlein.github.io/fastbook/2020/03/06/_production.html",
            "relUrl": "/2020/03/06/_production.html",
            "date": " • Mar 6, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "https://maxlein.github.io/fastbook/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Test Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://maxlein.github.io/fastbook/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://maxlein.github.io/fastbook/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}